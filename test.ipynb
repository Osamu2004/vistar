{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "from operator import attrgetter\n",
    "from typing import List, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def efficient_conv_bn_eval_forward(bn: nn.modules.batchnorm._BatchNorm,\n",
    "                                   conv: nn.modules.conv._ConvNd,\n",
    "                                   x: torch.Tensor):\n",
    "    \"\"\"Code borrowed from mmcv 2.0.1, so that this feature can be used for old\n",
    "    mmcv versions.\n",
    "\n",
    "    Implementation based on https://arxiv.org/abs/2305.11624\n",
    "    \"Tune-Mode ConvBN Blocks For Efficient Transfer Learning\"\n",
    "    It leverages the associative law between convolution and affine transform,\n",
    "    i.e., normalize (weight conv feature) = (normalize weight) conv feature.\n",
    "    It works for Eval mode of ConvBN blocks during validation, and can be used\n",
    "    for training as well. It reduces memory and computation cost.\n",
    "    Args:\n",
    "        bn (_BatchNorm): a BatchNorm module.\n",
    "        conv (nn._ConvNd): a conv module\n",
    "        x (torch.Tensor): Input feature map.\n",
    "    \"\"\"\n",
    "    # These lines of code are designed to deal with various cases\n",
    "    # like bn without affine transform, and conv without bias\n",
    "    weight_on_the_fly = conv.weight\n",
    "    if conv.bias is not None:\n",
    "        bias_on_the_fly = conv.bias\n",
    "    else:\n",
    "        bias_on_the_fly = torch.zeros_like(bn.running_var)\n",
    "\n",
    "    if bn.weight is not None:\n",
    "        bn_weight = bn.weight\n",
    "    else:\n",
    "        bn_weight = torch.ones_like(bn.running_var)\n",
    "\n",
    "    if bn.bias is not None:\n",
    "        bn_bias = bn.bias\n",
    "    else:\n",
    "        bn_bias = torch.zeros_like(bn.running_var)\n",
    "\n",
    "    # shape of [C_out, 1, 1, 1] in Conv2d\n",
    "    weight_coeff = torch.rsqrt(bn.running_var +\n",
    "                               bn.eps).reshape([-1] + [1] *\n",
    "                                               (len(conv.weight.shape) - 1))\n",
    "    # shape of [C_out, 1, 1, 1] in Conv2d\n",
    "    coefff_on_the_fly = bn_weight.view_as(weight_coeff) * weight_coeff\n",
    "\n",
    "    # shape of [C_out, C_in, k, k] in Conv2d\n",
    "    weight_on_the_fly = weight_on_the_fly * coefff_on_the_fly\n",
    "    # shape of [C_out] in Conv2d\n",
    "    bias_on_the_fly = bn_bias + coefff_on_the_fly.flatten() *\\\n",
    "        (bias_on_the_fly - bn.running_mean)\n",
    "\n",
    "    return conv._conv_forward(x, weight_on_the_fly, bias_on_the_fly)\n",
    "\n",
    "\n",
    "def efficient_conv_bn_eval_control(bn: nn.modules.batchnorm._BatchNorm,\n",
    "                                   conv: nn.modules.conv._ConvNd,\n",
    "                                   x: torch.Tensor):\n",
    "    \"\"\"This function controls whether to use `efficient_conv_bn_eval_forward`.\n",
    "\n",
    "    If the following `bn` is in `eval` mode, then we turn on the special\n",
    "    `efficient_conv_bn_eval_forward`.\n",
    "    \"\"\"\n",
    "    if not bn.training:\n",
    "        # bn in eval mode\n",
    "        output = efficient_conv_bn_eval_forward(bn, conv, x)\n",
    "        return output\n",
    "    else:\n",
    "        conv_out = conv._conv_forward(x, conv.weight, conv.bias)\n",
    "        return bn(conv_out)\n",
    "\n",
    "\n",
    "def efficient_conv_bn_eval_graph_transform(fx_model):\n",
    "    \"\"\"Find consecutive conv+bn calls in the graph, inplace modify the graph\n",
    "    with the fused operation.\"\"\"\n",
    "    modules = dict(fx_model.named_modules())\n",
    "\n",
    "    patterns = [(torch.nn.modules.conv._ConvNd,\n",
    "                 torch.nn.modules.batchnorm._BatchNorm)]\n",
    "\n",
    "    pairs = []\n",
    "    # Iterate through nodes in the graph to find ConvBN blocks\n",
    "    for node in fx_model.graph.nodes:\n",
    "        # If our current node isn't calling a Module then we can ignore it.\n",
    "        if node.op != 'call_module':\n",
    "            continue\n",
    "        target_module = modules[node.target]\n",
    "        found_pair = False\n",
    "        for conv_class, bn_class in patterns:\n",
    "            if isinstance(target_module, bn_class):\n",
    "                source_module = modules[node.args[0].target]\n",
    "                if isinstance(source_module, conv_class):\n",
    "                    found_pair = True\n",
    "        # Not a conv-BN pattern or output of conv is used by other nodes\n",
    "        if not found_pair or len(node.args[0].users) > 1:\n",
    "            continue\n",
    "\n",
    "        # Find a pair of conv and bn computation nodes to optimize\n",
    "        conv_node = node.args[0]\n",
    "        bn_node = node\n",
    "        pairs.append([conv_node, bn_node])\n",
    "\n",
    "    for conv_node, bn_node in pairs:\n",
    "        # set insertion point\n",
    "        fx_model.graph.inserting_before(conv_node)\n",
    "        # create `get_attr` node to access modules\n",
    "        # note that we directly call `create_node` to fill the `name`\n",
    "        # argument. `fx_model.graph.get_attr` and\n",
    "        # `fx_model.graph.call_function` does not allow the `name` argument.\n",
    "        conv_get_node = fx_model.graph.create_node(\n",
    "            op='get_attr', target=conv_node.target, name='get_conv')\n",
    "        bn_get_node = fx_model.graph.create_node(\n",
    "            op='get_attr', target=bn_node.target, name='get_bn')\n",
    "        # prepare args for the fused function\n",
    "        args = (bn_get_node, conv_get_node, conv_node.args[0])\n",
    "        # create a new node\n",
    "        new_node = fx_model.graph.create_node(\n",
    "            op='call_function',\n",
    "            target=efficient_conv_bn_eval_control,\n",
    "            args=args,\n",
    "            name='efficient_conv_bn_eval')\n",
    "        # this node replaces the original conv + bn, and therefore\n",
    "        # should replace the uses of bn_node\n",
    "        bn_node.replace_all_uses_with(new_node)\n",
    "        # take care of the deletion order:\n",
    "        # delete bn_node first, and then conv_node\n",
    "        fx_model.graph.erase_node(bn_node)\n",
    "        fx_model.graph.erase_node(conv_node)\n",
    "\n",
    "    # regenerate the code\n",
    "    fx_model.graph.lint()\n",
    "    fx_model.recompile()\n",
    "\n",
    "\n",
    "def turn_on_efficient_conv_bn_eval_for_single_model(model: torch.nn.Module):\n",
    "    import torch.fx as fx\n",
    "\n",
    "    # currently we use `fx.symbolic_trace` to trace models.\n",
    "    # in the future, we might turn to pytorch 2.0 compile infrastructure to\n",
    "    # get the `fx.GraphModule` IR. Nonetheless, the graph transform function\n",
    "    # can remain unchanged. We just need to change the way\n",
    "    # we get `fx.GraphModule`.\n",
    "    fx_model: fx.GraphModule = fx.symbolic_trace(model)\n",
    "    efficient_conv_bn_eval_graph_transform(fx_model)\n",
    "    model.forward = fx_model.forward\n",
    "\n",
    "\n",
    "def turn_on_efficient_conv_bn_eval(model: torch.nn.Module,\n",
    "                                   modules: Union[List[str], str]):\n",
    "    if isinstance(modules, str):\n",
    "        modules = [modules]\n",
    "    for module_name in modules:\n",
    "        module = attrgetter(module_name)(model)\n",
    "        turn_on_efficient_conv_bn_eval_for_single_model(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method SimpleCNN2.forward of SimpleCNN2(\n",
      "  (backbone): SimpleCNN(\n",
      "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc): Linear(in_features=32768, out_features=10, bias=True)\n",
      "  )\n",
      ")>\n",
      "Output shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.fx as fx\n",
    "\n",
    "# 定义一个简单的卷积神经网络作为示例\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, 1, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, 1, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.fc = nn.Linear(32 * 32 * 32, 10)  # 假设输入图像大小为32x32\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = x.view(x.size(0), -1)  # 展平\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class SimpleCNN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN2 ,self).__init__()\n",
    "        self.backbone = SimpleCNN()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "# 调用模型并检查优化情况\n",
    "model = SimpleCNN2()\n",
    "\n",
    "# 创建一个随机输入\n",
    "input_tensor = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "# 调用函数以启用高效的Conv + BN融合并打印优化前后的计算图\n",
    "\n",
    "\n",
    "model = model.eval()\n",
    "turn_on_efficient_conv_bn_eval(model, ['backbone'])\n",
    "print(model.forward)\n",
    "# 执行前向传播\n",
    "output = model(input_tensor)\n",
    "\n",
    "# 打印输出的形状\n",
    "print(f\"Output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method forward of SimpleCNN(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=32768, out_features=10, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# 检查替换后的 forward 方法\n",
    "print(model.forward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %conv1 : [num_users=1] = call_module[target=conv1](args = (%x,), kwargs = {})\n",
      "    %bn1 : [num_users=1] = call_module[target=bn1](args = (%conv1,), kwargs = {})\n",
      "    %relu : [num_users=1] = call_function[target=torch.relu](args = (%bn1,), kwargs = {})\n",
      "    %conv2 : [num_users=1] = call_module[target=conv2](args = (%relu,), kwargs = {})\n",
      "    %bn2 : [num_users=1] = call_module[target=bn2](args = (%conv2,), kwargs = {})\n",
      "    %relu_1 : [num_users=2] = call_function[target=torch.relu](args = (%bn2,), kwargs = {})\n",
      "    %size : [num_users=1] = call_method[target=size](args = (%relu_1, 0), kwargs = {})\n",
      "    %view : [num_users=1] = call_method[target=view](args = (%relu_1, %size, -1), kwargs = {})\n",
      "    %fc : [num_users=1] = call_module[target=fc](args = (%view,), kwargs = {})\n",
      "    return fc\n"
     ]
    }
   ],
   "source": [
    "# 打印 FX 计算图\n",
    "import torch.fx as fx\n",
    "fx_model = fx.symbolic_trace(model)\n",
    "print(fx_model.graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %conv1 : [num_users=1] = call_module[target=conv1](args = (%x,), kwargs = {})\n",
      "    %bn1 : [num_users=1] = call_module[target=bn1](args = (%conv1,), kwargs = {})\n",
      "    %relu : [num_users=1] = call_function[target=torch.relu](args = (%bn1,), kwargs = {})\n",
      "    %conv2 : [num_users=1] = call_module[target=conv2](args = (%relu,), kwargs = {})\n",
      "    %bn2 : [num_users=1] = call_module[target=bn2](args = (%conv2,), kwargs = {})\n",
      "    %relu_1 : [num_users=2] = call_function[target=torch.relu](args = (%bn2,), kwargs = {})\n",
      "    %size : [num_users=1] = call_method[target=size](args = (%relu_1, 0), kwargs = {})\n",
      "    %view : [num_users=1] = call_method[target=view](args = (%relu_1, %size, -1), kwargs = {})\n",
      "    %fc : [num_users=1] = call_module[target=fc](args = (%view,), kwargs = {})\n",
      "    return fc\n"
     ]
    }
   ],
   "source": [
    "# 打印 FX 计算图\n",
    "import torch.fx as fx\n",
    "fx_model = fx.symbolic_trace(model)\n",
    "print(fx_model.graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method forward of SimpleCNN(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=32768, out_features=10, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# 检查替换后的 forward 方法\n",
    "print(model.forward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.4928, -0.7771, -1.0377,  ..., -0.8974, -0.3298, -0.3931],\n",
      "          [ 0.2901,  0.2581,  0.7802,  ...,  0.9171,  0.9820,  0.1806],\n",
      "          [-1.3592,  0.2125,  1.1953,  ..., -0.4506,  0.3617,  1.6183],\n",
      "          ...,\n",
      "          [-0.5671,  0.9266,  2.0484,  ..., -0.4558, -0.8388,  0.0452],\n",
      "          [ 0.0935, -0.7979,  0.9503,  ...,  0.5598, -1.9626, -0.2911],\n",
      "          [ 0.1995,  0.6507, -0.2142,  ..., -0.3146, -0.2791,  0.4547]],\n",
      "\n",
      "         [[ 1.4356,  0.9129, -0.5031,  ..., -0.1020,  1.0058, -0.4193],\n",
      "          [-1.4852, -0.4363, -0.8256,  ..., -0.9477, -0.0374,  0.1647],\n",
      "          [-0.3652, -0.2904,  2.5691,  ...,  0.5813, -0.8416, -0.9438],\n",
      "          ...,\n",
      "          [ 0.4869,  2.5273,  1.1332,  ...,  2.2675,  0.0770,  0.6853],\n",
      "          [ 0.0430, -1.8617, -0.6892,  ..., -0.8342, -0.1863, -0.8786],\n",
      "          [ 0.0802,  0.3856, -0.6600,  ...,  0.8860,  1.6499, -0.8380]],\n",
      "\n",
      "         [[-0.1324,  0.0349,  0.6080,  ..., -0.8111,  0.4878, -0.4131],\n",
      "          [-0.3960, -0.8790, -0.4031,  ..., -0.4868, -1.6179, -0.3251],\n",
      "          [-0.3331,  0.0616, -0.1891,  ...,  0.2430, -2.2637, -0.6969],\n",
      "          ...,\n",
      "          [-0.3116,  1.3042,  2.2381,  ..., -0.5227,  0.2172,  0.3950],\n",
      "          [ 0.8119, -0.1822, -0.6837,  ...,  1.7518, -0.8511, -0.4028],\n",
      "          [ 0.3381,  1.2023,  1.0385,  ...,  1.3968,  1.8621,  0.7444]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.6047,  2.0174,  0.6382,  ...,  0.9876,  1.3423,  0.5428],\n",
      "          [-0.8295, -1.1071,  1.2423,  ...,  0.0537, -0.4261,  0.8158],\n",
      "          [ 0.0238,  0.0187,  0.3870,  ..., -1.4680,  0.3266, -0.0111],\n",
      "          ...,\n",
      "          [-0.3771,  0.5446,  1.8161,  ...,  0.5968, -1.2329,  0.0325],\n",
      "          [-1.0334, -0.0641, -0.1874,  ..., -0.6634, -0.6385, -0.4785],\n",
      "          [ 1.1764,  0.5247,  0.7442,  ...,  0.8770,  0.1319, -0.3058]],\n",
      "\n",
      "         [[-0.1252,  1.4410,  0.9017,  ...,  0.2556,  1.0272, -0.2598],\n",
      "          [ 0.2635,  0.9396, -0.2733,  ..., -0.3872,  1.0957, -1.3776],\n",
      "          [-0.3586,  1.7018, -0.8335,  ..., -2.0465,  1.1816,  0.5955],\n",
      "          ...,\n",
      "          [-0.5616, -0.6389, -0.8998,  ...,  0.0674, -0.1601,  1.6319],\n",
      "          [-0.4514,  0.6065, -0.5904,  ..., -0.0951, -0.4552,  1.8974],\n",
      "          [-0.2615, -1.1907, -0.3493,  ...,  0.1513, -0.8341, -0.1962]],\n",
      "\n",
      "         [[ 0.5387, -0.1873, -0.3129,  ...,  0.0514, -0.8812, -0.1566],\n",
      "          [-0.1119, -1.7238, -0.2912,  ...,  0.9898, -1.5354,  0.1751],\n",
      "          [ 0.0041,  0.1068,  0.3416,  ...,  0.0829, -1.5208, -0.6464],\n",
      "          ...,\n",
      "          [ 0.5925,  0.9936,  0.2095,  ..., -0.6645,  1.3712,  0.2904],\n",
      "          [-0.2313,  1.0972, -1.8082,  ...,  0.6864, -0.1910,  0.1683],\n",
      "          [-0.0717,  0.1626,  0.0124,  ..., -0.5204,  0.2968, -0.1452]]]],\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output_before_fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.2322, -0.3684, -0.4932,  ..., -0.4260, -0.1541, -0.1844],\n",
      "          [ 0.1428,  0.1274,  0.3775,  ...,  0.4430,  0.4741,  0.0903],\n",
      "          [-0.6471,  0.1056,  0.5763,  ..., -0.2120,  0.1770,  0.7788],\n",
      "          ...,\n",
      "          [-0.2678,  0.4476,  0.9848,  ..., -0.2145, -0.3979,  0.0255],\n",
      "          [ 0.0486, -0.3783,  0.4589,  ...,  0.2719, -0.9361, -0.1356],\n",
      "          [ 0.0993,  0.3154, -0.0988,  ..., -0.1468, -0.1298,  0.2216]],\n",
      "\n",
      "         [[ 0.8490,  0.5423, -0.2886,  ..., -0.0532,  0.5968, -0.2394],\n",
      "          [-0.8648, -0.2494, -0.4778,  ..., -0.5494, -0.0153,  0.1032],\n",
      "          [-0.2077, -0.1638,  1.5140,  ...,  0.3477, -0.4872, -0.5472],\n",
      "          ...,\n",
      "          [ 0.2923,  1.4895,  0.6715,  ...,  1.3371,  0.0518,  0.4087],\n",
      "          [ 0.0319, -1.0857, -0.3978,  ..., -0.4829, -0.1027, -0.5089],\n",
      "          [ 0.0537,  0.2329, -0.3806,  ...,  0.5265,  0.9747, -0.4851]],\n",
      "\n",
      "         [[-0.0990,  0.0130,  0.3966,  ..., -0.5533,  0.3161, -0.2869],\n",
      "          [-0.2754, -0.5987, -0.2802,  ..., -0.3362, -1.0933, -0.2280],\n",
      "          [-0.2333,  0.0309, -0.1369,  ...,  0.1523, -1.5255, -0.4768],\n",
      "          ...,\n",
      "          [-0.2189,  0.8626,  1.4877,  ..., -0.3602,  0.1350,  0.2540],\n",
      "          [ 0.5331, -0.1323, -0.4680,  ...,  1.1622, -0.5801, -0.2799],\n",
      "          [ 0.2159,  0.7944,  0.6848,  ...,  0.9246,  1.2360,  0.4879]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3908,  1.3232,  0.4129,  ...,  0.6436,  0.8776,  0.3500],\n",
      "          [-0.5557, -0.7390,  0.8117,  ...,  0.0271, -0.2895,  0.5301],\n",
      "          [ 0.0075,  0.0040,  0.2471,  ..., -0.9771,  0.2073, -0.0156],\n",
      "          ...,\n",
      "          [-0.2572,  0.3512,  1.1904,  ...,  0.3856, -0.8220,  0.0132],\n",
      "          [-0.6903, -0.0506, -0.1320,  ..., -0.4461, -0.4297, -0.3241],\n",
      "          [ 0.7682,  0.3380,  0.4829,  ...,  0.5705,  0.0788, -0.2101]],\n",
      "\n",
      "         [[-0.0906,  0.9307,  0.5790,  ...,  0.1577,  0.6609, -0.1784],\n",
      "          [ 0.1628,  0.6038, -0.1872,  ..., -0.2614,  0.7055, -0.9073],\n",
      "          [-0.2428,  1.1008, -0.5525,  ..., -1.3435,  0.7616,  0.3794],\n",
      "          ...,\n",
      "          [-0.3752, -0.4256, -0.5958,  ...,  0.0350, -0.1134,  1.0552],\n",
      "          [-0.3033,  0.3865, -0.3940,  ..., -0.0710, -0.3058,  1.2283],\n",
      "          [-0.1795, -0.7855, -0.2367,  ...,  0.0897, -0.5529, -0.1369]],\n",
      "\n",
      "         [[ 0.3335, -0.1123, -0.1895,  ...,  0.0343, -0.5384, -0.0935],\n",
      "          [-0.0660, -1.0559, -0.1761,  ...,  0.6106, -0.9402,  0.1103],\n",
      "          [ 0.0052,  0.0683,  0.2125,  ...,  0.0536, -0.9312, -0.3943],\n",
      "          ...,\n",
      "          [ 0.3666,  0.6129,  0.1314,  ..., -0.4054,  0.8448,  0.1811],\n",
      "          [-0.1393,  0.6765, -1.1078,  ...,  0.4243, -0.1146,  0.1061],\n",
      "          [-0.0413,  0.1026,  0.0103,  ..., -0.3169,  0.1850, -0.0865]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output_after_fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0116567611694336e-07\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "import unittest\n",
    "from unittest import TestCase\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BackboneModel(nn.Module):\n",
    "\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(6, 6, 6)\n",
    "        self.bn1 = nn.BatchNorm2d(6)\n",
    "        self.conv2 = nn.Conv2d(6, 6, 6)\n",
    "        self.bn2 = nn.BatchNorm2d(6)\n",
    "        self.conv3 = nn.Conv2d(6, 6, 6)\n",
    "        self.bn3 = nn.BatchNorm2d(6)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # this conv-bn pair can use efficient_conv_bn_eval feature\n",
    "        x = self.bn1(self.conv1(x))\n",
    "        # this conv-bn pair can use efficient_conv_bn_eval feature\n",
    "        # only for the second `self.conv2` call.\n",
    "        x = self.bn2(self.conv2(self.conv2(x)))\n",
    "        # this conv-bn pair can use efficient_conv_bn_eval feature\n",
    "        # just for the first forward of the `self.bn3`\n",
    "        x = self.bn3(self.bn3(self.conv3(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = BackboneModel()\n",
    "model.eval()\n",
    "input = torch.randn(64, 6, 32, 32)\n",
    "output = model(input)\n",
    "turn_on_efficient_conv_bn_eval_for_single_model(model)\n",
    "output2 = model(input)\n",
    "print((output - output2).abs().max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output before fusion:  torch.Size([1, 16, 32, 32])\n",
      "Output after fusion:  torch.Size([1, 16, 32, 32])\n",
      "Are the outputs close?  True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvBn(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', deploy=False):\n",
    "        super(ConvBn, self).__init__()\n",
    "        self.deploy = deploy\n",
    "        if deploy:\n",
    "            self.fused_conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(kernel_size,kernel_size), stride=stride,\n",
    "                                      padding=padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode)\n",
    "        else:\n",
    "            self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                         kernel_size=(kernel_size, kernel_size), stride=stride,\n",
    "                                         padding=padding, dilation=dilation, groups=groups, bias=False,\n",
    "                                         padding_mode=padding_mode)\n",
    "            self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
    "\n",
    "\n",
    "    def _fuse_bn_tensor(self, conv, bn):\n",
    "        std = (bn.running_var + bn.eps).sqrt()\n",
    "        t = (bn.weight / std).reshape(-1, 1, 1, 1)\n",
    "        return conv.weight * t, bn.bias - bn.running_mean * bn.weight / std\n",
    "\n",
    "\n",
    "    def switch_to_deploy(self):\n",
    "        if self.bn.training:\n",
    "            raise RuntimeError(\"BatchNorm should be in evaluation mode (eval) before deployment.\")\n",
    "        deploy_k, deploy_b = self._fuse_bn_tensor(self.conv, self.bn)\n",
    "        self.deploy = True\n",
    "        self.fused_conv = nn.Conv2d(in_channels=self.conv.in_channels, out_channels=self.conv.out_channels,\n",
    "                                    kernel_size=self.conv.kernel_size, stride=self.conv.stride,\n",
    "                                    padding=self.conv.padding, dilation=self.conv.dilation, groups=self.conv.groups, bias=True,\n",
    "                                    padding_mode=self.conv.padding_mode)\n",
    "        self.__delattr__('conv')\n",
    "        self.__delattr__('bn')\n",
    "        self.fused_conv.weight.data = deploy_k\n",
    "        self.fused_conv.bias.data = deploy_b\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.deploy:\n",
    "            return self.fused_conv(input)\n",
    "        else:\n",
    "            square_outputs = self.conv(input)\n",
    "            square_outputs = self.bn(square_outputs)\n",
    "            return square_outputs\n",
    "\n",
    "\n",
    "conv_bn_layer = ConvBn(3, 16, 3, 1, 1).eval()\n",
    "input_tensor = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "output_before_fusion = conv_bn_layer(input_tensor)\n",
    "conv_bn_layer.switch_to_deploy()\n",
    "output_after_fusion = conv_bn_layer(input_tensor)\n",
    "\n",
    "print(\"Output before fusion: \", output_before_fusion.shape)\n",
    "print(\"Output after fusion: \", output_after_fusion.shape)\n",
    "print(\"Are the outputs close? \", torch.allclose(output_before_fusion, output_after_fusion, atol=1e-6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of conv.weight: 2.000000\n",
      "Mean of bn.weight: 3.000000\n",
      "Mean of bn.bias: 1.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from model.base_module import BaseModule\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class ConvBn(BaseModule):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups=1, init_cfg=None):\n",
    "        super().__init__(init_cfg)\n",
    "        self.in_channels = in_channels\n",
    "        self.groups = groups\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, groups=groups, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'rbr_reparam'):\n",
    "            return self.rbr_reparam(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "init_cfg = [\n",
    "    dict(type='Constant',val=2,layer='Conv2d'),\n",
    "    dict(type='Constant',override=dict(name='bn'),val =3,bias=1), \n",
    "]\n",
    "# Example of usage:\n",
    "conv_bn_layer = ConvBn(3, 16, 3, 1, 1,init_cfg = init_cfg )\n",
    "\n",
    "# Define the initialization configuration for Conv2d and BatchNorm2d\n",
    "  # Example using Xavier initialization for Conv2d\n",
    "\n",
    "# Initialize weights\n",
    "conv_bn_layer.init_weights()\n",
    "\n",
    "# Create a random input tensor\n",
    "input_tensor = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "# Forward pass to see the output\n",
    "output = conv_bn_layer(input_tensor)\n",
    "\n",
    "# Print initialized weights of Conv2d and BatchNorm2d\n",
    "# 打印每一层的权重均值和偏置\n",
    "param_mean = {name: p.mean().item() for name, p in conv_bn_layer.named_parameters()}\n",
    "for name, mean in param_mean.items():\n",
    "    print(f\"Mean of {name}: {mean:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.19.8-py3-none-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting click!=8.0.0,>=7.1 (from wandb)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from wandb) (3.10.0)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 (from wandb)\n",
      "  Downloading protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from wandb) (5.9.0)\n",
      "Collecting pydantic<3,>=2.6 (from wandb)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.24.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.5-cp310-cp310-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from wandb) (75.8.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=2.6->wandb)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=2.6->wandb)\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading wandb-0.19.8-py3-none-macosx_11_0_arm64.whl (19.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Downloading protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl (417 kB)\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp310-cp310-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-2.24.0-py2.py3-none-any.whl (336 kB)\n",
      "Downloading setproctitle-1.3.5-cp310-cp310-macosx_11_0_arm64.whl (11 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, setproctitle, sentry-sdk, pydantic-core, protobuf, docker-pycreds, click, annotated-types, pydantic, gitdb, gitpython, wandb\n",
      "Successfully installed annotated-types-0.7.0 click-8.1.8 docker-pycreds-0.4.0 gitdb-4.0.12 gitpython-3.1.44 protobuf-5.29.4 pydantic-2.10.6 pydantic-core-2.27.2 sentry-sdk-2.24.0 setproctitle-1.3.5 smmap-5.0.2 wandb-0.19.8\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: model\n",
      "Loading module: model.backbone\n",
      "Loading module: model.efficient_conv_bn_eval\n",
      "Loading module: model.resnet\n",
      "Loading module: model.utils\n",
      "Unexpected error while loading module model.utils: name 'nn' is not defined\n",
      "Loading module: model.weight_url\n",
      "Loading module: model.base_module\n",
      "Loading module: model.weight_init\n",
      "Processing directory: model/__pycache__\n",
      "Processing directory: model/segmentor\n",
      "Loading module: model.segmentor.encoderdecoder\n",
      "ModuleNotFoundError: No module named 'mmengine'\n",
      "Loading module: model.segmentor.segmentor\n",
      "Unexpected error while loading module model.segmentor.segmentor: name 'Tensor' is not defined\n",
      "Processing directory: model/segmentor/__pycache__\n",
      "Mean of conv1.weight: 0.000029\n",
      "Mean of bn1.weight: 0.257577\n",
      "Mean of bn1.bias: 0.181120\n",
      "Mean of layer1.0.conv1.weight: -0.003087\n",
      "Mean of layer1.0.bn1.weight: 0.339601\n",
      "Mean of layer1.0.bn1.bias: -0.034137\n",
      "Mean of layer1.0.conv2.weight: -0.000889\n",
      "Mean of layer1.0.bn2.weight: 0.333055\n",
      "Mean of layer1.0.bn2.bias: 0.003463\n",
      "Mean of layer1.1.conv1.weight: -0.002420\n",
      "Mean of layer1.1.bn1.weight: 0.328692\n",
      "Mean of layer1.1.bn1.bias: -0.083574\n",
      "Mean of layer1.1.conv2.weight: -0.001260\n",
      "Mean of layer1.1.bn2.weight: 0.392430\n",
      "Mean of layer1.1.bn2.bias: -0.029984\n",
      "Mean of layer2.0.conv1.weight: -0.001454\n",
      "Mean of layer2.0.bn1.weight: 0.316419\n",
      "Mean of layer2.0.bn1.bias: -0.067346\n",
      "Mean of layer2.0.conv2.weight: -0.001248\n",
      "Mean of layer2.0.bn2.weight: 0.327573\n",
      "Mean of layer2.0.bn2.bias: -0.003555\n",
      "Mean of layer2.0.downsample.0.weight: -0.002588\n",
      "Mean of layer2.0.downsample.1.weight: 0.195085\n",
      "Mean of layer2.0.downsample.1.bias: -0.003555\n",
      "Mean of layer2.1.conv1.weight: -0.001530\n",
      "Mean of layer2.1.bn1.weight: 0.321264\n",
      "Mean of layer2.1.bn1.bias: -0.210250\n",
      "Mean of layer2.1.conv2.weight: -0.001272\n",
      "Mean of layer2.1.bn2.weight: 0.282911\n",
      "Mean of layer2.1.bn2.bias: -0.151284\n",
      "Mean of layer3.0.conv1.weight: -0.001368\n",
      "Mean of layer3.0.bn1.weight: 0.312275\n",
      "Mean of layer3.0.bn1.bias: -0.114786\n",
      "Mean of layer3.0.conv2.weight: -0.000787\n",
      "Mean of layer3.0.bn2.weight: 0.320250\n",
      "Mean of layer3.0.bn2.bias: -0.030766\n",
      "Mean of layer3.0.downsample.0.weight: -0.001896\n",
      "Mean of layer3.0.downsample.1.weight: 0.082104\n",
      "Mean of layer3.0.downsample.1.bias: -0.030766\n",
      "Mean of layer3.1.conv1.weight: -0.001662\n",
      "Mean of layer3.1.bn1.weight: 0.278211\n",
      "Mean of layer3.1.bn1.bias: -0.237468\n",
      "Mean of layer3.1.conv2.weight: -0.001441\n",
      "Mean of layer3.1.bn2.weight: 0.245850\n",
      "Mean of layer3.1.bn2.bias: -0.163723\n",
      "Mean of layer4.0.conv1.weight: -0.001565\n",
      "Mean of layer4.0.bn1.weight: 0.264318\n",
      "Mean of layer4.0.bn1.bias: -0.225722\n",
      "Mean of layer4.0.conv2.weight: -0.001303\n",
      "Mean of layer4.0.bn2.weight: 0.424319\n",
      "Mean of layer4.0.bn2.bias: -0.197634\n",
      "Mean of layer4.0.downsample.0.weight: -0.000843\n",
      "Mean of layer4.0.downsample.1.weight: 0.250642\n",
      "Mean of layer4.0.downsample.1.bias: -0.197634\n",
      "Mean of layer4.1.conv1.weight: -0.002261\n",
      "Mean of layer4.1.bn1.weight: 0.288600\n",
      "Mean of layer4.1.bn1.bias: -0.241740\n",
      "Mean of layer4.1.conv2.weight: -0.000108\n",
      "Mean of layer4.1.bn2.weight: 1.853810\n",
      "Mean of layer4.1.bn2.bias: 0.273822\n"
     ]
    }
   ],
   "source": [
    "from apps.builder import make_model\n",
    "from apps.registry import register_dir\n",
    "\n",
    "\n",
    "register_dir('model')\n",
    "model =  make_model(dict(type='resnet18',pretrained =True,dataset='imagenet1k_v1'))\n",
    "model.init_weights()\n",
    "param_mean = {name: p.mean().item() for name, p in model.named_parameters()}\n",
    "for name, mean in param_mean.items():\n",
    "    print(f\"Mean of {name}: {mean:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBn(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', deploy=False):\n",
    "        super(ConvBn, self).__init__()\n",
    "        self.deploy = deploy\n",
    "        if deploy:\n",
    "            self.fused_conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(kernel_size,kernel_size), stride=stride,\n",
    "                                      padding=padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode)\n",
    "        else:\n",
    "            self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                         kernel_size=(kernel_size, kernel_size), stride=stride,\n",
    "                                         padding=padding, dilation=dilation, groups=groups, bias=False,\n",
    "                                         padding_mode=padding_mode)\n",
    "            self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
    "\n",
    "\n",
    "    def _fuse_bn_tensor(self, conv, bn):\n",
    "        std = (bn.running_var + bn.eps).sqrt()\n",
    "        t = (bn.weight / std).reshape(-1, 1, 1, 1)\n",
    "        return conv.weight * t, bn.bias - bn.running_mean * bn.weight / std\n",
    "\n",
    "\n",
    "    def switch_to_deploy(self):\n",
    "        deploy_k, deploy_b = self._fuse_bn_tensor(self.conv, self.bn)\n",
    "        self.deploy = True\n",
    "        self.fused_conv = nn.Conv2d(in_channels=self.conv.in_channels, out_channels=self.conv.out_channels,\n",
    "                                    kernel_size=self.conv.kernel_size, stride=self.conv.stride,\n",
    "                                    padding=self.conv.padding, dilation=self.conv.dilation, groups=self.conv.groups, bias=True,\n",
    "                                    padding_mode=self.conv.padding_mode)\n",
    "        self.__delattr__('conv')\n",
    "        self.__delattr__('bn')\n",
    "        self.fused_conv.weight.data = deploy_k\n",
    "        self.fused_conv.bias.data = deploy_b\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.deploy:\n",
    "            return self.fused_conv(input)\n",
    "        else:\n",
    "            square_outputs = self.conv(input)\n",
    "            square_outputs = self.bn(square_outputs)\n",
    "            return square_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class RepConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_sizes=(3, 7, 11), stride=1, padding=None, dilation=1, deploy=False, groups=1,use_bn):\n",
    "        \"\"\"\n",
    "        Reparameterized convolution module for Conv2d, with an interface consistent with nn.Conv2d.\n",
    "        :param in_channels: Number of input channels\n",
    "        :param out_channels: Number of output channels\n",
    "        :param kernel_sizes: List of kernel sizes\n",
    "        :param stride: Stride of the convolution\n",
    "        :param padding: Padding size; if None, automatically computes \"same\" padding\n",
    "        :param dilation: Dilation rate\n",
    "        :param bias: Whether to include a bias term\n",
    "        :param use_identity: Whether to use identity mapping (only effective when in_channels == out_channels and groups == 1)\n",
    "        :param groups: Number of groups for group convolution\n",
    "        \"\"\"\n",
    "        super(RepConv2d, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_sizes = sorted(kernel_sizes)\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "\n",
    "\n",
    "        self.max_kernel_size = max(self.kernel_sizes)\n",
    "        self.padding = padding if padding is not None else (self.max_kernel_size - 1) // 2 * dilation\n",
    "        if use_bn:\n",
    "            self.convs = nn.ModuleList([\n",
    "            ConvBn(in_channels, out_channels, k, stride=stride, dilation=dilation,\n",
    "                      padding=(k - 1) // 2 * dilation, bias=False, groups=groups)\n",
    "            for k in self.kernel_sizes\n",
    "            ])\n",
    "        else:\n",
    "            self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels, out_channels, k, stride=stride, dilation=dilation,\n",
    "                      padding=(k - 1) // 2 * dilation, bias=False, groups=groups)\n",
    "            for k in self.kernel_sizes\n",
    "            ])\n",
    "        self.deploy = deploy \n",
    "        if deploy:\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.reparameterized:\n",
    "            return self.rbr_reparam(\n",
    "                x\n",
    "            )\n",
    "        else:\n",
    "            conv_outputs = []\n",
    "            for conv in self.convs:\n",
    "                conv_outputs.append(conv(x))\n",
    "            if self.use_identity:\n",
    "                return sum(conv_outputs) / len(self.convs) + x\n",
    "            else:\n",
    "                return sum(conv_outputs) / len(self.convs)\n",
    "\n",
    "    def _convert_weight_and_bias(self):\n",
    "        weight = self.convs[-1].weight\n",
    "        bias = self.convs[-1].bias if self.convs[-1].bias is not None else torch.zeros(self.out_channels, device=weight.device)\n",
    "\n",
    "        for conv in self.convs[:-1]:\n",
    "            pad = (self.max_kernel_size - conv.weight.shape[-1]) // 2\n",
    "            weight = weight + F.pad(conv.weight, [pad, pad, pad, pad])\n",
    "            conv_bias = conv.bias if conv.bias is not None else torch.zeros(self.out_channels, device=weight.device)\n",
    "            bias = bias + conv_bias\n",
    "\n",
    "        weight = weight / len(self.convs)\n",
    "        bias = bias / len(self.convs)\n",
    "\n",
    "        if self.use_identity:\n",
    "            pad = (self.max_kernel_size - 1) // 2\n",
    "            identity_weight = F.pad(\n",
    "                torch.eye(self.out_channels, self.in_channels // self.groups).unsqueeze(-1).unsqueeze(-1).to(weight.device),\n",
    "                [pad, pad, pad, pad]\n",
    "            ).repeat(1, self.groups, 1, 1).reshape(self.out_channels, self.in_channels // self.groups, self.max_kernel_size, self.max_kernel_size)\n",
    "            weight = weight + identity_weight\n",
    "\n",
    "        self.weight = weight.detach()\n",
    "        self.bias = bias.detach()\n",
    "\n",
    "    def switch_to_deploy(self):\n",
    "        if hasattr(self, 'rbr_reparam'):\n",
    "            return\n",
    "        self._convert_weight_and_bias()\n",
    "        self.rbr_reparam = nn.Conv2d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=(self.max_kernel_size, self.max_kernel_size),\n",
    "            stride=self.stride,\n",
    "            padding=self.padding,\n",
    "            dilation=self.dilation,\n",
    "            groups=self.groups,\n",
    "            bias=True\n",
    "        )\n",
    "        self.rbr_reparam.weight.data = self.weight\n",
    "        self.rbr_reparam.bias.data = self.bias\n",
    "        del self.convs\n",
    "        self.reparameterized = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize the RepConv2d model\n",
    "model = RepConv2d(in_channels=3, out_channels=16, kernel_sizes=(3, 5, 7), stride=1).eval()\n",
    "\n",
    "# Create a dummy input tensor\n",
    "x = torch.randn(1, 3, 224, 224)  # Batch size 1, 3 channels, 224x224 image\n",
    "\n",
    "# Run forward pass before deployment (pre-deployment)\n",
    "output_before_deploy = model(x)\n",
    "\n",
    "# Deploy the model\n",
    "model.switch_to_deploy()\n",
    "\n",
    "# Run forward pass after deployment (post-deployment)\n",
    "output_after_deploy = model(x)\n",
    "\n",
    "# Compare the outputs\n",
    "print(torch.allclose(output_before_deploy, output_after_deploy))  # Should be True if the outputs are the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.0509e-01, -1.6618e-01,  2.3734e-02,  ...,  4.3876e-02,\n",
       "           -2.7332e-01,  7.8780e-02],\n",
       "          [-4.3497e-02, -2.1642e-01,  1.4031e-01,  ...,  1.1755e-01,\n",
       "           -2.8986e-01, -7.6739e-02],\n",
       "          [ 1.0535e-01, -2.4501e-01, -9.3907e-02,  ...,  1.5210e-01,\n",
       "            1.1109e-01, -3.8096e-02],\n",
       "          ...,\n",
       "          [-4.9162e-02,  1.0591e-01,  2.6854e-01,  ...,  3.2434e-01,\n",
       "            1.1938e-01,  1.5615e-01],\n",
       "          [ 1.6362e-01,  3.3904e-01,  1.2398e-01,  ..., -1.7295e-01,\n",
       "            1.4540e-01,  1.7867e-03],\n",
       "          [ 3.7581e-02,  3.7735e-01,  7.6096e-02,  ...,  3.2586e-01,\n",
       "            5.5626e-01, -5.7446e-02]],\n",
       "\n",
       "         [[-1.8109e-01,  3.9394e-02,  3.0771e-01,  ...,  8.3651e-02,\n",
       "           -3.2509e-01, -1.4123e-01],\n",
       "          [ 7.4080e-02, -2.8459e-01, -5.4809e-02,  ...,  4.1137e-02,\n",
       "            1.3135e-01,  3.6649e-01],\n",
       "          [-9.8002e-02,  4.2682e-01,  2.3538e-03,  ...,  3.5936e-01,\n",
       "            2.5884e-01, -3.5275e-01],\n",
       "          ...,\n",
       "          [-7.6958e-02,  1.1001e-01, -2.1666e-01,  ...,  2.5912e-01,\n",
       "            4.6311e-01,  1.0677e-01],\n",
       "          [-1.2450e-01,  5.7921e-01, -3.4372e-01,  ...,  1.1061e-01,\n",
       "           -2.1486e-01,  5.7938e-02],\n",
       "          [-3.8688e-02, -2.2093e-02,  5.6106e-01,  ...,  1.6838e-01,\n",
       "            1.5817e-01,  2.3281e-01]],\n",
       "\n",
       "         [[-1.0894e-02,  1.8954e-01, -3.5832e-02,  ...,  4.6218e-01,\n",
       "           -4.1146e-01, -9.6226e-02],\n",
       "          [ 2.4744e-01, -3.8143e-01, -5.9170e-01,  ...,  4.8771e-02,\n",
       "            8.9603e-02, -1.9898e-01],\n",
       "          [ 3.0614e-01,  1.8092e-01, -2.9465e-02,  ..., -3.3937e-01,\n",
       "           -8.9875e-02,  8.9298e-02],\n",
       "          ...,\n",
       "          [-1.0664e-01,  3.8526e-01,  3.2055e-01,  ..., -1.9398e-01,\n",
       "           -4.5279e-02,  3.9769e-01],\n",
       "          [-2.9203e-01,  3.3534e-01,  6.0844e-01,  ...,  1.9899e-01,\n",
       "           -4.3176e-01, -2.6826e-01],\n",
       "          [-1.3407e-01, -3.3269e-01, -3.4315e-02,  ..., -4.4963e-01,\n",
       "           -4.9919e-01,  6.1906e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.9034e-01,  7.3505e-02,  8.6552e-02,  ..., -3.4830e-01,\n",
       "           -2.7727e-01,  2.8850e-02],\n",
       "          [ 2.6271e-01,  9.1909e-02,  1.6791e-02,  ...,  3.8670e-02,\n",
       "            6.3160e-01,  1.3287e-01],\n",
       "          [-3.0033e-01,  3.3922e-01,  1.9802e-01,  ..., -5.4369e-01,\n",
       "            8.5222e-02, -1.2343e-01],\n",
       "          ...,\n",
       "          [-5.8591e-02,  2.4301e-01,  1.7809e-04,  ...,  6.0583e-01,\n",
       "            1.7878e-01, -8.1960e-02],\n",
       "          [ 3.0094e-01, -7.7375e-01, -3.3122e-01,  ..., -2.4812e-02,\n",
       "           -4.7006e-01,  2.2782e-01],\n",
       "          [ 6.1376e-02,  1.0328e-01,  8.7527e-02,  ...,  2.0196e-02,\n",
       "           -3.3378e-02, -6.3306e-02]],\n",
       "\n",
       "         [[-1.2842e-01, -2.0225e-02, -4.7466e-02,  ...,  1.6582e-01,\n",
       "            2.0710e-01,  7.5919e-02],\n",
       "          [-3.6354e-01, -2.0423e-01, -1.0628e-01,  ..., -2.2852e-01,\n",
       "           -7.3149e-01, -3.8529e-01],\n",
       "          [-4.9332e-02, -2.3332e-01, -3.1077e-01,  ..., -6.2605e-01,\n",
       "           -3.4752e-01, -2.8183e-02],\n",
       "          ...,\n",
       "          [-5.3266e-01, -4.1405e-01, -1.0516e-01,  ..., -5.0515e-01,\n",
       "           -2.2890e-02, -1.7347e-01],\n",
       "          [-9.9413e-02,  4.9966e-01, -9.5683e-02,  ...,  2.5346e-01,\n",
       "           -2.1047e-01,  2.3567e-02],\n",
       "          [ 3.1842e-01,  2.4698e-01,  1.3175e-01,  ...,  7.1634e-02,\n",
       "           -2.2469e-01, -6.1062e-01]],\n",
       "\n",
       "         [[-8.3371e-02,  3.0713e-01,  1.3768e-02,  ...,  2.8686e-01,\n",
       "            1.6393e-01, -2.3217e-01],\n",
       "          [-8.0951e-02,  1.4463e-01, -2.5413e-01,  ..., -3.6405e-01,\n",
       "           -3.7708e-01,  8.5218e-01],\n",
       "          [-6.1974e-02, -9.5626e-02,  1.3724e-01,  ...,  1.3480e-01,\n",
       "            7.7468e-03, -3.9745e-01],\n",
       "          ...,\n",
       "          [ 1.4236e-01,  5.3777e-02,  1.9422e-01,  ..., -1.5561e-01,\n",
       "            4.6266e-02,  3.1290e-01],\n",
       "          [ 1.8462e-01,  1.3874e-01, -3.3064e-01,  ...,  3.0305e-01,\n",
       "           -4.4506e-01, -2.1772e-01],\n",
       "          [ 5.7041e-01, -1.1243e-01, -9.6366e-02,  ..., -1.4998e-01,\n",
       "            8.7994e-02,  6.3286e-01]]]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_before_deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.0509e-01, -1.6618e-01,  2.3734e-02,  ...,  4.3876e-02,\n",
       "           -2.7332e-01,  7.8780e-02],\n",
       "          [-4.3497e-02, -2.1642e-01,  1.4031e-01,  ...,  1.1755e-01,\n",
       "           -2.8986e-01, -7.6739e-02],\n",
       "          [ 1.0535e-01, -2.4501e-01, -9.3907e-02,  ...,  1.5210e-01,\n",
       "            1.1109e-01, -3.8096e-02],\n",
       "          ...,\n",
       "          [-4.9162e-02,  1.0591e-01,  2.6854e-01,  ...,  3.2434e-01,\n",
       "            1.1938e-01,  1.5615e-01],\n",
       "          [ 1.6362e-01,  3.3904e-01,  1.2398e-01,  ..., -1.7295e-01,\n",
       "            1.4540e-01,  1.7867e-03],\n",
       "          [ 3.7581e-02,  3.7735e-01,  7.6096e-02,  ...,  3.2586e-01,\n",
       "            5.5626e-01, -5.7446e-02]],\n",
       "\n",
       "         [[-1.8109e-01,  3.9394e-02,  3.0771e-01,  ...,  8.3651e-02,\n",
       "           -3.2509e-01, -1.4123e-01],\n",
       "          [ 7.4080e-02, -2.8459e-01, -5.4809e-02,  ...,  4.1137e-02,\n",
       "            1.3135e-01,  3.6649e-01],\n",
       "          [-9.8002e-02,  4.2682e-01,  2.3537e-03,  ...,  3.5936e-01,\n",
       "            2.5884e-01, -3.5275e-01],\n",
       "          ...,\n",
       "          [-7.6958e-02,  1.1001e-01, -2.1666e-01,  ...,  2.5912e-01,\n",
       "            4.6311e-01,  1.0677e-01],\n",
       "          [-1.2450e-01,  5.7921e-01, -3.4372e-01,  ...,  1.1061e-01,\n",
       "           -2.1486e-01,  5.7938e-02],\n",
       "          [-3.8689e-02, -2.2093e-02,  5.6106e-01,  ...,  1.6838e-01,\n",
       "            1.5817e-01,  2.3281e-01]],\n",
       "\n",
       "         [[-1.0894e-02,  1.8954e-01, -3.5832e-02,  ...,  4.6218e-01,\n",
       "           -4.1146e-01, -9.6226e-02],\n",
       "          [ 2.4744e-01, -3.8143e-01, -5.9170e-01,  ...,  4.8772e-02,\n",
       "            8.9603e-02, -1.9898e-01],\n",
       "          [ 3.0614e-01,  1.8092e-01, -2.9465e-02,  ..., -3.3937e-01,\n",
       "           -8.9876e-02,  8.9298e-02],\n",
       "          ...,\n",
       "          [-1.0664e-01,  3.8526e-01,  3.2055e-01,  ..., -1.9398e-01,\n",
       "           -4.5279e-02,  3.9769e-01],\n",
       "          [-2.9203e-01,  3.3534e-01,  6.0844e-01,  ...,  1.9899e-01,\n",
       "           -4.3176e-01, -2.6826e-01],\n",
       "          [-1.3407e-01, -3.3269e-01, -3.4315e-02,  ..., -4.4963e-01,\n",
       "           -4.9919e-01,  6.1906e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.9034e-01,  7.3505e-02,  8.6552e-02,  ..., -3.4830e-01,\n",
       "           -2.7727e-01,  2.8850e-02],\n",
       "          [ 2.6271e-01,  9.1909e-02,  1.6791e-02,  ...,  3.8670e-02,\n",
       "            6.3160e-01,  1.3287e-01],\n",
       "          [-3.0033e-01,  3.3922e-01,  1.9802e-01,  ..., -5.4369e-01,\n",
       "            8.5221e-02, -1.2343e-01],\n",
       "          ...,\n",
       "          [-5.8591e-02,  2.4301e-01,  1.7799e-04,  ...,  6.0583e-01,\n",
       "            1.7878e-01, -8.1960e-02],\n",
       "          [ 3.0094e-01, -7.7375e-01, -3.3122e-01,  ..., -2.4812e-02,\n",
       "           -4.7006e-01,  2.2782e-01],\n",
       "          [ 6.1376e-02,  1.0328e-01,  8.7527e-02,  ...,  2.0196e-02,\n",
       "           -3.3378e-02, -6.3306e-02]],\n",
       "\n",
       "         [[-1.2842e-01, -2.0225e-02, -4.7466e-02,  ...,  1.6582e-01,\n",
       "            2.0710e-01,  7.5919e-02],\n",
       "          [-3.6354e-01, -2.0423e-01, -1.0628e-01,  ..., -2.2852e-01,\n",
       "           -7.3149e-01, -3.8529e-01],\n",
       "          [-4.9332e-02, -2.3332e-01, -3.1077e-01,  ..., -6.2605e-01,\n",
       "           -3.4752e-01, -2.8183e-02],\n",
       "          ...,\n",
       "          [-5.3266e-01, -4.1405e-01, -1.0516e-01,  ..., -5.0515e-01,\n",
       "           -2.2890e-02, -1.7347e-01],\n",
       "          [-9.9413e-02,  4.9966e-01, -9.5683e-02,  ...,  2.5346e-01,\n",
       "           -2.1047e-01,  2.3567e-02],\n",
       "          [ 3.1842e-01,  2.4698e-01,  1.3175e-01,  ...,  7.1634e-02,\n",
       "           -2.2469e-01, -6.1062e-01]],\n",
       "\n",
       "         [[-8.3371e-02,  3.0713e-01,  1.3768e-02,  ...,  2.8686e-01,\n",
       "            1.6393e-01, -2.3217e-01],\n",
       "          [-8.0951e-02,  1.4463e-01, -2.5413e-01,  ..., -3.6405e-01,\n",
       "           -3.7708e-01,  8.5218e-01],\n",
       "          [-6.1974e-02, -9.5626e-02,  1.3724e-01,  ...,  1.3480e-01,\n",
       "            7.7468e-03, -3.9745e-01],\n",
       "          ...,\n",
       "          [ 1.4236e-01,  5.3777e-02,  1.9422e-01,  ..., -1.5561e-01,\n",
       "            4.6266e-02,  3.1290e-01],\n",
       "          [ 1.8462e-01,  1.3874e-01, -3.3064e-01,  ...,  3.0305e-01,\n",
       "           -4.4506e-01, -2.1772e-01],\n",
       "          [ 5.7041e-01, -1.1243e-01, -9.6366e-02,  ..., -1.4998e-01,\n",
       "            8.7994e-02,  6.3286e-01]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_after_deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vistar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
