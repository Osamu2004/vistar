{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "from operator import attrgetter\n",
    "from typing import List, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def efficient_conv_bn_eval_forward(bn: nn.modules.batchnorm._BatchNorm,\n",
    "                                   conv: nn.modules.conv._ConvNd,\n",
    "                                   x: torch.Tensor):\n",
    "    \"\"\"Code borrowed from mmcv 2.0.1, so that this feature can be used for old\n",
    "    mmcv versions.\n",
    "\n",
    "    Implementation based on https://arxiv.org/abs/2305.11624\n",
    "    \"Tune-Mode ConvBN Blocks For Efficient Transfer Learning\"\n",
    "    It leverages the associative law between convolution and affine transform,\n",
    "    i.e., normalize (weight conv feature) = (normalize weight) conv feature.\n",
    "    It works for Eval mode of ConvBN blocks during validation, and can be used\n",
    "    for training as well. It reduces memory and computation cost.\n",
    "    Args:\n",
    "        bn (_BatchNorm): a BatchNorm module.\n",
    "        conv (nn._ConvNd): a conv module\n",
    "        x (torch.Tensor): Input feature map.\n",
    "    \"\"\"\n",
    "    # These lines of code are designed to deal with various cases\n",
    "    # like bn without affine transform, and conv without bias\n",
    "    weight_on_the_fly = conv.weight\n",
    "    if conv.bias is not None:\n",
    "        bias_on_the_fly = conv.bias\n",
    "    else:\n",
    "        bias_on_the_fly = torch.zeros_like(bn.running_var)\n",
    "\n",
    "    if bn.weight is not None:\n",
    "        bn_weight = bn.weight\n",
    "    else:\n",
    "        bn_weight = torch.ones_like(bn.running_var)\n",
    "\n",
    "    if bn.bias is not None:\n",
    "        bn_bias = bn.bias\n",
    "    else:\n",
    "        bn_bias = torch.zeros_like(bn.running_var)\n",
    "\n",
    "    # shape of [C_out, 1, 1, 1] in Conv2d\n",
    "    weight_coeff = torch.rsqrt(bn.running_var +\n",
    "                               bn.eps).reshape([-1] + [1] *\n",
    "                                               (len(conv.weight.shape) - 1))\n",
    "    # shape of [C_out, 1, 1, 1] in Conv2d\n",
    "    coefff_on_the_fly = bn_weight.view_as(weight_coeff) * weight_coeff\n",
    "\n",
    "    # shape of [C_out, C_in, k, k] in Conv2d\n",
    "    weight_on_the_fly = weight_on_the_fly * coefff_on_the_fly\n",
    "    # shape of [C_out] in Conv2d\n",
    "    bias_on_the_fly = bn_bias + coefff_on_the_fly.flatten() *\\\n",
    "        (bias_on_the_fly - bn.running_mean)\n",
    "\n",
    "    return conv._conv_forward(x, weight_on_the_fly, bias_on_the_fly)\n",
    "\n",
    "\n",
    "def efficient_conv_bn_eval_control(bn: nn.modules.batchnorm._BatchNorm,\n",
    "                                   conv: nn.modules.conv._ConvNd,\n",
    "                                   x: torch.Tensor):\n",
    "    \"\"\"This function controls whether to use `efficient_conv_bn_eval_forward`.\n",
    "\n",
    "    If the following `bn` is in `eval` mode, then we turn on the special\n",
    "    `efficient_conv_bn_eval_forward`.\n",
    "    \"\"\"\n",
    "    if not bn.training:\n",
    "        # bn in eval mode\n",
    "        output = efficient_conv_bn_eval_forward(bn, conv, x)\n",
    "        return output\n",
    "    else:\n",
    "        conv_out = conv._conv_forward(x, conv.weight, conv.bias)\n",
    "        return bn(conv_out)\n",
    "\n",
    "\n",
    "def efficient_conv_bn_eval_graph_transform(fx_model):\n",
    "    \"\"\"Find consecutive conv+bn calls in the graph, inplace modify the graph\n",
    "    with the fused operation.\"\"\"\n",
    "    modules = dict(fx_model.named_modules())\n",
    "\n",
    "    patterns = [(torch.nn.modules.conv._ConvNd,\n",
    "                 torch.nn.modules.batchnorm._BatchNorm)]\n",
    "\n",
    "    pairs = []\n",
    "    # Iterate through nodes in the graph to find ConvBN blocks\n",
    "    for node in fx_model.graph.nodes:\n",
    "        # If our current node isn't calling a Module then we can ignore it.\n",
    "        if node.op != 'call_module':\n",
    "            continue\n",
    "        target_module = modules[node.target]\n",
    "        found_pair = False\n",
    "        for conv_class, bn_class in patterns:\n",
    "            if isinstance(target_module, bn_class):\n",
    "                source_module = modules[node.args[0].target]\n",
    "                if isinstance(source_module, conv_class):\n",
    "                    found_pair = True\n",
    "        # Not a conv-BN pattern or output of conv is used by other nodes\n",
    "        if not found_pair or len(node.args[0].users) > 1:\n",
    "            continue\n",
    "\n",
    "        # Find a pair of conv and bn computation nodes to optimize\n",
    "        conv_node = node.args[0]\n",
    "        bn_node = node\n",
    "        pairs.append([conv_node, bn_node])\n",
    "\n",
    "    for conv_node, bn_node in pairs:\n",
    "        # set insertion point\n",
    "        fx_model.graph.inserting_before(conv_node)\n",
    "        # create `get_attr` node to access modules\n",
    "        # note that we directly call `create_node` to fill the `name`\n",
    "        # argument. `fx_model.graph.get_attr` and\n",
    "        # `fx_model.graph.call_function` does not allow the `name` argument.\n",
    "        conv_get_node = fx_model.graph.create_node(\n",
    "            op='get_attr', target=conv_node.target, name='get_conv')\n",
    "        bn_get_node = fx_model.graph.create_node(\n",
    "            op='get_attr', target=bn_node.target, name='get_bn')\n",
    "        # prepare args for the fused function\n",
    "        args = (bn_get_node, conv_get_node, conv_node.args[0])\n",
    "        # create a new node\n",
    "        new_node = fx_model.graph.create_node(\n",
    "            op='call_function',\n",
    "            target=efficient_conv_bn_eval_control,\n",
    "            args=args,\n",
    "            name='efficient_conv_bn_eval')\n",
    "        # this node replaces the original conv + bn, and therefore\n",
    "        # should replace the uses of bn_node\n",
    "        bn_node.replace_all_uses_with(new_node)\n",
    "        # take care of the deletion order:\n",
    "        # delete bn_node first, and then conv_node\n",
    "        fx_model.graph.erase_node(bn_node)\n",
    "        fx_model.graph.erase_node(conv_node)\n",
    "\n",
    "    # regenerate the code\n",
    "    fx_model.graph.lint()\n",
    "    fx_model.recompile()\n",
    "\n",
    "\n",
    "def turn_on_efficient_conv_bn_eval_for_single_model(model: torch.nn.Module):\n",
    "    import torch.fx as fx\n",
    "\n",
    "    # currently we use `fx.symbolic_trace` to trace models.\n",
    "    # in the future, we might turn to pytorch 2.0 compile infrastructure to\n",
    "    # get the `fx.GraphModule` IR. Nonetheless, the graph transform function\n",
    "    # can remain unchanged. We just need to change the way\n",
    "    # we get `fx.GraphModule`.\n",
    "    fx_model: fx.GraphModule = fx.symbolic_trace(model)\n",
    "    efficient_conv_bn_eval_graph_transform(fx_model)\n",
    "    model.forward = fx_model.forward\n",
    "\n",
    "\n",
    "def turn_on_efficient_conv_bn_eval(model: torch.nn.Module,\n",
    "                                   modules: Union[List[str], str]):\n",
    "    if isinstance(modules, str):\n",
    "        modules = [modules]\n",
    "    for module_name in modules:\n",
    "        module = attrgetter(module_name)(model)\n",
    "        turn_on_efficient_conv_bn_eval_for_single_model(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method SimpleCNN2.forward of SimpleCNN2(\n",
      "  (backbone): SimpleCNN(\n",
      "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc): Linear(in_features=32768, out_features=10, bias=True)\n",
      "  )\n",
      ")>\n",
      "Output shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.fx as fx\n",
    "\n",
    "# 定义一个简单的卷积神经网络作为示例\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, 1, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, 1, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.fc = nn.Linear(32 * 32 * 32, 10)  # 假设输入图像大小为32x32\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = x.view(x.size(0), -1)  # 展平\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class SimpleCNN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN2 ,self).__init__()\n",
    "        self.backbone = SimpleCNN()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "# 调用模型并检查优化情况\n",
    "model = SimpleCNN2()\n",
    "\n",
    "# 创建一个随机输入\n",
    "input_tensor = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "# 调用函数以启用高效的Conv + BN融合并打印优化前后的计算图\n",
    "\n",
    "\n",
    "model = model.eval()\n",
    "turn_on_efficient_conv_bn_eval(model, ['backbone'])\n",
    "print(model.forward)\n",
    "# 执行前向传播\n",
    "output = model(input_tensor)\n",
    "\n",
    "# 打印输出的形状\n",
    "print(f\"Output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method forward of SimpleCNN(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=32768, out_features=10, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# 检查替换后的 forward 方法\n",
    "print(model.forward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %conv1 : [num_users=1] = call_module[target=conv1](args = (%x,), kwargs = {})\n",
      "    %bn1 : [num_users=1] = call_module[target=bn1](args = (%conv1,), kwargs = {})\n",
      "    %relu : [num_users=1] = call_function[target=torch.relu](args = (%bn1,), kwargs = {})\n",
      "    %conv2 : [num_users=1] = call_module[target=conv2](args = (%relu,), kwargs = {})\n",
      "    %bn2 : [num_users=1] = call_module[target=bn2](args = (%conv2,), kwargs = {})\n",
      "    %relu_1 : [num_users=2] = call_function[target=torch.relu](args = (%bn2,), kwargs = {})\n",
      "    %size : [num_users=1] = call_method[target=size](args = (%relu_1, 0), kwargs = {})\n",
      "    %view : [num_users=1] = call_method[target=view](args = (%relu_1, %size, -1), kwargs = {})\n",
      "    %fc : [num_users=1] = call_module[target=fc](args = (%view,), kwargs = {})\n",
      "    return fc\n"
     ]
    }
   ],
   "source": [
    "# 打印 FX 计算图\n",
    "import torch.fx as fx\n",
    "fx_model = fx.symbolic_trace(model)\n",
    "print(fx_model.graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %conv1 : [num_users=1] = call_module[target=conv1](args = (%x,), kwargs = {})\n",
      "    %bn1 : [num_users=1] = call_module[target=bn1](args = (%conv1,), kwargs = {})\n",
      "    %relu : [num_users=1] = call_function[target=torch.relu](args = (%bn1,), kwargs = {})\n",
      "    %conv2 : [num_users=1] = call_module[target=conv2](args = (%relu,), kwargs = {})\n",
      "    %bn2 : [num_users=1] = call_module[target=bn2](args = (%conv2,), kwargs = {})\n",
      "    %relu_1 : [num_users=2] = call_function[target=torch.relu](args = (%bn2,), kwargs = {})\n",
      "    %size : [num_users=1] = call_method[target=size](args = (%relu_1, 0), kwargs = {})\n",
      "    %view : [num_users=1] = call_method[target=view](args = (%relu_1, %size, -1), kwargs = {})\n",
      "    %fc : [num_users=1] = call_module[target=fc](args = (%view,), kwargs = {})\n",
      "    return fc\n"
     ]
    }
   ],
   "source": [
    "# 打印 FX 计算图\n",
    "import torch.fx as fx\n",
    "fx_model = fx.symbolic_trace(model)\n",
    "print(fx_model.graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method forward of SimpleCNN(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=32768, out_features=10, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# 检查替换后的 forward 方法\n",
    "print(model.forward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.4928, -0.7771, -1.0377,  ..., -0.8974, -0.3298, -0.3931],\n",
      "          [ 0.2901,  0.2581,  0.7802,  ...,  0.9171,  0.9820,  0.1806],\n",
      "          [-1.3592,  0.2125,  1.1953,  ..., -0.4506,  0.3617,  1.6183],\n",
      "          ...,\n",
      "          [-0.5671,  0.9266,  2.0484,  ..., -0.4558, -0.8388,  0.0452],\n",
      "          [ 0.0935, -0.7979,  0.9503,  ...,  0.5598, -1.9626, -0.2911],\n",
      "          [ 0.1995,  0.6507, -0.2142,  ..., -0.3146, -0.2791,  0.4547]],\n",
      "\n",
      "         [[ 1.4356,  0.9129, -0.5031,  ..., -0.1020,  1.0058, -0.4193],\n",
      "          [-1.4852, -0.4363, -0.8256,  ..., -0.9477, -0.0374,  0.1647],\n",
      "          [-0.3652, -0.2904,  2.5691,  ...,  0.5813, -0.8416, -0.9438],\n",
      "          ...,\n",
      "          [ 0.4869,  2.5273,  1.1332,  ...,  2.2675,  0.0770,  0.6853],\n",
      "          [ 0.0430, -1.8617, -0.6892,  ..., -0.8342, -0.1863, -0.8786],\n",
      "          [ 0.0802,  0.3856, -0.6600,  ...,  0.8860,  1.6499, -0.8380]],\n",
      "\n",
      "         [[-0.1324,  0.0349,  0.6080,  ..., -0.8111,  0.4878, -0.4131],\n",
      "          [-0.3960, -0.8790, -0.4031,  ..., -0.4868, -1.6179, -0.3251],\n",
      "          [-0.3331,  0.0616, -0.1891,  ...,  0.2430, -2.2637, -0.6969],\n",
      "          ...,\n",
      "          [-0.3116,  1.3042,  2.2381,  ..., -0.5227,  0.2172,  0.3950],\n",
      "          [ 0.8119, -0.1822, -0.6837,  ...,  1.7518, -0.8511, -0.4028],\n",
      "          [ 0.3381,  1.2023,  1.0385,  ...,  1.3968,  1.8621,  0.7444]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.6047,  2.0174,  0.6382,  ...,  0.9876,  1.3423,  0.5428],\n",
      "          [-0.8295, -1.1071,  1.2423,  ...,  0.0537, -0.4261,  0.8158],\n",
      "          [ 0.0238,  0.0187,  0.3870,  ..., -1.4680,  0.3266, -0.0111],\n",
      "          ...,\n",
      "          [-0.3771,  0.5446,  1.8161,  ...,  0.5968, -1.2329,  0.0325],\n",
      "          [-1.0334, -0.0641, -0.1874,  ..., -0.6634, -0.6385, -0.4785],\n",
      "          [ 1.1764,  0.5247,  0.7442,  ...,  0.8770,  0.1319, -0.3058]],\n",
      "\n",
      "         [[-0.1252,  1.4410,  0.9017,  ...,  0.2556,  1.0272, -0.2598],\n",
      "          [ 0.2635,  0.9396, -0.2733,  ..., -0.3872,  1.0957, -1.3776],\n",
      "          [-0.3586,  1.7018, -0.8335,  ..., -2.0465,  1.1816,  0.5955],\n",
      "          ...,\n",
      "          [-0.5616, -0.6389, -0.8998,  ...,  0.0674, -0.1601,  1.6319],\n",
      "          [-0.4514,  0.6065, -0.5904,  ..., -0.0951, -0.4552,  1.8974],\n",
      "          [-0.2615, -1.1907, -0.3493,  ...,  0.1513, -0.8341, -0.1962]],\n",
      "\n",
      "         [[ 0.5387, -0.1873, -0.3129,  ...,  0.0514, -0.8812, -0.1566],\n",
      "          [-0.1119, -1.7238, -0.2912,  ...,  0.9898, -1.5354,  0.1751],\n",
      "          [ 0.0041,  0.1068,  0.3416,  ...,  0.0829, -1.5208, -0.6464],\n",
      "          ...,\n",
      "          [ 0.5925,  0.9936,  0.2095,  ..., -0.6645,  1.3712,  0.2904],\n",
      "          [-0.2313,  1.0972, -1.8082,  ...,  0.6864, -0.1910,  0.1683],\n",
      "          [-0.0717,  0.1626,  0.0124,  ..., -0.5204,  0.2968, -0.1452]]]],\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output_before_fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.2322, -0.3684, -0.4932,  ..., -0.4260, -0.1541, -0.1844],\n",
      "          [ 0.1428,  0.1274,  0.3775,  ...,  0.4430,  0.4741,  0.0903],\n",
      "          [-0.6471,  0.1056,  0.5763,  ..., -0.2120,  0.1770,  0.7788],\n",
      "          ...,\n",
      "          [-0.2678,  0.4476,  0.9848,  ..., -0.2145, -0.3979,  0.0255],\n",
      "          [ 0.0486, -0.3783,  0.4589,  ...,  0.2719, -0.9361, -0.1356],\n",
      "          [ 0.0993,  0.3154, -0.0988,  ..., -0.1468, -0.1298,  0.2216]],\n",
      "\n",
      "         [[ 0.8490,  0.5423, -0.2886,  ..., -0.0532,  0.5968, -0.2394],\n",
      "          [-0.8648, -0.2494, -0.4778,  ..., -0.5494, -0.0153,  0.1032],\n",
      "          [-0.2077, -0.1638,  1.5140,  ...,  0.3477, -0.4872, -0.5472],\n",
      "          ...,\n",
      "          [ 0.2923,  1.4895,  0.6715,  ...,  1.3371,  0.0518,  0.4087],\n",
      "          [ 0.0319, -1.0857, -0.3978,  ..., -0.4829, -0.1027, -0.5089],\n",
      "          [ 0.0537,  0.2329, -0.3806,  ...,  0.5265,  0.9747, -0.4851]],\n",
      "\n",
      "         [[-0.0990,  0.0130,  0.3966,  ..., -0.5533,  0.3161, -0.2869],\n",
      "          [-0.2754, -0.5987, -0.2802,  ..., -0.3362, -1.0933, -0.2280],\n",
      "          [-0.2333,  0.0309, -0.1369,  ...,  0.1523, -1.5255, -0.4768],\n",
      "          ...,\n",
      "          [-0.2189,  0.8626,  1.4877,  ..., -0.3602,  0.1350,  0.2540],\n",
      "          [ 0.5331, -0.1323, -0.4680,  ...,  1.1622, -0.5801, -0.2799],\n",
      "          [ 0.2159,  0.7944,  0.6848,  ...,  0.9246,  1.2360,  0.4879]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3908,  1.3232,  0.4129,  ...,  0.6436,  0.8776,  0.3500],\n",
      "          [-0.5557, -0.7390,  0.8117,  ...,  0.0271, -0.2895,  0.5301],\n",
      "          [ 0.0075,  0.0040,  0.2471,  ..., -0.9771,  0.2073, -0.0156],\n",
      "          ...,\n",
      "          [-0.2572,  0.3512,  1.1904,  ...,  0.3856, -0.8220,  0.0132],\n",
      "          [-0.6903, -0.0506, -0.1320,  ..., -0.4461, -0.4297, -0.3241],\n",
      "          [ 0.7682,  0.3380,  0.4829,  ...,  0.5705,  0.0788, -0.2101]],\n",
      "\n",
      "         [[-0.0906,  0.9307,  0.5790,  ...,  0.1577,  0.6609, -0.1784],\n",
      "          [ 0.1628,  0.6038, -0.1872,  ..., -0.2614,  0.7055, -0.9073],\n",
      "          [-0.2428,  1.1008, -0.5525,  ..., -1.3435,  0.7616,  0.3794],\n",
      "          ...,\n",
      "          [-0.3752, -0.4256, -0.5958,  ...,  0.0350, -0.1134,  1.0552],\n",
      "          [-0.3033,  0.3865, -0.3940,  ..., -0.0710, -0.3058,  1.2283],\n",
      "          [-0.1795, -0.7855, -0.2367,  ...,  0.0897, -0.5529, -0.1369]],\n",
      "\n",
      "         [[ 0.3335, -0.1123, -0.1895,  ...,  0.0343, -0.5384, -0.0935],\n",
      "          [-0.0660, -1.0559, -0.1761,  ...,  0.6106, -0.9402,  0.1103],\n",
      "          [ 0.0052,  0.0683,  0.2125,  ...,  0.0536, -0.9312, -0.3943],\n",
      "          ...,\n",
      "          [ 0.3666,  0.6129,  0.1314,  ..., -0.4054,  0.8448,  0.1811],\n",
      "          [-0.1393,  0.6765, -1.1078,  ...,  0.4243, -0.1146,  0.1061],\n",
      "          [-0.0413,  0.1026,  0.0103,  ..., -0.3169,  0.1850, -0.0865]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output_after_fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0116567611694336e-07\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "import unittest\n",
    "from unittest import TestCase\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BackboneModel(nn.Module):\n",
    "\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(6, 6, 6)\n",
    "        self.bn1 = nn.BatchNorm2d(6)\n",
    "        self.conv2 = nn.Conv2d(6, 6, 6)\n",
    "        self.bn2 = nn.BatchNorm2d(6)\n",
    "        self.conv3 = nn.Conv2d(6, 6, 6)\n",
    "        self.bn3 = nn.BatchNorm2d(6)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # this conv-bn pair can use efficient_conv_bn_eval feature\n",
    "        x = self.bn1(self.conv1(x))\n",
    "        # this conv-bn pair can use efficient_conv_bn_eval feature\n",
    "        # only for the second `self.conv2` call.\n",
    "        x = self.bn2(self.conv2(self.conv2(x)))\n",
    "        # this conv-bn pair can use efficient_conv_bn_eval feature\n",
    "        # just for the first forward of the `self.bn3`\n",
    "        x = self.bn3(self.bn3(self.conv3(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = BackboneModel()\n",
    "model.eval()\n",
    "input = torch.randn(64, 6, 32, 32)\n",
    "output = model(input)\n",
    "turn_on_efficient_conv_bn_eval_for_single_model(model)\n",
    "output2 = model(input)\n",
    "print((output - output2).abs().max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output before fusion:  torch.Size([1, 16, 32, 32])\n",
      "Output after fusion:  torch.Size([1, 16, 32, 32])\n",
      "Are the outputs close?  True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvBn(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', deploy=False):\n",
    "        super(ConvBn, self).__init__()\n",
    "        self.deploy = deploy\n",
    "        if deploy:\n",
    "            self.fused_conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(kernel_size,kernel_size), stride=stride,\n",
    "                                      padding=padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode)\n",
    "        else:\n",
    "            self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                         kernel_size=(kernel_size, kernel_size), stride=stride,\n",
    "                                         padding=padding, dilation=dilation, groups=groups, bias=False,\n",
    "                                         padding_mode=padding_mode)\n",
    "            self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
    "\n",
    "\n",
    "    def _fuse_bn_tensor(self, conv, bn):\n",
    "        std = (bn.running_var + bn.eps).sqrt()\n",
    "        t = (bn.weight / std).reshape(-1, 1, 1, 1)\n",
    "        return conv.weight * t, bn.bias - bn.running_mean * bn.weight / std\n",
    "\n",
    "\n",
    "    def switch_to_deploy(self):\n",
    "        if self.bn.training:\n",
    "            raise RuntimeError(\"BatchNorm should be in evaluation mode (eval) before deployment.\")\n",
    "        deploy_k, deploy_b = self._fuse_bn_tensor(self.conv, self.bn)\n",
    "        self.deploy = True\n",
    "        self.fused_conv = nn.Conv2d(in_channels=self.conv.in_channels, out_channels=self.conv.out_channels,\n",
    "                                    kernel_size=self.conv.kernel_size, stride=self.conv.stride,\n",
    "                                    padding=self.conv.padding, dilation=self.conv.dilation, groups=self.conv.groups, bias=True,\n",
    "                                    padding_mode=self.conv.padding_mode)\n",
    "        self.__delattr__('conv')\n",
    "        self.__delattr__('bn')\n",
    "        self.fused_conv.weight.data = deploy_k\n",
    "        self.fused_conv.bias.data = deploy_b\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.deploy:\n",
    "            return self.fused_conv(input)\n",
    "        else:\n",
    "            square_outputs = self.conv(input)\n",
    "            square_outputs = self.bn(square_outputs)\n",
    "            return square_outputs\n",
    "\n",
    "\n",
    "conv_bn_layer = ConvBn(3, 16, 3, 1, 1).eval()\n",
    "input_tensor = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "output_before_fusion = conv_bn_layer(input_tensor)\n",
    "conv_bn_layer.switch_to_deploy()\n",
    "output_after_fusion = conv_bn_layer(input_tensor)\n",
    "\n",
    "print(\"Output before fusion: \", output_before_fusion.shape)\n",
    "print(\"Output after fusion: \", output_after_fusion.shape)\n",
    "print(\"Are the outputs close? \", torch.allclose(output_before_fusion, output_after_fusion, atol=1e-6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of conv.weight: 2.000000\n",
      "Mean of bn.weight: 3.000000\n",
      "Mean of bn.bias: 1.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from model.base_module import BaseModule\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class ConvBn(BaseModule):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups=1, init_cfg=None):\n",
    "        super().__init__(init_cfg)\n",
    "        self.in_channels = in_channels\n",
    "        self.groups = groups\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, groups=groups, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'rbr_reparam'):\n",
    "            return self.rbr_reparam(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "init_cfg = [\n",
    "    dict(type='Constant',val=2,layer='Conv2d'),\n",
    "    dict(type='Constant',override=dict(name='bn'),val =3,bias=1), \n",
    "]\n",
    "# Example of usage:\n",
    "conv_bn_layer = ConvBn(3, 16, 3, 1, 1,init_cfg = init_cfg )\n",
    "\n",
    "# Define the initialization configuration for Conv2d and BatchNorm2d\n",
    "  # Example using Xavier initialization for Conv2d\n",
    "\n",
    "# Initialize weights\n",
    "conv_bn_layer.init_weights()\n",
    "\n",
    "# Create a random input tensor\n",
    "input_tensor = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "# Forward pass to see the output\n",
    "output = conv_bn_layer(input_tensor)\n",
    "\n",
    "# Print initialized weights of Conv2d and BatchNorm2d\n",
    "# 打印每一层的权重均值和偏置\n",
    "param_mean = {name: p.mean().item() for name, p in conv_bn_layer.named_parameters()}\n",
    "for name, mean in param_mean.items():\n",
    "    print(f\"Mean of {name}: {mean:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.19.8-py3-none-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting click!=8.0.0,>=7.1 (from wandb)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from wandb) (3.10.0)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 (from wandb)\n",
      "  Downloading protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from wandb) (5.9.0)\n",
      "Collecting pydantic<3,>=2.6 (from wandb)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.24.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.5-cp310-cp310-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from wandb) (75.8.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=2.6->wandb)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=2.6->wandb)\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading wandb-0.19.8-py3-none-macosx_11_0_arm64.whl (19.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Downloading protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl (417 kB)\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp310-cp310-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-2.24.0-py2.py3-none-any.whl (336 kB)\n",
      "Downloading setproctitle-1.3.5-cp310-cp310-macosx_11_0_arm64.whl (11 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, setproctitle, sentry-sdk, pydantic-core, protobuf, docker-pycreds, click, annotated-types, pydantic, gitdb, gitpython, wandb\n",
      "Successfully installed annotated-types-0.7.0 click-8.1.8 docker-pycreds-0.4.0 gitdb-4.0.12 gitpython-3.1.44 protobuf-5.29.4 pydantic-2.10.6 pydantic-core-2.27.2 sentry-sdk-2.24.0 setproctitle-1.3.5 smmap-5.0.2 wandb-0.19.8\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: model\n",
      "Loading module: model.backbone\n",
      "Loading module: model.efficient_conv_bn_eval\n",
      "Loading module: model.resnet\n",
      "Loading module: model.utils\n",
      "Unexpected error while loading module model.utils: name 'nn' is not defined\n",
      "Loading module: model.weight_url\n",
      "Loading module: model.base_module\n",
      "Loading module: model.weight_init\n",
      "Processing directory: model/__pycache__\n",
      "Processing directory: model/segmentor\n",
      "Loading module: model.segmentor.encoderdecoder\n",
      "ModuleNotFoundError: No module named 'mmengine'\n",
      "Loading module: model.segmentor.segmentor\n",
      "Unexpected error while loading module model.segmentor.segmentor: name 'Tensor' is not defined\n",
      "Processing directory: model/segmentor/__pycache__\n",
      "Mean of conv1.weight: 0.000029\n",
      "Mean of bn1.weight: 0.257577\n",
      "Mean of bn1.bias: 0.181120\n",
      "Mean of layer1.0.conv1.weight: -0.003087\n",
      "Mean of layer1.0.bn1.weight: 0.339601\n",
      "Mean of layer1.0.bn1.bias: -0.034137\n",
      "Mean of layer1.0.conv2.weight: -0.000889\n",
      "Mean of layer1.0.bn2.weight: 0.333055\n",
      "Mean of layer1.0.bn2.bias: 0.003463\n",
      "Mean of layer1.1.conv1.weight: -0.002420\n",
      "Mean of layer1.1.bn1.weight: 0.328692\n",
      "Mean of layer1.1.bn1.bias: -0.083574\n",
      "Mean of layer1.1.conv2.weight: -0.001260\n",
      "Mean of layer1.1.bn2.weight: 0.392430\n",
      "Mean of layer1.1.bn2.bias: -0.029984\n",
      "Mean of layer2.0.conv1.weight: -0.001454\n",
      "Mean of layer2.0.bn1.weight: 0.316419\n",
      "Mean of layer2.0.bn1.bias: -0.067346\n",
      "Mean of layer2.0.conv2.weight: -0.001248\n",
      "Mean of layer2.0.bn2.weight: 0.327573\n",
      "Mean of layer2.0.bn2.bias: -0.003555\n",
      "Mean of layer2.0.downsample.0.weight: -0.002588\n",
      "Mean of layer2.0.downsample.1.weight: 0.195085\n",
      "Mean of layer2.0.downsample.1.bias: -0.003555\n",
      "Mean of layer2.1.conv1.weight: -0.001530\n",
      "Mean of layer2.1.bn1.weight: 0.321264\n",
      "Mean of layer2.1.bn1.bias: -0.210250\n",
      "Mean of layer2.1.conv2.weight: -0.001272\n",
      "Mean of layer2.1.bn2.weight: 0.282911\n",
      "Mean of layer2.1.bn2.bias: -0.151284\n",
      "Mean of layer3.0.conv1.weight: -0.001368\n",
      "Mean of layer3.0.bn1.weight: 0.312275\n",
      "Mean of layer3.0.bn1.bias: -0.114786\n",
      "Mean of layer3.0.conv2.weight: -0.000787\n",
      "Mean of layer3.0.bn2.weight: 0.320250\n",
      "Mean of layer3.0.bn2.bias: -0.030766\n",
      "Mean of layer3.0.downsample.0.weight: -0.001896\n",
      "Mean of layer3.0.downsample.1.weight: 0.082104\n",
      "Mean of layer3.0.downsample.1.bias: -0.030766\n",
      "Mean of layer3.1.conv1.weight: -0.001662\n",
      "Mean of layer3.1.bn1.weight: 0.278211\n",
      "Mean of layer3.1.bn1.bias: -0.237468\n",
      "Mean of layer3.1.conv2.weight: -0.001441\n",
      "Mean of layer3.1.bn2.weight: 0.245850\n",
      "Mean of layer3.1.bn2.bias: -0.163723\n",
      "Mean of layer4.0.conv1.weight: -0.001565\n",
      "Mean of layer4.0.bn1.weight: 0.264318\n",
      "Mean of layer4.0.bn1.bias: -0.225722\n",
      "Mean of layer4.0.conv2.weight: -0.001303\n",
      "Mean of layer4.0.bn2.weight: 0.424319\n",
      "Mean of layer4.0.bn2.bias: -0.197634\n",
      "Mean of layer4.0.downsample.0.weight: -0.000843\n",
      "Mean of layer4.0.downsample.1.weight: 0.250642\n",
      "Mean of layer4.0.downsample.1.bias: -0.197634\n",
      "Mean of layer4.1.conv1.weight: -0.002261\n",
      "Mean of layer4.1.bn1.weight: 0.288600\n",
      "Mean of layer4.1.bn1.bias: -0.241740\n",
      "Mean of layer4.1.conv2.weight: -0.000108\n",
      "Mean of layer4.1.bn2.weight: 1.853810\n",
      "Mean of layer4.1.bn2.bias: 0.273822\n"
     ]
    }
   ],
   "source": [
    "from apps.builder import make_model\n",
    "from apps.registry import register_dir\n",
    "\n",
    "\n",
    "register_dir('model')\n",
    "model =  make_model(dict(type='resnet18',pretrained =True,dataset='imagenet1k_v1'))\n",
    "model.init_weights()\n",
    "param_mean = {name: p.mean().item() for name, p in model.named_parameters()}\n",
    "for name, mean in param_mean.items():\n",
    "    print(f\"Mean of {name}: {mean:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBn(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', deploy=False):\n",
    "        super(ConvBn, self).__init__()\n",
    "        self.deploy = deploy\n",
    "        if deploy:\n",
    "            self.fused_conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(kernel_size,kernel_size), stride=stride,\n",
    "                                      padding=padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode)\n",
    "        else:\n",
    "            self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                         kernel_size=(kernel_size, kernel_size), stride=stride,\n",
    "                                         padding=padding, dilation=dilation, groups=groups, bias=False,\n",
    "                                         padding_mode=padding_mode)\n",
    "            self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
    "\n",
    "\n",
    "    def _fuse_bn_tensor(self, conv, bn):\n",
    "        std = (bn.running_var + bn.eps).sqrt()\n",
    "        t = (bn.weight / std).reshape(-1, 1, 1, 1)\n",
    "        return conv.weight * t, bn.bias - bn.running_mean * bn.weight / std\n",
    "\n",
    "\n",
    "    def switch_to_deploy(self):\n",
    "        deploy_k, deploy_b = self._fuse_bn_tensor(self.conv, self.bn)\n",
    "        self.deploy = True\n",
    "        self.fused_conv = nn.Conv2d(in_channels=self.conv.in_channels, out_channels=self.conv.out_channels,\n",
    "                                    kernel_size=self.conv.kernel_size, stride=self.conv.stride,\n",
    "                                    padding=self.conv.padding, dilation=self.conv.dilation, groups=self.conv.groups, bias=True,\n",
    "                                    padding_mode=self.conv.padding_mode)\n",
    "        self.__delattr__('conv')\n",
    "        self.__delattr__('bn')\n",
    "        self.fused_conv.weight.data = deploy_k\n",
    "        self.fused_conv.bias.data = deploy_b\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.deploy:\n",
    "            return self.fused_conv(input)\n",
    "        else:\n",
    "            square_outputs = self.conv(input)\n",
    "            square_outputs = self.bn(square_outputs)\n",
    "            return square_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class RepConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_sizes=(3, 7, 11), stride=1, padding=None, dilation=1, deploy=False, groups=1,use_bn):\n",
    "        \"\"\"\n",
    "        Reparameterized convolution module for Conv2d, with an interface consistent with nn.Conv2d.\n",
    "        :param in_channels: Number of input channels\n",
    "        :param out_channels: Number of output channels\n",
    "        :param kernel_sizes: List of kernel sizes\n",
    "        :param stride: Stride of the convolution\n",
    "        :param padding: Padding size; if None, automatically computes \"same\" padding\n",
    "        :param dilation: Dilation rate\n",
    "        :param bias: Whether to include a bias term\n",
    "        :param use_identity: Whether to use identity mapping (only effective when in_channels == out_channels and groups == 1)\n",
    "        :param groups: Number of groups for group convolution\n",
    "        \"\"\"\n",
    "        super(RepConv2d, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_sizes = sorted(kernel_sizes)\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "\n",
    "\n",
    "        self.max_kernel_size = max(self.kernel_sizes)\n",
    "        self.padding = padding if padding is not None else (self.max_kernel_size - 1) // 2 * dilation\n",
    "        if use_bn:\n",
    "            self.convs = nn.ModuleList([\n",
    "            ConvBn(in_channels, out_channels, k, stride=stride, dilation=dilation,\n",
    "                      padding=(k - 1) // 2 * dilation, bias=False, groups=groups)\n",
    "            for k in self.kernel_sizes\n",
    "            ])\n",
    "        else:\n",
    "            self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels, out_channels, k, stride=stride, dilation=dilation,\n",
    "                      padding=(k - 1) // 2 * dilation, bias=False, groups=groups)\n",
    "            for k in self.kernel_sizes\n",
    "            ])\n",
    "        self.deploy = deploy \n",
    "        if deploy:\n",
    "            self.fused_conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(kernel_size,kernel_size), stride=stride,\n",
    "                                      padding=padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.reparameterized:\n",
    "            return self.rbr_reparam(\n",
    "                x\n",
    "            )\n",
    "        else:\n",
    "            conv_outputs = []\n",
    "            for conv in self.convs:\n",
    "                conv_outputs.append(conv(x))\n",
    "            if self.use_identity:\n",
    "                return sum(conv_outputs) / len(self.convs) + x\n",
    "            else:\n",
    "                return sum(conv_outputs) / len(self.convs)\n",
    "\n",
    "    def _convert_weight_and_bias(self):\n",
    "        weight = self.convs[-1].weight\n",
    "        bias = self.convs[-1].bias if self.convs[-1].bias is not None else torch.zeros(self.out_channels, device=weight.device)\n",
    "\n",
    "        for conv in self.convs[:-1]:\n",
    "            pad = (self.max_kernel_size - conv.weight.shape[-1]) // 2\n",
    "            weight = weight + F.pad(conv.weight, [pad, pad, pad, pad])\n",
    "            conv_bias = conv.bias if conv.bias is not None else torch.zeros(self.out_channels, device=weight.device)\n",
    "            bias = bias + conv_bias\n",
    "\n",
    "        weight = weight / len(self.convs)\n",
    "        bias = bias / len(self.convs)\n",
    "\n",
    "        if self.use_identity:\n",
    "            pad = (self.max_kernel_size - 1) // 2\n",
    "            identity_weight = F.pad(\n",
    "                torch.eye(self.out_channels, self.in_channels // self.groups).unsqueeze(-1).unsqueeze(-1).to(weight.device),\n",
    "                [pad, pad, pad, pad]\n",
    "            ).repeat(1, self.groups, 1, 1).reshape(self.out_channels, self.in_channels // self.groups, self.max_kernel_size, self.max_kernel_size)\n",
    "            weight = weight + identity_weight\n",
    "\n",
    "        self.weight = weight.detach()\n",
    "        self.bias = bias.detach()\n",
    "\n",
    "    def switch_to_deploy(self):\n",
    "        if hasattr(self, 'rbr_reparam'):\n",
    "            return\n",
    "        self._convert_weight_and_bias()\n",
    "        self.rbr_reparam = nn.Conv2d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=(self.max_kernel_size, self.max_kernel_size),\n",
    "            stride=self.stride,\n",
    "            padding=self.padding,\n",
    "            dilation=self.dilation,\n",
    "            groups=self.groups,\n",
    "            bias=True\n",
    "        )\n",
    "        self.rbr_reparam.weight.data = self.weight\n",
    "        self.rbr_reparam.bias.data = self.bias\n",
    "        del self.convs\n",
    "        self.reparameterized = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize the RepConv2d model\n",
    "model = RepConv2d(in_channels=3, out_channels=16, kernel_sizes=(3, 5, 7), stride=1).eval()\n",
    "\n",
    "# Create a dummy input tensor\n",
    "x = torch.randn(1, 3, 224, 224)  # Batch size 1, 3 channels, 224x224 image\n",
    "\n",
    "# Run forward pass before deployment (pre-deployment)\n",
    "output_before_deploy = model(x)\n",
    "\n",
    "# Deploy the model\n",
    "model.switch_to_deploy()\n",
    "\n",
    "# Run forward pass after deployment (post-deployment)\n",
    "output_after_deploy = model(x)\n",
    "\n",
    "# Compare the outputs\n",
    "print(torch.allclose(output_before_deploy, output_after_deploy))  # Should be True if the outputs are the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.0509e-01, -1.6618e-01,  2.3734e-02,  ...,  4.3876e-02,\n",
       "           -2.7332e-01,  7.8780e-02],\n",
       "          [-4.3497e-02, -2.1642e-01,  1.4031e-01,  ...,  1.1755e-01,\n",
       "           -2.8986e-01, -7.6739e-02],\n",
       "          [ 1.0535e-01, -2.4501e-01, -9.3907e-02,  ...,  1.5210e-01,\n",
       "            1.1109e-01, -3.8096e-02],\n",
       "          ...,\n",
       "          [-4.9162e-02,  1.0591e-01,  2.6854e-01,  ...,  3.2434e-01,\n",
       "            1.1938e-01,  1.5615e-01],\n",
       "          [ 1.6362e-01,  3.3904e-01,  1.2398e-01,  ..., -1.7295e-01,\n",
       "            1.4540e-01,  1.7867e-03],\n",
       "          [ 3.7581e-02,  3.7735e-01,  7.6096e-02,  ...,  3.2586e-01,\n",
       "            5.5626e-01, -5.7446e-02]],\n",
       "\n",
       "         [[-1.8109e-01,  3.9394e-02,  3.0771e-01,  ...,  8.3651e-02,\n",
       "           -3.2509e-01, -1.4123e-01],\n",
       "          [ 7.4080e-02, -2.8459e-01, -5.4809e-02,  ...,  4.1137e-02,\n",
       "            1.3135e-01,  3.6649e-01],\n",
       "          [-9.8002e-02,  4.2682e-01,  2.3538e-03,  ...,  3.5936e-01,\n",
       "            2.5884e-01, -3.5275e-01],\n",
       "          ...,\n",
       "          [-7.6958e-02,  1.1001e-01, -2.1666e-01,  ...,  2.5912e-01,\n",
       "            4.6311e-01,  1.0677e-01],\n",
       "          [-1.2450e-01,  5.7921e-01, -3.4372e-01,  ...,  1.1061e-01,\n",
       "           -2.1486e-01,  5.7938e-02],\n",
       "          [-3.8688e-02, -2.2093e-02,  5.6106e-01,  ...,  1.6838e-01,\n",
       "            1.5817e-01,  2.3281e-01]],\n",
       "\n",
       "         [[-1.0894e-02,  1.8954e-01, -3.5832e-02,  ...,  4.6218e-01,\n",
       "           -4.1146e-01, -9.6226e-02],\n",
       "          [ 2.4744e-01, -3.8143e-01, -5.9170e-01,  ...,  4.8771e-02,\n",
       "            8.9603e-02, -1.9898e-01],\n",
       "          [ 3.0614e-01,  1.8092e-01, -2.9465e-02,  ..., -3.3937e-01,\n",
       "           -8.9875e-02,  8.9298e-02],\n",
       "          ...,\n",
       "          [-1.0664e-01,  3.8526e-01,  3.2055e-01,  ..., -1.9398e-01,\n",
       "           -4.5279e-02,  3.9769e-01],\n",
       "          [-2.9203e-01,  3.3534e-01,  6.0844e-01,  ...,  1.9899e-01,\n",
       "           -4.3176e-01, -2.6826e-01],\n",
       "          [-1.3407e-01, -3.3269e-01, -3.4315e-02,  ..., -4.4963e-01,\n",
       "           -4.9919e-01,  6.1906e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.9034e-01,  7.3505e-02,  8.6552e-02,  ..., -3.4830e-01,\n",
       "           -2.7727e-01,  2.8850e-02],\n",
       "          [ 2.6271e-01,  9.1909e-02,  1.6791e-02,  ...,  3.8670e-02,\n",
       "            6.3160e-01,  1.3287e-01],\n",
       "          [-3.0033e-01,  3.3922e-01,  1.9802e-01,  ..., -5.4369e-01,\n",
       "            8.5222e-02, -1.2343e-01],\n",
       "          ...,\n",
       "          [-5.8591e-02,  2.4301e-01,  1.7809e-04,  ...,  6.0583e-01,\n",
       "            1.7878e-01, -8.1960e-02],\n",
       "          [ 3.0094e-01, -7.7375e-01, -3.3122e-01,  ..., -2.4812e-02,\n",
       "           -4.7006e-01,  2.2782e-01],\n",
       "          [ 6.1376e-02,  1.0328e-01,  8.7527e-02,  ...,  2.0196e-02,\n",
       "           -3.3378e-02, -6.3306e-02]],\n",
       "\n",
       "         [[-1.2842e-01, -2.0225e-02, -4.7466e-02,  ...,  1.6582e-01,\n",
       "            2.0710e-01,  7.5919e-02],\n",
       "          [-3.6354e-01, -2.0423e-01, -1.0628e-01,  ..., -2.2852e-01,\n",
       "           -7.3149e-01, -3.8529e-01],\n",
       "          [-4.9332e-02, -2.3332e-01, -3.1077e-01,  ..., -6.2605e-01,\n",
       "           -3.4752e-01, -2.8183e-02],\n",
       "          ...,\n",
       "          [-5.3266e-01, -4.1405e-01, -1.0516e-01,  ..., -5.0515e-01,\n",
       "           -2.2890e-02, -1.7347e-01],\n",
       "          [-9.9413e-02,  4.9966e-01, -9.5683e-02,  ...,  2.5346e-01,\n",
       "           -2.1047e-01,  2.3567e-02],\n",
       "          [ 3.1842e-01,  2.4698e-01,  1.3175e-01,  ...,  7.1634e-02,\n",
       "           -2.2469e-01, -6.1062e-01]],\n",
       "\n",
       "         [[-8.3371e-02,  3.0713e-01,  1.3768e-02,  ...,  2.8686e-01,\n",
       "            1.6393e-01, -2.3217e-01],\n",
       "          [-8.0951e-02,  1.4463e-01, -2.5413e-01,  ..., -3.6405e-01,\n",
       "           -3.7708e-01,  8.5218e-01],\n",
       "          [-6.1974e-02, -9.5626e-02,  1.3724e-01,  ...,  1.3480e-01,\n",
       "            7.7468e-03, -3.9745e-01],\n",
       "          ...,\n",
       "          [ 1.4236e-01,  5.3777e-02,  1.9422e-01,  ..., -1.5561e-01,\n",
       "            4.6266e-02,  3.1290e-01],\n",
       "          [ 1.8462e-01,  1.3874e-01, -3.3064e-01,  ...,  3.0305e-01,\n",
       "           -4.4506e-01, -2.1772e-01],\n",
       "          [ 5.7041e-01, -1.1243e-01, -9.6366e-02,  ..., -1.4998e-01,\n",
       "            8.7994e-02,  6.3286e-01]]]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_before_deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.0509e-01, -1.6618e-01,  2.3734e-02,  ...,  4.3876e-02,\n",
       "           -2.7332e-01,  7.8780e-02],\n",
       "          [-4.3497e-02, -2.1642e-01,  1.4031e-01,  ...,  1.1755e-01,\n",
       "           -2.8986e-01, -7.6739e-02],\n",
       "          [ 1.0535e-01, -2.4501e-01, -9.3907e-02,  ...,  1.5210e-01,\n",
       "            1.1109e-01, -3.8096e-02],\n",
       "          ...,\n",
       "          [-4.9162e-02,  1.0591e-01,  2.6854e-01,  ...,  3.2434e-01,\n",
       "            1.1938e-01,  1.5615e-01],\n",
       "          [ 1.6362e-01,  3.3904e-01,  1.2398e-01,  ..., -1.7295e-01,\n",
       "            1.4540e-01,  1.7867e-03],\n",
       "          [ 3.7581e-02,  3.7735e-01,  7.6096e-02,  ...,  3.2586e-01,\n",
       "            5.5626e-01, -5.7446e-02]],\n",
       "\n",
       "         [[-1.8109e-01,  3.9394e-02,  3.0771e-01,  ...,  8.3651e-02,\n",
       "           -3.2509e-01, -1.4123e-01],\n",
       "          [ 7.4080e-02, -2.8459e-01, -5.4809e-02,  ...,  4.1137e-02,\n",
       "            1.3135e-01,  3.6649e-01],\n",
       "          [-9.8002e-02,  4.2682e-01,  2.3537e-03,  ...,  3.5936e-01,\n",
       "            2.5884e-01, -3.5275e-01],\n",
       "          ...,\n",
       "          [-7.6958e-02,  1.1001e-01, -2.1666e-01,  ...,  2.5912e-01,\n",
       "            4.6311e-01,  1.0677e-01],\n",
       "          [-1.2450e-01,  5.7921e-01, -3.4372e-01,  ...,  1.1061e-01,\n",
       "           -2.1486e-01,  5.7938e-02],\n",
       "          [-3.8689e-02, -2.2093e-02,  5.6106e-01,  ...,  1.6838e-01,\n",
       "            1.5817e-01,  2.3281e-01]],\n",
       "\n",
       "         [[-1.0894e-02,  1.8954e-01, -3.5832e-02,  ...,  4.6218e-01,\n",
       "           -4.1146e-01, -9.6226e-02],\n",
       "          [ 2.4744e-01, -3.8143e-01, -5.9170e-01,  ...,  4.8772e-02,\n",
       "            8.9603e-02, -1.9898e-01],\n",
       "          [ 3.0614e-01,  1.8092e-01, -2.9465e-02,  ..., -3.3937e-01,\n",
       "           -8.9876e-02,  8.9298e-02],\n",
       "          ...,\n",
       "          [-1.0664e-01,  3.8526e-01,  3.2055e-01,  ..., -1.9398e-01,\n",
       "           -4.5279e-02,  3.9769e-01],\n",
       "          [-2.9203e-01,  3.3534e-01,  6.0844e-01,  ...,  1.9899e-01,\n",
       "           -4.3176e-01, -2.6826e-01],\n",
       "          [-1.3407e-01, -3.3269e-01, -3.4315e-02,  ..., -4.4963e-01,\n",
       "           -4.9919e-01,  6.1906e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.9034e-01,  7.3505e-02,  8.6552e-02,  ..., -3.4830e-01,\n",
       "           -2.7727e-01,  2.8850e-02],\n",
       "          [ 2.6271e-01,  9.1909e-02,  1.6791e-02,  ...,  3.8670e-02,\n",
       "            6.3160e-01,  1.3287e-01],\n",
       "          [-3.0033e-01,  3.3922e-01,  1.9802e-01,  ..., -5.4369e-01,\n",
       "            8.5221e-02, -1.2343e-01],\n",
       "          ...,\n",
       "          [-5.8591e-02,  2.4301e-01,  1.7799e-04,  ...,  6.0583e-01,\n",
       "            1.7878e-01, -8.1960e-02],\n",
       "          [ 3.0094e-01, -7.7375e-01, -3.3122e-01,  ..., -2.4812e-02,\n",
       "           -4.7006e-01,  2.2782e-01],\n",
       "          [ 6.1376e-02,  1.0328e-01,  8.7527e-02,  ...,  2.0196e-02,\n",
       "           -3.3378e-02, -6.3306e-02]],\n",
       "\n",
       "         [[-1.2842e-01, -2.0225e-02, -4.7466e-02,  ...,  1.6582e-01,\n",
       "            2.0710e-01,  7.5919e-02],\n",
       "          [-3.6354e-01, -2.0423e-01, -1.0628e-01,  ..., -2.2852e-01,\n",
       "           -7.3149e-01, -3.8529e-01],\n",
       "          [-4.9332e-02, -2.3332e-01, -3.1077e-01,  ..., -6.2605e-01,\n",
       "           -3.4752e-01, -2.8183e-02],\n",
       "          ...,\n",
       "          [-5.3266e-01, -4.1405e-01, -1.0516e-01,  ..., -5.0515e-01,\n",
       "           -2.2890e-02, -1.7347e-01],\n",
       "          [-9.9413e-02,  4.9966e-01, -9.5683e-02,  ...,  2.5346e-01,\n",
       "           -2.1047e-01,  2.3567e-02],\n",
       "          [ 3.1842e-01,  2.4698e-01,  1.3175e-01,  ...,  7.1634e-02,\n",
       "           -2.2469e-01, -6.1062e-01]],\n",
       "\n",
       "         [[-8.3371e-02,  3.0713e-01,  1.3768e-02,  ...,  2.8686e-01,\n",
       "            1.6393e-01, -2.3217e-01],\n",
       "          [-8.0951e-02,  1.4463e-01, -2.5413e-01,  ..., -3.6405e-01,\n",
       "           -3.7708e-01,  8.5218e-01],\n",
       "          [-6.1974e-02, -9.5626e-02,  1.3724e-01,  ...,  1.3480e-01,\n",
       "            7.7468e-03, -3.9745e-01],\n",
       "          ...,\n",
       "          [ 1.4236e-01,  5.3777e-02,  1.9422e-01,  ..., -1.5561e-01,\n",
       "            4.6266e-02,  3.1290e-01],\n",
       "          [ 1.8462e-01,  1.3874e-01, -3.3064e-01,  ...,  3.0305e-01,\n",
       "           -4.4506e-01, -2.1772e-01],\n",
       "          [ 5.7041e-01, -1.1243e-01, -9.6366e-02,  ..., -1.4998e-01,\n",
       "            8.7994e-02,  6.3286e-01]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_after_deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class ConvBn(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', deploy=False):\n",
    "        super(ConvBn, self).__init__()\n",
    "        self.deploy = deploy\n",
    "        if deploy:\n",
    "            self.fused_conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(kernel_size, kernel_size), stride=stride,\n",
    "                                      padding=padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode)\n",
    "        else:\n",
    "            self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                         kernel_size=(kernel_size, kernel_size), stride=stride,\n",
    "                                         padding=padding, dilation=dilation, groups=groups, bias=False,\n",
    "                                         padding_mode=padding_mode)\n",
    "            self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
    "\n",
    "    def _fuse_bn_tensor(self, conv, bn):\n",
    "        std = (bn.running_var + bn.eps).sqrt()\n",
    "        t = (bn.weight / std).reshape(-1, 1, 1, 1)\n",
    "        return conv.weight * t, bn.bias - bn.running_mean * bn.weight / std\n",
    "\n",
    "    def switch_to_deploy(self):\n",
    "        if self.bn.training:\n",
    "            raise RuntimeError(\"BatchNorm should be in evaluation mode (eval) before deployment.\")\n",
    "        deploy_k, deploy_b = self._fuse_bn_tensor(self.conv, self.bn)\n",
    "        self.deploy = True\n",
    "        self.fused_conv = nn.Conv2d(in_channels=self.conv.in_channels, out_channels=self.conv.out_channels,\n",
    "                                    kernel_size=self.conv.kernel_size, stride=self.conv.stride,\n",
    "                                    padding=self.conv.padding, dilation=self.conv.dilation, groups=self.conv.groups, bias=True,\n",
    "                                    padding_mode=self.conv.padding_mode)\n",
    "        self.__delattr__('conv')\n",
    "        self.__delattr__('bn')\n",
    "        self.fused_conv.weight.data = deploy_k\n",
    "        self.fused_conv.bias.data = deploy_b\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.deploy:\n",
    "            return self.fused_conv(input)\n",
    "        else:\n",
    "            square_outputs = self.conv(input)\n",
    "            square_outputs = self.bn(square_outputs)\n",
    "            return square_outputs\n",
    "class RepConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_sizes=(0, 3, 7, 11), stride=1, dilation=1, groups=1, use_bn=True, deploy=False):\n",
    "        super(RepConv2d, self).__init__()\n",
    "        self.deploy = deploy\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_sizes = sorted(kernel_sizes)\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.max_kernel_size = max(self.kernel_sizes)\n",
    "        self.max_padding = (self.max_kernel_size - 1) // 2 * dilation\n",
    "\n",
    "        if deploy:\n",
    "            self.fused_conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=self.max_kernel_size, stride=stride,\n",
    "                                        padding=self.max_padding, dilation=dilation, groups=groups, bias=True)\n",
    "        else:\n",
    "            convs = []\n",
    "            if use_bn:\n",
    "                if 0 in self.kernel_sizes:\n",
    "                    assert in_channels == out_channels, \"in_channels and out_channels should be equal when kernel_size is 0\"\n",
    "                    convs.append(nn.BatchNorm2d(out_channels))  # Add BatchNorm for kernel size 0\n",
    "                    self.kernel_sizes.remove(0)\n",
    "                # Add ConvBn layers for other kernel sizes\n",
    "                convs.extend([\n",
    "                    ConvBn(in_channels, out_channels, k, stride=stride, dilation=dilation,\n",
    "                           padding=(k - 1) // 2 * dilation, groups=groups)\n",
    "                    for k in self.kernel_sizes\n",
    "                ])\n",
    "            else:\n",
    "                if 0 in self.kernel_sizes:\n",
    "                    assert in_channels == out_channels, \"in_channels and out_channels should be equal when kernel_size is 0\"\n",
    "                    convs.append(nn.Identity())  # Identity for kernel size 0\n",
    "                    self.kernel_sizes.remove(0)\n",
    "                # Add Conv2d layers for other kernel sizes\n",
    "                convs.extend([\n",
    "                    nn.Conv2d(in_channels, out_channels, k, stride=stride, dilation=dilation,\n",
    "                              padding=(k - 1) // 2 * dilation, bias=True, groups=groups)\n",
    "                    for k in self.kernel_sizes\n",
    "                ])\n",
    "            self.convs = nn.ModuleList(convs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.deploy:\n",
    "            return self.fused_conv(x)\n",
    "        else:\n",
    "            conv_outputs = []\n",
    "            for conv in self.convs:\n",
    "                conv_outputs.append(conv(x))\n",
    "            return sum(conv_outputs)\n",
    "\n",
    "    def _convert_weight_and_bias(self):\n",
    "        if hasattr(self.convs[-1], 'switch_to_deploy'):\n",
    "            self.convs[-1].switch_to_deploy()\n",
    "            weight = self.convs[-1].fused_conv.weight\n",
    "            bias = self.convs[-1].fused_conv.bias\n",
    "        else:\n",
    "            weight = self.convs[-1].weight\n",
    "            print(weight)\n",
    "            bias = self.convs[-1].bias\n",
    "\n",
    "        for conv in self.convs[:-1]:\n",
    "            if isinstance(conv, nn.BatchNorm2d):\n",
    "                std = (conv.running_var + conv.eps).sqrt()\n",
    "                t = (conv.weight / std).reshape(-1, 1, 1, 1)\n",
    "                pad = (self.max_kernel_size - conv.weight.shape[-1]) // 2\n",
    "                identity_weight = F.pad(\n",
    "                    torch.eye(self.out_channels, self.in_channels // self.groups).unsqueeze(-1).unsqueeze(-1).to(weight.device),\n",
    "                    [pad, pad, pad, pad]\n",
    "                ).repeat(1, self.groups, 1, 1).reshape(self.out_channels, self.in_channels // self.groups, self.max_kernel_size, self.max_kernel_size)\n",
    "                weight = weight + identity_weight* t\n",
    "                bias = bias + conv.bias - conv.running_mean * conv.weight / std\n",
    "            elif isinstance(conv, nn.Identity):\n",
    "                pad = (self.max_kernel_size - 1) // 2\n",
    "                identity_weight = F.pad(\n",
    "                    torch.eye(self.out_channels, self.in_channels // self.groups).unsqueeze(-1).unsqueeze(-1).to(weight.device),\n",
    "                    [pad, pad, pad, pad]\n",
    "                ).repeat(1, self.groups, 1, 1).reshape(self.out_channels, self.in_channels // self.groups, self.max_kernel_size, self.max_kernel_size)\n",
    "                print(identity_weight)\n",
    "                weight = weight + identity_weight\n",
    "            elif isinstance(conv, ConvBn):\n",
    "                conv.switch_to_deploy()\n",
    "                conv_weight = conv.fused_conv.weight\n",
    "                pad = (self.max_kernel_size - conv.fused_conv.weight.shape[-1]) // 2\n",
    "                conv_weight = F.pad(conv_weight, [pad, pad, pad, pad])\n",
    "                conv_bias = conv.fused_conv.bias\n",
    "                weight = weight + conv_weight\n",
    "                bias = bias + conv_bias\n",
    "            elif isinstance(conv, nn.Conv2d):\n",
    "                conv_weight = conv.weight\n",
    "                print(conv.weight.shape[-1])\n",
    "                pad = (self.max_kernel_size - conv.weight.shape[-1]) // 2\n",
    "                conv_weight = F.pad(conv_weight, [pad, pad, pad, pad])\n",
    "                print(conv_weight)\n",
    "                conv_bias = conv.bias\n",
    "                weight = weight + conv_weight\n",
    "                bias = bias + conv_bias\n",
    "            else:\n",
    "                raise TypeError(f\"Unsupported layer type: {type(conv)}\")\n",
    "\n",
    "        self.weight = weight.detach()\n",
    "        self.bias = bias.detach()\n",
    "    @torch.no_grad()\n",
    "    def switch_to_deploy(self):\n",
    "        if hasattr(self, 'rbr_reparam'):\n",
    "            return\n",
    "        self._convert_weight_and_bias()\n",
    "        self.fused_conv = nn.Conv2d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=(self.max_kernel_size, self.max_kernel_size),\n",
    "            stride=self.stride,\n",
    "            padding=self.max_padding,\n",
    "            dilation=self.dilation,\n",
    "            groups=self.groups,\n",
    "            bias=True\n",
    "        )\n",
    "        self.fused_conv.weight.data = self.weight\n",
    "        self.fused_conv.bias.data = self.bias\n",
    "        print(self.fused_conv.weight,11111)\n",
    "        del self.convs\n",
    "        self.deploy = True\n",
    "def initialize_conv_weights_to_one(model):\n",
    "    \"\"\"\n",
    "    Initialize all Conv2d layers in the model to have weight values equal to 1.\n",
    "    \"\"\"\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            # Initialize the weight to 1\n",
    "            nn.init.constant_(module.weight, 2)\n",
    "            # Initialize bias to 0 if it exists\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RepConv2d(\n",
      "  (convs): ModuleList(\n",
      "    (0): Identity()\n",
      "    (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), dilation=(3, 3))\n",
      "    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3))\n",
      "    (3): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3))\n",
      "    (4): Conv2d(16, 16, kernel_size=(11, 11), stride=(1, 1), padding=(15, 15), dilation=(3, 3))\n",
      "  )\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[[[ 1.8109e-02, -2.0485e-02,  5.2946e-03,  ...,  1.5188e-02,\n",
      "           -7.0638e-03,  5.3876e-03],\n",
      "          [-1.0938e-02, -2.8759e-03,  1.4745e-02,  ..., -7.8913e-03,\n",
      "           -2.0768e-02,  8.0515e-03],\n",
      "          [ 1.8660e-02, -2.1418e-02, -3.0451e-03,  ..., -9.5498e-03,\n",
      "            8.5726e-03,  7.6391e-03],\n",
      "          ...,\n",
      "          [ 1.2474e-02, -3.2262e-03, -1.6963e-04,  ...,  7.2465e-03,\n",
      "           -1.6129e-02,  1.1251e-02],\n",
      "          [-1.8547e-02, -2.0286e-02, -2.1047e-02,  ..., -1.9354e-02,\n",
      "            1.1459e-02,  1.6453e-02],\n",
      "          [-3.4359e-03,  5.9197e-03,  7.4856e-03,  ...,  2.0661e-03,\n",
      "            1.0975e-02, -1.5494e-02]],\n",
      "\n",
      "         [[ 4.3046e-03,  7.4427e-03, -1.5095e-02,  ...,  2.2059e-02,\n",
      "            2.0167e-02, -1.5006e-02],\n",
      "          [-7.5907e-03, -2.8238e-03, -4.0229e-03,  ...,  7.9141e-03,\n",
      "           -7.6794e-03,  1.7732e-02],\n",
      "          [-1.3106e-02,  1.9208e-02, -1.7640e-02,  ..., -1.7201e-02,\n",
      "            1.8761e-02, -1.8155e-02],\n",
      "          ...,\n",
      "          [-1.9190e-02, -1.5993e-02,  1.7930e-02,  ...,  1.4848e-02,\n",
      "            1.6207e-02, -2.1187e-02],\n",
      "          [ 1.9451e-02, -4.7563e-03,  4.0510e-03,  ...,  3.6922e-03,\n",
      "           -1.5047e-02, -6.3862e-03],\n",
      "          [ 1.1645e-02,  9.8081e-03, -4.0169e-03,  ...,  1.7872e-03,\n",
      "            4.9732e-03, -1.5422e-02]],\n",
      "\n",
      "         [[ 9.6917e-03,  1.8019e-02,  1.1072e-04,  ..., -5.7808e-03,\n",
      "            6.0760e-03,  1.3902e-03],\n",
      "          [ 6.1997e-03,  5.7043e-03,  1.6037e-02,  ...,  2.1324e-02,\n",
      "            9.2208e-03, -1.4132e-02],\n",
      "          [ 1.4110e-02,  9.3689e-03, -1.8633e-02,  ...,  1.2232e-05,\n",
      "            3.7025e-03,  2.8993e-03],\n",
      "          ...,\n",
      "          [-2.1314e-02,  3.4488e-03,  2.1793e-02,  ...,  7.4140e-03,\n",
      "            8.0582e-04, -1.4218e-02],\n",
      "          [ 1.3898e-02, -1.2502e-02, -9.5170e-03,  ...,  4.5808e-03,\n",
      "           -8.6745e-03,  1.2248e-02],\n",
      "          [ 1.8046e-02, -2.5477e-03, -1.4103e-02,  ..., -4.9156e-03,\n",
      "           -1.2048e-02, -3.0230e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.9855e-02, -1.9071e-02, -2.1872e-02,  ..., -6.2470e-03,\n",
      "           -6.8119e-03, -1.1706e-03],\n",
      "          [-1.8625e-02, -5.4878e-03,  8.2473e-03,  ..., -1.7421e-02,\n",
      "            6.8328e-03, -5.2530e-04],\n",
      "          [-1.4359e-02,  2.0205e-02, -1.4851e-02,  ..., -2.0933e-02,\n",
      "           -2.0475e-02,  2.0107e-03],\n",
      "          ...,\n",
      "          [ 1.5195e-02, -2.1218e-02, -3.3800e-03,  ..., -1.3747e-02,\n",
      "            1.1179e-02, -1.7942e-02],\n",
      "          [-1.2119e-02, -1.8121e-02,  3.8592e-03,  ...,  1.1185e-02,\n",
      "            1.0019e-02,  2.1154e-02],\n",
      "          [ 1.5767e-02,  6.6571e-03,  9.8442e-03,  ...,  2.0943e-02,\n",
      "            1.4505e-03,  1.5473e-02]],\n",
      "\n",
      "         [[-2.2533e-02, -1.5088e-02, -6.9118e-03,  ..., -6.5119e-03,\n",
      "           -1.3158e-02, -3.9110e-03],\n",
      "          [ 6.6004e-03,  9.5189e-03, -3.2527e-04,  ...,  1.1857e-02,\n",
      "            1.4834e-03, -6.2638e-03],\n",
      "          [-1.9040e-02,  2.1573e-02,  1.5467e-02,  ..., -1.2295e-02,\n",
      "            1.0070e-02,  6.3976e-03],\n",
      "          ...,\n",
      "          [-1.1992e-02,  2.2159e-02,  1.1014e-02,  ...,  1.8071e-02,\n",
      "            1.2387e-02,  1.5605e-02],\n",
      "          [ 1.0978e-02, -6.9413e-03,  1.4997e-02,  ..., -1.4104e-02,\n",
      "            2.8498e-03,  2.0800e-02],\n",
      "          [-4.1267e-03, -1.4948e-02, -1.9726e-03,  ...,  9.9044e-03,\n",
      "           -1.4588e-02,  3.9094e-03]],\n",
      "\n",
      "         [[-1.2597e-02, -1.0856e-02,  1.7806e-02,  ..., -3.3109e-03,\n",
      "            4.2294e-03,  1.1558e-02],\n",
      "          [-2.2112e-02, -2.0887e-02,  1.2626e-02,  ..., -1.5667e-02,\n",
      "           -7.5830e-03, -6.3956e-03],\n",
      "          [ 3.4575e-04, -1.0939e-02, -2.0807e-02,  ..., -5.3686e-03,\n",
      "           -7.3739e-03, -1.6999e-02],\n",
      "          ...,\n",
      "          [ 1.9798e-02, -2.2442e-02,  5.0907e-03,  ..., -1.8633e-02,\n",
      "           -1.1709e-02, -2.0819e-02],\n",
      "          [-1.9918e-02,  2.0290e-02,  9.1337e-03,  ...,  1.5053e-02,\n",
      "           -5.4448e-03,  1.1739e-02],\n",
      "          [ 1.3623e-02, -9.1779e-03,  3.0426e-04,  ..., -4.2383e-03,\n",
      "            2.1359e-02,  2.1344e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.1286e-02,  2.2576e-02, -8.4526e-03,  ...,  2.2168e-02,\n",
      "           -1.7883e-02,  1.7646e-02],\n",
      "          [-9.6213e-04, -1.5553e-02,  1.5350e-02,  ...,  4.4812e-03,\n",
      "            1.0516e-02, -2.2158e-03],\n",
      "          [-1.6701e-02, -8.8889e-03, -1.5973e-02,  ..., -1.5854e-03,\n",
      "           -2.2050e-02, -1.8530e-03],\n",
      "          ...,\n",
      "          [-5.5956e-04, -5.8117e-03,  4.5152e-03,  ...,  1.0634e-02,\n",
      "            5.6272e-03, -8.8095e-03],\n",
      "          [-9.9171e-05, -2.2108e-02, -2.1546e-02,  ..., -1.9733e-02,\n",
      "           -2.2659e-02,  3.1412e-03],\n",
      "          [-5.4781e-03, -1.2224e-02, -1.7038e-02,  ...,  1.7620e-02,\n",
      "           -7.3328e-03, -8.6369e-03]],\n",
      "\n",
      "         [[-1.1732e-02,  9.8155e-03, -1.7447e-02,  ...,  1.2140e-03,\n",
      "            1.3464e-02,  6.9319e-03],\n",
      "          [ 2.1788e-02,  2.1614e-02, -1.7821e-02,  ...,  4.8623e-03,\n",
      "            1.2592e-02,  8.8297e-03],\n",
      "          [ 1.6080e-02,  1.3897e-02, -3.3500e-03,  ..., -5.6676e-04,\n",
      "            2.2062e-02,  8.0065e-03],\n",
      "          ...,\n",
      "          [-1.9211e-02, -3.2202e-03,  5.4277e-03,  ...,  4.5994e-03,\n",
      "           -2.2509e-02, -2.0864e-02],\n",
      "          [ 1.7268e-02, -1.9023e-02,  2.8198e-04,  ..., -1.2279e-03,\n",
      "            1.1192e-02, -9.1453e-03],\n",
      "          [ 6.3237e-03,  1.0389e-02,  2.0564e-02,  ...,  1.5274e-02,\n",
      "           -1.1194e-02,  6.3133e-03]],\n",
      "\n",
      "         [[ 5.9179e-03, -1.8819e-02, -1.5997e-02,  ..., -3.3346e-04,\n",
      "            1.3726e-02,  1.2811e-02],\n",
      "          [-9.0349e-03,  1.5748e-03,  1.1271e-02,  ..., -9.0385e-03,\n",
      "            1.5829e-02,  1.7824e-02],\n",
      "          [ 1.2947e-02, -2.4599e-03,  1.4115e-02,  ...,  1.6940e-02,\n",
      "            7.5688e-03,  2.6515e-04],\n",
      "          ...,\n",
      "          [-1.1647e-02, -6.4886e-03, -1.4921e-02,  ...,  1.2653e-02,\n",
      "           -2.0377e-02,  1.4320e-02],\n",
      "          [-2.1139e-03, -1.7133e-02,  8.0747e-03,  ..., -1.0330e-02,\n",
      "           -1.1949e-02,  1.9506e-02],\n",
      "          [ 1.9728e-02, -5.4782e-03, -2.0227e-03,  ...,  1.4212e-03,\n",
      "            9.9942e-04,  1.6394e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.4079e-02,  1.5995e-02,  8.5388e-03,  ...,  9.0664e-03,\n",
      "            1.4836e-02,  1.2437e-02],\n",
      "          [ 9.6164e-03, -2.6811e-03, -7.9213e-03,  ..., -4.5768e-03,\n",
      "           -1.4386e-02,  1.5382e-02],\n",
      "          [-6.2153e-03, -2.0962e-02, -2.1483e-02,  ...,  1.2407e-02,\n",
      "            1.2489e-02,  4.8483e-03],\n",
      "          ...,\n",
      "          [ 1.7795e-02,  4.2753e-03,  1.4372e-03,  ..., -1.9960e-02,\n",
      "           -1.9641e-02, -1.2341e-02],\n",
      "          [-2.8884e-03, -4.1146e-03,  9.1075e-03,  ...,  4.6568e-03,\n",
      "            6.9238e-03, -5.8737e-03],\n",
      "          [-1.9913e-03, -1.5914e-02,  3.2222e-03,  ..., -1.0473e-02,\n",
      "           -1.7343e-02, -1.5590e-02]],\n",
      "\n",
      "         [[ 1.0301e-03,  2.1860e-02, -3.3163e-03,  ..., -1.7589e-02,\n",
      "           -1.3211e-02,  3.8635e-03],\n",
      "          [ 2.0434e-02, -6.3532e-03, -9.3882e-03,  ..., -1.5350e-02,\n",
      "            2.0937e-02,  2.3891e-03],\n",
      "          [ 1.2531e-02,  1.3195e-02, -1.6144e-02,  ...,  5.6404e-03,\n",
      "            1.7746e-02, -2.1170e-02],\n",
      "          ...,\n",
      "          [ 1.0498e-02, -8.9868e-03, -9.1929e-05,  ...,  2.2609e-02,\n",
      "            4.1484e-03,  2.0924e-02],\n",
      "          [ 1.5737e-02,  1.6137e-02, -1.1462e-03,  ...,  1.6396e-02,\n",
      "           -1.3076e-02, -1.1087e-02],\n",
      "          [ 1.8334e-02,  3.7660e-03, -3.8673e-03,  ...,  9.3072e-03,\n",
      "           -9.6438e-03, -1.7849e-02]],\n",
      "\n",
      "         [[-1.2115e-02, -1.0483e-03,  6.4863e-03,  ..., -1.8323e-02,\n",
      "           -1.7019e-03,  1.2776e-03],\n",
      "          [ 1.6787e-02, -4.2354e-03, -1.8669e-02,  ...,  1.2304e-02,\n",
      "           -1.0774e-03,  1.1373e-02],\n",
      "          [-1.0243e-02, -2.1836e-02,  2.2728e-03,  ...,  1.6808e-02,\n",
      "            5.2527e-03, -4.1510e-03],\n",
      "          ...,\n",
      "          [ 5.2596e-05,  1.7075e-02, -3.3086e-03,  ...,  1.3358e-02,\n",
      "           -1.2047e-02, -2.1632e-02],\n",
      "          [-1.8281e-03,  7.1539e-03,  1.3779e-02,  ...,  9.5900e-03,\n",
      "            5.7458e-03,  2.1714e-02],\n",
      "          [-1.0705e-02,  1.5101e-02, -2.5427e-03,  ..., -1.0197e-02,\n",
      "           -1.4091e-02,  1.4229e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 6.9979e-03, -1.2292e-02, -4.3431e-03,  ...,  2.2310e-02,\n",
      "            1.8332e-02, -4.1405e-03],\n",
      "          [ 1.2094e-02,  1.5117e-02,  8.9351e-03,  ..., -1.1053e-02,\n",
      "           -2.2265e-02,  4.8214e-03],\n",
      "          [ 1.3300e-02, -6.4013e-03,  7.2704e-03,  ...,  4.4361e-03,\n",
      "           -1.9479e-02, -1.8179e-02],\n",
      "          ...,\n",
      "          [-1.2999e-02,  1.6452e-02, -6.7528e-03,  ..., -7.6313e-03,\n",
      "            1.6342e-02,  4.8434e-05],\n",
      "          [-1.9704e-03,  1.7379e-04,  8.0086e-03,  ..., -2.7438e-03,\n",
      "           -4.5063e-03,  2.0979e-02],\n",
      "          [-8.6926e-03, -9.9459e-03,  1.8813e-03,  ...,  1.9044e-03,\n",
      "            1.8594e-02,  4.0757e-03]],\n",
      "\n",
      "         [[-1.7791e-02,  1.5144e-02,  1.5235e-02,  ..., -1.2484e-02,\n",
      "           -1.7082e-02,  1.9066e-02],\n",
      "          [-2.1954e-02, -1.1016e-02,  9.2578e-03,  ..., -1.1711e-02,\n",
      "            2.0832e-02, -2.2453e-02],\n",
      "          [ 1.1555e-02, -1.8234e-02, -2.9875e-03,  ..., -1.1089e-02,\n",
      "           -9.7565e-03,  2.7166e-03],\n",
      "          ...,\n",
      "          [-1.7363e-02, -1.8232e-02, -5.6447e-03,  ..., -3.1572e-03,\n",
      "            2.0473e-03, -1.9258e-02],\n",
      "          [ 5.1088e-03,  1.9255e-02, -2.2715e-02,  ..., -1.6001e-02,\n",
      "            2.0740e-02,  1.5105e-03],\n",
      "          [-5.9326e-04,  3.0945e-03,  1.1818e-02,  ...,  1.4406e-02,\n",
      "           -7.8926e-03, -6.3831e-03]],\n",
      "\n",
      "         [[-1.6798e-02,  1.2117e-02, -7.4709e-03,  ..., -2.1267e-02,\n",
      "            2.2178e-02, -7.8679e-03],\n",
      "          [ 1.6672e-03, -1.7623e-02, -7.7323e-03,  ...,  1.7682e-02,\n",
      "           -1.3038e-02, -2.1425e-02],\n",
      "          [-1.7921e-02,  2.2568e-02,  4.0005e-03,  ...,  1.2011e-02,\n",
      "            2.0615e-02,  1.6814e-02],\n",
      "          ...,\n",
      "          [-1.4681e-02,  1.1884e-02,  2.0916e-02,  ..., -5.3445e-03,\n",
      "           -7.4426e-03, -9.3698e-03],\n",
      "          [-1.6926e-02, -1.9732e-02,  1.8262e-02,  ...,  6.1135e-03,\n",
      "            1.6639e-02, -1.4644e-02],\n",
      "          [-3.0033e-03,  7.2607e-03,  1.7058e-02,  ..., -3.8019e-03,\n",
      "            4.8815e-03,  1.6271e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-9.3885e-03, -4.8883e-03,  2.7457e-03,  ..., -9.5636e-03,\n",
      "           -9.2917e-03, -9.5674e-03],\n",
      "          [-1.5343e-02,  1.7980e-02,  8.2250e-03,  ...,  1.9754e-02,\n",
      "            1.4570e-02, -2.6882e-03],\n",
      "          [-2.0936e-02,  1.6084e-02,  1.2643e-03,  ..., -1.1004e-02,\n",
      "           -3.5637e-03,  1.2645e-02],\n",
      "          ...,\n",
      "          [ 1.7969e-02, -1.5820e-02,  3.5142e-03,  ..., -1.4124e-02,\n",
      "           -1.9012e-02, -1.1040e-02],\n",
      "          [ 8.0502e-03,  1.6607e-02,  8.6041e-03,  ..., -2.0461e-02,\n",
      "           -1.0823e-03, -1.8841e-02],\n",
      "          [-1.4281e-02,  1.2025e-03,  2.3169e-03,  ...,  1.9681e-02,\n",
      "            3.7054e-03, -2.2056e-02]],\n",
      "\n",
      "         [[-1.1362e-03,  2.1937e-02,  1.7873e-02,  ..., -1.4961e-02,\n",
      "            2.5140e-03,  1.5648e-02],\n",
      "          [-1.6006e-03, -6.2006e-03, -1.1167e-03,  ...,  1.8014e-02,\n",
      "           -1.1761e-02,  4.4439e-03],\n",
      "          [-2.1281e-02, -1.3029e-02,  1.0283e-02,  ..., -1.7251e-02,\n",
      "           -2.2493e-02,  2.2564e-02],\n",
      "          ...,\n",
      "          [ 1.1383e-02, -1.6011e-02,  9.2264e-03,  ..., -6.2313e-03,\n",
      "           -1.7137e-02, -8.5114e-03],\n",
      "          [ 2.0049e-02, -1.0905e-02, -1.2128e-02,  ...,  2.0908e-02,\n",
      "            3.8433e-03, -1.4652e-02],\n",
      "          [ 1.7953e-02, -4.8520e-03,  1.0436e-02,  ...,  1.2164e-03,\n",
      "           -7.9491e-03, -4.1335e-03]],\n",
      "\n",
      "         [[-1.8501e-02, -1.2117e-02, -2.9985e-03,  ...,  3.4956e-03,\n",
      "            1.8291e-02, -1.2542e-02],\n",
      "          [ 1.4985e-03,  9.4044e-03, -1.0367e-02,  ...,  1.7077e-02,\n",
      "            1.6847e-02, -1.4965e-02],\n",
      "          [-3.5668e-03,  1.7118e-02,  8.8271e-04,  ...,  5.6147e-03,\n",
      "           -4.4474e-03,  2.2288e-02],\n",
      "          ...,\n",
      "          [ 1.1122e-02,  2.1620e-02, -1.4170e-02,  ...,  5.7674e-04,\n",
      "            5.7216e-03, -1.9570e-02],\n",
      "          [ 6.0990e-04,  7.1543e-03,  2.2093e-02,  ...,  1.6245e-02,\n",
      "           -3.9893e-03, -3.5606e-04],\n",
      "          [-2.0282e-02, -1.8040e-02,  2.1476e-02,  ...,  1.5956e-03,\n",
      "            1.6167e-02, -1.4140e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-5.6494e-03,  7.1688e-03,  1.3667e-02,  ...,  1.1899e-02,\n",
      "           -7.4006e-04,  6.6634e-03],\n",
      "          [-1.9340e-03,  1.3104e-02,  1.1349e-02,  ...,  2.0246e-02,\n",
      "            1.4709e-02,  4.2860e-03],\n",
      "          [-3.6218e-03,  1.3910e-02,  2.1387e-02,  ...,  2.0502e-02,\n",
      "            3.1793e-03,  1.1723e-02],\n",
      "          ...,\n",
      "          [ 1.0887e-03, -1.7977e-02,  3.1192e-03,  ..., -2.0243e-02,\n",
      "            2.0133e-02,  2.0167e-02],\n",
      "          [ 2.6829e-03, -1.0590e-02, -1.8966e-02,  ...,  4.0776e-03,\n",
      "            8.3583e-03,  3.0868e-03],\n",
      "          [ 1.6167e-02, -2.5122e-03,  1.6505e-02,  ..., -7.0042e-03,\n",
      "            1.7153e-02,  1.6997e-02]],\n",
      "\n",
      "         [[-7.8715e-03, -9.2376e-03, -1.0939e-02,  ...,  4.6003e-03,\n",
      "           -1.5675e-02,  1.1551e-02],\n",
      "          [-1.7284e-02,  8.2942e-03,  1.3424e-02,  ...,  1.5319e-02,\n",
      "            1.1930e-02,  7.6626e-03],\n",
      "          [-2.2596e-02,  5.6918e-03, -1.0876e-03,  ..., -1.7664e-02,\n",
      "           -1.1754e-02,  1.8492e-02],\n",
      "          ...,\n",
      "          [ 2.1381e-02,  1.6851e-02,  1.6761e-02,  ...,  2.1283e-02,\n",
      "            1.7082e-02,  1.8022e-02],\n",
      "          [ 2.1025e-02, -1.4230e-02,  8.4459e-03,  ..., -8.1787e-03,\n",
      "           -1.4496e-02,  6.7964e-03],\n",
      "          [ 1.9432e-02,  1.1985e-02, -4.9012e-03,  ..., -2.1934e-02,\n",
      "           -2.5637e-03,  9.5481e-03]],\n",
      "\n",
      "         [[ 7.9007e-03,  6.2072e-03, -1.8031e-02,  ...,  1.3182e-02,\n",
      "           -2.2252e-02,  1.3015e-02],\n",
      "          [-1.0529e-02, -8.8966e-03,  1.3231e-02,  ...,  9.1973e-03,\n",
      "           -2.2153e-02,  9.7795e-03],\n",
      "          [ 1.3971e-02,  7.3739e-05, -1.0975e-02,  ..., -1.1909e-02,\n",
      "            3.0728e-03, -1.3245e-03],\n",
      "          ...,\n",
      "          [-1.8722e-02, -1.1112e-02,  2.2112e-02,  ..., -1.6185e-02,\n",
      "           -7.9151e-03, -1.4012e-02],\n",
      "          [ 1.9181e-02, -4.2348e-04,  1.9569e-02,  ..., -1.0951e-02,\n",
      "           -3.6051e-03,  1.4279e-02],\n",
      "          [-1.3249e-02, -8.1482e-03,  1.5504e-02,  ...,  1.6324e-02,\n",
      "            2.8964e-03,  1.1970e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.0027e-03, -4.3254e-03,  1.8059e-02,  ...,  9.0608e-03,\n",
      "            1.6433e-02,  4.7514e-03],\n",
      "          [-7.3297e-03,  6.7450e-03, -2.0749e-02,  ..., -2.1129e-02,\n",
      "           -1.7697e-02, -1.7484e-02],\n",
      "          [-6.7801e-04,  1.7511e-03,  1.9235e-02,  ..., -1.7335e-02,\n",
      "           -9.5609e-03,  1.1427e-02],\n",
      "          ...,\n",
      "          [ 1.5182e-02,  9.5189e-03, -1.3116e-02,  ..., -1.5419e-02,\n",
      "            1.1238e-02,  1.0238e-02],\n",
      "          [ 1.5876e-02, -2.2043e-02,  5.0673e-03,  ...,  8.5195e-03,\n",
      "            2.0599e-02, -1.8081e-02],\n",
      "          [-1.6609e-02,  1.8909e-02,  4.4467e-03,  ..., -5.0890e-03,\n",
      "            6.8084e-03,  1.7079e-02]],\n",
      "\n",
      "         [[ 1.3888e-02, -6.1435e-03,  1.8965e-02,  ..., -4.4798e-03,\n",
      "           -2.1480e-02,  1.2806e-02],\n",
      "          [-3.5666e-03, -1.3394e-02,  5.5216e-03,  ..., -1.0663e-02,\n",
      "            1.7556e-02,  1.4484e-02],\n",
      "          [-4.4144e-03, -2.7824e-03,  1.1613e-02,  ...,  1.7978e-02,\n",
      "            3.6164e-03,  1.6213e-02],\n",
      "          ...,\n",
      "          [ 6.5168e-03, -2.2677e-02,  3.8462e-03,  ...,  9.9185e-03,\n",
      "            1.0965e-02,  1.8077e-02],\n",
      "          [-2.1950e-02, -7.8661e-03,  1.4561e-02,  ..., -1.9097e-02,\n",
      "           -6.1096e-03, -1.3664e-02],\n",
      "          [ 2.1860e-02, -1.9461e-02,  5.3359e-03,  ..., -1.7214e-02,\n",
      "           -2.2563e-02, -7.9327e-04]],\n",
      "\n",
      "         [[ 2.1930e-02, -2.1811e-02, -8.2961e-03,  ...,  1.3112e-02,\n",
      "           -1.7626e-02,  3.7686e-03],\n",
      "          [-1.1171e-02,  1.0020e-02,  1.5199e-02,  ...,  9.6507e-03,\n",
      "            1.1021e-02, -1.6544e-03],\n",
      "          [-1.6592e-03,  1.5695e-02,  1.8005e-02,  ...,  1.2043e-02,\n",
      "           -2.2316e-02,  2.0531e-02],\n",
      "          ...,\n",
      "          [ 7.9590e-03,  2.1184e-02,  6.4454e-05,  ...,  1.4536e-02,\n",
      "            2.3009e-03,  1.4779e-02],\n",
      "          [-2.7655e-04, -5.9636e-04,  2.0292e-02,  ...,  2.2112e-02,\n",
      "           -1.7039e-02, -1.0133e-02],\n",
      "          [ 2.4687e-03, -1.2695e-02,  1.0885e-03,  ...,  5.0274e-03,\n",
      "           -9.5705e-03,  9.0314e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8398e-02, -3.6240e-03,  1.5568e-02,  ...,  9.1697e-03,\n",
      "           -1.1255e-03, -1.6194e-02],\n",
      "          [-1.3110e-02,  1.3176e-02,  1.4252e-02,  ...,  1.6034e-02,\n",
      "            3.9257e-03,  6.1099e-03],\n",
      "          [-1.7384e-02,  2.0009e-02, -1.0797e-02,  ..., -1.5775e-03,\n",
      "           -1.5915e-02, -1.9586e-02],\n",
      "          ...,\n",
      "          [-2.1021e-02, -1.2227e-02, -5.0197e-03,  ...,  8.1010e-03,\n",
      "            1.5832e-02,  8.3910e-03],\n",
      "          [-1.1489e-02, -5.7271e-03,  2.2577e-02,  ..., -1.4916e-02,\n",
      "            2.2373e-02,  2.1892e-03],\n",
      "          [-1.7905e-02, -4.0676e-03, -8.6435e-03,  ...,  7.3950e-03,\n",
      "           -1.7733e-02, -8.1507e-03]],\n",
      "\n",
      "         [[-7.4631e-03, -1.5370e-02, -1.7337e-02,  ..., -1.5996e-02,\n",
      "            2.1924e-02, -9.7179e-03],\n",
      "          [ 6.0023e-03, -4.8449e-03,  1.7297e-02,  ...,  3.6974e-03,\n",
      "           -1.1684e-02, -6.7710e-03],\n",
      "          [ 3.7584e-03, -1.4278e-02, -7.4649e-03,  ...,  1.8591e-02,\n",
      "            1.5493e-02, -1.8699e-04],\n",
      "          ...,\n",
      "          [-1.8156e-02, -1.8005e-02, -1.1232e-02,  ..., -5.7079e-03,\n",
      "           -2.0049e-02, -4.7466e-03],\n",
      "          [-9.3287e-03, -2.1796e-02,  1.6897e-02,  ..., -3.4846e-03,\n",
      "            1.1161e-02, -1.0669e-02],\n",
      "          [-2.1623e-02,  1.4643e-02,  1.0652e-02,  ...,  3.1670e-03,\n",
      "            1.3370e-02, -6.7959e-03]],\n",
      "\n",
      "         [[ 6.6600e-03,  4.5773e-04,  9.3950e-03,  ...,  1.0884e-02,\n",
      "            1.6098e-02,  2.0708e-02],\n",
      "          [-8.9112e-03,  5.2030e-03,  1.1853e-02,  ..., -9.8059e-03,\n",
      "           -2.1676e-02, -1.9289e-02],\n",
      "          [ 7.2154e-03, -2.2646e-02, -1.5725e-02,  ...,  1.5535e-02,\n",
      "           -5.5616e-03,  6.6211e-03],\n",
      "          ...,\n",
      "          [ 2.4110e-03, -1.7123e-02,  4.3143e-03,  ..., -1.6758e-02,\n",
      "           -2.1548e-02, -1.6763e-02],\n",
      "          [-2.2169e-02, -4.9228e-03,  1.1988e-02,  ...,  1.9970e-02,\n",
      "            1.7264e-02, -5.0536e-03],\n",
      "          [ 1.2766e-02,  5.5328e-03, -2.1339e-02,  ...,  1.1479e-03,\n",
      "           -1.7752e-02, -1.0402e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.2290e-03, -2.1911e-02, -2.1052e-02,  ...,  1.2068e-02,\n",
      "           -5.6563e-03, -6.9996e-03],\n",
      "          [ 4.6714e-03,  2.1719e-02,  2.1896e-02,  ..., -4.1320e-03,\n",
      "           -4.1528e-03,  1.4149e-02],\n",
      "          [ 9.7108e-03,  1.9704e-02,  4.7830e-03,  ..., -1.7402e-02,\n",
      "           -1.0178e-02,  5.0007e-03],\n",
      "          ...,\n",
      "          [ 4.2352e-03, -1.8198e-02,  4.0848e-03,  ..., -1.0947e-02,\n",
      "           -2.1732e-02, -1.2447e-02],\n",
      "          [ 1.4554e-02,  1.5620e-03,  1.8648e-02,  ..., -1.3303e-02,\n",
      "            2.4491e-03,  5.2818e-03],\n",
      "          [-2.3192e-03,  1.0513e-02, -2.0891e-02,  ...,  7.3781e-03,\n",
      "           -1.7766e-02,  1.1319e-02]],\n",
      "\n",
      "         [[-2.1097e-02, -1.9247e-02,  1.3851e-02,  ..., -1.9584e-02,\n",
      "            1.5769e-02, -2.1442e-02],\n",
      "          [ 1.4700e-02, -2.1180e-03, -1.1347e-02,  ...,  1.3545e-02,\n",
      "            1.3810e-04,  2.1099e-02],\n",
      "          [ 1.8098e-02,  8.6537e-03,  9.5984e-03,  ...,  7.9325e-03,\n",
      "            1.5761e-02,  1.0843e-02],\n",
      "          ...,\n",
      "          [ 3.2821e-03,  2.7016e-03, -4.8404e-03,  ...,  9.0568e-04,\n",
      "           -2.0703e-02,  5.9386e-03],\n",
      "          [-2.1488e-02, -1.0389e-03,  2.7198e-04,  ..., -1.3696e-02,\n",
      "            3.3707e-03, -2.0317e-03],\n",
      "          [ 1.6569e-02, -2.1339e-02, -6.1290e-03,  ..., -2.2611e-02,\n",
      "           -3.0471e-03, -2.1476e-02]],\n",
      "\n",
      "         [[ 2.0452e-02, -9.5516e-03, -2.5439e-03,  ...,  2.1784e-03,\n",
      "            2.0199e-02,  1.3465e-02],\n",
      "          [ 2.0459e-02,  1.8875e-02,  5.8779e-03,  ..., -2.6330e-03,\n",
      "           -2.0115e-02, -2.0923e-02],\n",
      "          [-1.7828e-02,  2.1628e-02,  2.2464e-02,  ...,  7.9405e-03,\n",
      "           -1.5163e-02,  1.0248e-02],\n",
      "          ...,\n",
      "          [ 6.6904e-03, -1.1935e-02,  1.3154e-02,  ...,  1.4261e-02,\n",
      "           -1.2942e-02, -1.4376e-02],\n",
      "          [-1.0011e-02, -9.4659e-03,  1.1633e-02,  ...,  8.0845e-03,\n",
      "            1.1847e-02, -8.0934e-03],\n",
      "          [-2.1806e-02, -1.3954e-02, -1.8932e-02,  ...,  7.7857e-03,\n",
      "           -1.5009e-02,  2.0792e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 9.2710e-03,  1.4037e-02,  9.2149e-03,  ...,  1.9663e-03,\n",
      "           -1.3898e-02, -1.9405e-02],\n",
      "          [-2.2212e-02,  2.0775e-02,  1.9051e-02,  ...,  6.4407e-03,\n",
      "           -1.5741e-02,  2.0571e-02],\n",
      "          [ 1.9837e-02,  1.8797e-03, -9.1724e-04,  ..., -1.7642e-02,\n",
      "           -4.4433e-04,  1.6301e-02],\n",
      "          ...,\n",
      "          [-7.3506e-03,  1.8318e-02, -4.7287e-03,  ...,  7.1973e-05,\n",
      "            2.1411e-02, -5.9824e-03],\n",
      "          [ 2.0508e-02,  6.3494e-03,  8.5532e-03,  ..., -1.7058e-02,\n",
      "           -2.1287e-02, -8.9159e-03],\n",
      "          [ 3.3889e-03, -4.9679e-03,  6.5395e-03,  ...,  1.8284e-02,\n",
      "            7.5328e-03, -1.8885e-02]],\n",
      "\n",
      "         [[-5.7566e-03,  1.4389e-02,  1.2969e-02,  ..., -1.6405e-02,\n",
      "            1.3502e-02, -2.8486e-03],\n",
      "          [-2.1878e-02, -1.7219e-02,  1.8653e-02,  ..., -1.7826e-03,\n",
      "            1.9433e-02, -2.0119e-02],\n",
      "          [ 2.2128e-02, -1.1544e-02,  1.2527e-02,  ..., -1.1034e-02,\n",
      "            2.9504e-03, -1.4996e-02],\n",
      "          ...,\n",
      "          [ 1.8362e-02,  7.1258e-03,  1.2905e-02,  ...,  6.2603e-03,\n",
      "            1.5446e-02,  8.2925e-04],\n",
      "          [-1.6264e-02, -2.2133e-02, -1.2474e-03,  ..., -1.4518e-02,\n",
      "            5.6986e-03,  1.0117e-02],\n",
      "          [-2.1302e-02, -6.7496e-03,  1.8661e-03,  ..., -9.4111e-03,\n",
      "            2.8610e-03,  1.9673e-02]],\n",
      "\n",
      "         [[-7.6129e-03, -1.5370e-02,  2.0162e-02,  ...,  1.3538e-02,\n",
      "            2.2039e-03, -1.1795e-02],\n",
      "          [ 7.7022e-03, -2.7759e-03,  3.3521e-04,  ..., -1.5520e-02,\n",
      "            1.5059e-02,  2.5783e-03],\n",
      "          [ 1.6712e-02, -2.5610e-03,  1.8463e-02,  ..., -2.2643e-02,\n",
      "           -1.3410e-02,  1.4762e-02],\n",
      "          ...,\n",
      "          [-1.9552e-02,  3.1163e-03,  7.2791e-03,  ...,  1.0126e-02,\n",
      "           -5.9499e-03,  1.2323e-02],\n",
      "          [-2.0778e-03, -3.2999e-03, -2.2640e-02,  ...,  7.1531e-03,\n",
      "            1.7074e-02, -5.4162e-03],\n",
      "          [-1.8430e-02,  1.6582e-03,  7.4096e-03,  ...,  1.0599e-02,\n",
      "            9.7643e-03,  1.0625e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-8.2011e-03,  1.7486e-02,  5.4532e-03,  ...,  6.2399e-03,\n",
      "            1.7619e-03,  7.3095e-03],\n",
      "          [-1.5975e-02,  9.1640e-03, -6.0373e-03,  ...,  1.9505e-02,\n",
      "           -4.0635e-03,  1.2546e-02],\n",
      "          [ 2.8657e-04,  2.1963e-03,  1.7094e-02,  ..., -1.3483e-02,\n",
      "            1.4437e-02, -1.0737e-02],\n",
      "          ...,\n",
      "          [ 5.9849e-03, -1.5255e-02, -1.6504e-02,  ...,  1.0000e-02,\n",
      "            5.6241e-03,  4.4205e-03],\n",
      "          [ 1.4574e-04, -1.5835e-02,  6.9212e-03,  ...,  8.1221e-03,\n",
      "            2.1298e-02, -4.4645e-03],\n",
      "          [ 2.2652e-02, -1.3618e-02, -6.6505e-04,  ...,  2.0340e-02,\n",
      "            1.5653e-02, -6.4455e-03]],\n",
      "\n",
      "         [[ 9.5643e-03, -2.2057e-02,  7.9561e-03,  ..., -1.8478e-02,\n",
      "           -1.9491e-02, -6.9141e-03],\n",
      "          [-6.7306e-03, -1.3389e-02,  2.1509e-02,  ..., -1.4963e-02,\n",
      "            1.0320e-02, -2.1256e-02],\n",
      "          [ 1.0673e-02, -2.0737e-02, -2.2128e-02,  ..., -2.2300e-02,\n",
      "            5.7379e-03,  1.7105e-02],\n",
      "          ...,\n",
      "          [ 2.1722e-02, -2.9431e-03, -2.2430e-02,  ...,  8.5253e-03,\n",
      "           -4.2937e-04, -8.8402e-03],\n",
      "          [ 1.5644e-02, -2.0697e-02, -1.1537e-02,  ..., -7.6360e-03,\n",
      "            1.9066e-03,  1.9989e-02],\n",
      "          [-2.0685e-02, -1.1248e-02,  2.1629e-02,  ...,  2.8325e-03,\n",
      "            4.7991e-03, -7.7661e-03]],\n",
      "\n",
      "         [[ 3.0409e-03, -2.2268e-02, -2.2520e-02,  ..., -1.9549e-02,\n",
      "            2.7387e-03, -2.2284e-02],\n",
      "          [ 2.0342e-02, -7.4967e-03,  1.5689e-02,  ...,  7.1514e-04,\n",
      "           -2.2605e-02,  9.3572e-03],\n",
      "          [ 1.4273e-02,  1.1831e-02, -1.0703e-02,  ..., -6.8166e-03,\n",
      "            5.5475e-03,  4.0873e-03],\n",
      "          ...,\n",
      "          [ 1.8071e-03,  1.6477e-02, -5.7313e-03,  ...,  2.1837e-02,\n",
      "           -7.4727e-04, -1.7519e-02],\n",
      "          [ 7.4621e-03, -1.8529e-02,  3.0136e-03,  ...,  1.6455e-02,\n",
      "            7.2142e-03,  1.1779e-02],\n",
      "          [-2.2268e-03,  6.0389e-03, -5.7412e-03,  ...,  9.1087e-03,\n",
      "            1.0623e-03, -1.5178e-03]]]], requires_grad=True)\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "1\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "3\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "7\n",
      "tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.1690e-02,  ...,  1.0821e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  3.3349e-02,  ...,  2.7381e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.5373e-02,  ..., -2.8110e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00, -5.2395e-03,  ...,  2.4710e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  2.6082e-02,  ...,  7.0220e-04,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  1.7436e-02,  ...,  8.2441e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  1.2818e-02,  ..., -4.9501e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  2.4068e-02,  ...,  3.4891e-04,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  1.0766e-02,  ..., -1.9800e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  2.5217e-02,  ..., -3.0724e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00, -6.3907e-03,  ...,  2.8378e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  9.5849e-03,  ...,  9.4568e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00, -1.2620e-02,  ..., -3.4519e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00, -8.5323e-03,  ..., -1.0465e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4777e-02,  ...,  7.2016e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00, -1.3648e-03,  ..., -2.3909e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.2707e-02,  ...,  2.3777e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.1096e-02,  ...,  1.2589e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  1.7330e-02,  ...,  2.8385e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  2.8264e-02,  ...,  2.7908e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00, -1.2865e-02,  ...,  3.2337e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3178e-05,  ..., -2.6811e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  1.8797e-02,  ..., -9.9197e-04,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  2.3473e-02,  ..., -1.7781e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  2.6801e-02,  ..., -2.4021e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00, -1.1129e-02,  ...,  3.4424e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.8343e-02,  ..., -1.5523e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  3.3042e-02,  ..., -3.5738e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.9773e-02,  ..., -8.3549e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00, -1.2605e-02,  ..., -1.2254e-04,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  3.5644e-02,  ..., -1.6588e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00, -1.1408e-02,  ...,  8.2533e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.7719e-02,  ..., -3.1774e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.8124e-02,  ..., -1.7642e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00, -1.1425e-03,  ...,  6.6687e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4038e-02,  ...,  1.1627e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  2.9619e-02,  ..., -1.1007e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.0577e-02,  ..., -2.1436e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4025e-04,  ..., -2.2158e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.2271e-03,  ...,  1.5161e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.1212e-02,  ...,  3.4411e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  1.3898e-02,  ...,  2.6139e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00, -3.4675e-02,  ...,  1.3846e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00, -1.1510e-02,  ..., -9.3505e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  2.3209e-02,  ..., -2.1658e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00, -1.6186e-02,  ...,  3.5562e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.2355e-02,  ..., -3.2165e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  1.8890e-02,  ..., -1.2087e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  2.5211e-02,  ..., -2.8097e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  6.1546e-03,  ..., -3.4778e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  3.0935e-02,  ..., -1.2056e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  2.3790e-02,  ..., -7.1682e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  2.9868e-02,  ...,  2.7619e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00, -9.6720e-03,  ..., -3.0045e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  2.2406e-02,  ..., -3.1674e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  9.4968e-03,  ..., -2.6332e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00, -1.3634e-02,  ..., -2.5870e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  1.3059e-02,  ..., -2.1023e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00, -1.8530e-02,  ..., -3.4698e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  5.1424e-03,  ...,  3.0528e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00, -7.3159e-03,  ...,  6.9389e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  3.5517e-02,  ..., -1.2732e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.5438e-02,  ..., -1.4680e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  8.8823e-03,  ..., -7.7367e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.2193e-03,  ..., -2.6160e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00, -4.8244e-04,  ..., -1.0557e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3050e-03,  ...,  1.7248e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  2.2219e-02,  ..., -4.8545e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  6.5923e-03,  ..., -1.9825e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00, -4.6081e-03,  ...,  9.4480e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  1.9297e-02,  ...,  1.9481e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  1.6303e-03,  ...,  2.5823e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]]])\n",
      "Parameter containing:\n",
      "tensor([[[[ 1.8109e-02, -2.0485e-02,  5.2946e-03,  ...,  1.5188e-02,\n",
      "           -7.0638e-03,  5.3876e-03],\n",
      "          [-1.0938e-02, -2.8759e-03,  1.4745e-02,  ..., -7.8913e-03,\n",
      "           -2.0768e-02,  8.0515e-03],\n",
      "          [ 1.8660e-02, -2.1418e-02, -2.4735e-02,  ...,  1.2709e-03,\n",
      "            8.5726e-03,  7.6391e-03],\n",
      "          ...,\n",
      "          [ 1.2474e-02, -3.2262e-03,  3.3179e-02,  ...,  3.4628e-02,\n",
      "           -1.6129e-02,  1.1251e-02],\n",
      "          [-1.8547e-02, -2.0286e-02, -2.1047e-02,  ..., -1.9354e-02,\n",
      "            1.1459e-02,  1.6453e-02],\n",
      "          [-3.4359e-03,  5.9197e-03,  7.4856e-03,  ...,  2.0661e-03,\n",
      "            1.0975e-02, -1.5494e-02]],\n",
      "\n",
      "         [[ 4.3046e-03,  7.4427e-03, -1.5095e-02,  ...,  2.2059e-02,\n",
      "            2.0167e-02, -1.5006e-02],\n",
      "          [-7.5907e-03, -2.8238e-03, -4.0229e-03,  ...,  7.9141e-03,\n",
      "           -7.6794e-03,  1.7732e-02],\n",
      "          [-1.3106e-02,  1.9208e-02, -4.3014e-02,  ..., -4.5311e-02,\n",
      "            1.8761e-02, -1.8155e-02],\n",
      "          ...,\n",
      "          [-1.9190e-02, -1.5993e-02,  1.2691e-02,  ...,  1.7319e-02,\n",
      "            1.6207e-02, -2.1187e-02],\n",
      "          [ 1.9451e-02, -4.7563e-03,  4.0510e-03,  ...,  3.6922e-03,\n",
      "           -1.5047e-02, -6.3862e-03],\n",
      "          [ 1.1645e-02,  9.8081e-03, -4.0169e-03,  ...,  1.7872e-03,\n",
      "            4.9732e-03, -1.5422e-02]],\n",
      "\n",
      "         [[ 9.6917e-03,  1.8019e-02,  1.1072e-04,  ..., -5.7808e-03,\n",
      "            6.0760e-03,  1.3902e-03],\n",
      "          [ 6.1997e-03,  5.7043e-03,  1.6037e-02,  ...,  2.1324e-02,\n",
      "            9.2208e-03, -1.4132e-02],\n",
      "          [ 1.4110e-02,  9.3689e-03,  7.4489e-03,  ...,  7.1443e-04,\n",
      "            3.7025e-03,  2.8993e-03],\n",
      "          ...,\n",
      "          [-2.1314e-02,  3.4488e-03,  3.9229e-02,  ...,  1.5658e-02,\n",
      "            8.0582e-04, -1.4218e-02],\n",
      "          [ 1.3898e-02, -1.2502e-02, -9.5170e-03,  ...,  4.5808e-03,\n",
      "           -8.6745e-03,  1.2248e-02],\n",
      "          [ 1.8046e-02, -2.5477e-03, -1.4103e-02,  ..., -4.9156e-03,\n",
      "           -1.2048e-02, -3.0230e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.9855e-02, -1.9071e-02, -2.1872e-02,  ..., -6.2470e-03,\n",
      "           -6.8119e-03, -1.1706e-03],\n",
      "          [-1.8625e-02, -5.4878e-03,  8.2473e-03,  ..., -1.7421e-02,\n",
      "            6.8328e-03, -5.2530e-04],\n",
      "          [-1.4359e-02,  2.0205e-02, -2.0325e-03,  ..., -2.5883e-02,\n",
      "           -2.0475e-02,  2.0107e-03],\n",
      "          ...,\n",
      "          [ 1.5195e-02, -2.1218e-02,  2.0688e-02,  ..., -1.3399e-02,\n",
      "            1.1179e-02, -1.7942e-02],\n",
      "          [-1.2119e-02, -1.8121e-02,  3.8592e-03,  ...,  1.1185e-02,\n",
      "            1.0019e-02,  2.1154e-02],\n",
      "          [ 1.5767e-02,  6.6571e-03,  9.8442e-03,  ...,  2.0943e-02,\n",
      "            1.4505e-03,  1.5473e-02]],\n",
      "\n",
      "         [[-2.2533e-02, -1.5088e-02, -6.9118e-03,  ..., -6.5119e-03,\n",
      "           -1.3158e-02, -3.9110e-03],\n",
      "          [ 6.6004e-03,  9.5189e-03, -3.2527e-04,  ...,  1.1857e-02,\n",
      "            1.4834e-03, -6.2638e-03],\n",
      "          [-1.9040e-02,  2.1573e-02,  2.6233e-02,  ..., -3.2095e-02,\n",
      "            1.0070e-02,  6.3976e-03],\n",
      "          ...,\n",
      "          [-1.1992e-02,  2.2159e-02,  3.6231e-02,  ..., -1.2653e-02,\n",
      "            1.2387e-02,  1.5605e-02],\n",
      "          [ 1.0978e-02, -6.9413e-03,  1.4997e-02,  ..., -1.4104e-02,\n",
      "            2.8498e-03,  2.0800e-02],\n",
      "          [-4.1267e-03, -1.4948e-02, -1.9726e-03,  ...,  9.9044e-03,\n",
      "           -1.4588e-02,  3.9094e-03]],\n",
      "\n",
      "         [[-1.2597e-02, -1.0856e-02,  1.7806e-02,  ..., -3.3109e-03,\n",
      "            4.2294e-03,  1.1558e-02],\n",
      "          [-2.2112e-02, -2.0887e-02,  1.2626e-02,  ..., -1.5667e-02,\n",
      "           -7.5830e-03, -6.3956e-03],\n",
      "          [ 3.4575e-04, -1.0939e-02, -2.7198e-02,  ...,  2.3009e-02,\n",
      "           -7.3739e-03, -1.6999e-02],\n",
      "          ...,\n",
      "          [ 1.9798e-02, -2.2442e-02,  1.4676e-02,  ..., -9.1763e-03,\n",
      "           -1.1709e-02, -2.0819e-02],\n",
      "          [-1.9918e-02,  2.0290e-02,  9.1337e-03,  ...,  1.5053e-02,\n",
      "           -5.4448e-03,  1.1739e-02],\n",
      "          [ 1.3623e-02, -9.1779e-03,  3.0426e-04,  ..., -4.2383e-03,\n",
      "            2.1359e-02,  2.1344e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.1286e-02,  2.2576e-02, -8.4526e-03,  ...,  2.2168e-02,\n",
      "           -1.7883e-02,  1.7646e-02],\n",
      "          [-9.6213e-04, -1.5553e-02,  1.5350e-02,  ...,  4.4812e-03,\n",
      "            1.0516e-02, -2.2158e-03],\n",
      "          [-1.6701e-02, -8.8889e-03, -2.8593e-02,  ..., -5.0374e-03,\n",
      "           -2.2050e-02, -1.8530e-03],\n",
      "          ...,\n",
      "          [-5.5956e-04, -5.8117e-03, -4.0170e-03,  ...,  1.6906e-04,\n",
      "            5.6272e-03, -8.8095e-03],\n",
      "          [-9.9171e-05, -2.2108e-02, -2.1546e-02,  ..., -1.9733e-02,\n",
      "           -2.2659e-02,  3.1412e-03],\n",
      "          [-5.4781e-03, -1.2224e-02, -1.7038e-02,  ...,  1.7620e-02,\n",
      "           -7.3328e-03, -8.6369e-03]],\n",
      "\n",
      "         [[-1.1732e-02,  9.8155e-03, -1.7447e-02,  ...,  1.2140e-03,\n",
      "            1.3464e-02,  6.9319e-03],\n",
      "          [ 2.1788e-02,  2.1614e-02, -1.7821e-02,  ...,  4.8623e-03,\n",
      "            1.2592e-02,  8.8297e-03],\n",
      "          [ 1.6080e-02,  1.3897e-02, -3.8127e-02,  ...,  6.6349e-03,\n",
      "            2.2062e-02,  8.0065e-03],\n",
      "          ...,\n",
      "          [-1.9211e-02, -3.2202e-03,  4.0629e-03,  ..., -1.9310e-02,\n",
      "           -2.2509e-02, -2.0864e-02],\n",
      "          [ 1.7268e-02, -1.9023e-02,  2.8198e-04,  ..., -1.2279e-03,\n",
      "            1.1192e-02, -9.1453e-03],\n",
      "          [ 6.3237e-03,  1.0389e-02,  2.0564e-02,  ...,  1.5274e-02,\n",
      "           -1.1194e-02,  6.3133e-03]],\n",
      "\n",
      "         [[ 5.9179e-03, -1.8819e-02, -1.5997e-02,  ..., -3.3346e-04,\n",
      "            1.3726e-02,  1.2811e-02],\n",
      "          [-9.0349e-03,  1.5748e-03,  1.1271e-02,  ..., -9.0385e-03,\n",
      "            1.5829e-02,  1.7824e-02],\n",
      "          [ 1.2947e-02, -2.4599e-03, -8.5912e-03,  ...,  4.0717e-02,\n",
      "            7.5688e-03,  2.6515e-04],\n",
      "          ...,\n",
      "          [-1.1647e-02, -6.4886e-03, -3.6017e-02,  ...,  2.5242e-02,\n",
      "           -2.0377e-02,  1.4320e-02],\n",
      "          [-2.1139e-03, -1.7133e-02,  8.0747e-03,  ..., -1.0330e-02,\n",
      "           -1.1949e-02,  1.9506e-02],\n",
      "          [ 1.9728e-02, -5.4782e-03, -2.0227e-03,  ...,  1.4212e-03,\n",
      "            9.9942e-04,  1.6394e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.4079e-02,  1.5995e-02,  8.5388e-03,  ...,  9.0664e-03,\n",
      "            1.4836e-02,  1.2437e-02],\n",
      "          [ 9.6164e-03, -2.6811e-03, -7.9213e-03,  ..., -4.5768e-03,\n",
      "           -1.4386e-02,  1.5382e-02],\n",
      "          [-6.2153e-03, -2.0962e-02, -4.1532e-03,  ...,  1.5246e-02,\n",
      "            1.2489e-02,  4.8483e-03],\n",
      "          ...,\n",
      "          [ 1.7795e-02,  4.2753e-03,  2.9701e-02,  ...,  7.9479e-03,\n",
      "           -1.9641e-02, -1.2341e-02],\n",
      "          [-2.8884e-03, -4.1146e-03,  9.1075e-03,  ...,  4.6568e-03,\n",
      "            6.9238e-03, -5.8737e-03],\n",
      "          [-1.9913e-03, -1.5914e-02,  3.2222e-03,  ..., -1.0473e-02,\n",
      "           -1.7343e-02, -1.5590e-02]],\n",
      "\n",
      "         [[ 1.0301e-03,  2.1860e-02, -3.3163e-03,  ..., -1.7589e-02,\n",
      "           -1.3211e-02,  3.8635e-03],\n",
      "          [ 2.0434e-02, -6.3532e-03, -9.3882e-03,  ..., -1.5350e-02,\n",
      "            2.0937e-02,  2.3891e-03],\n",
      "          [ 1.2531e-02,  1.3195e-02, -2.9009e-02,  ...,  3.7977e-02,\n",
      "            1.7746e-02, -2.1170e-02],\n",
      "          ...,\n",
      "          [ 1.0498e-02, -8.9868e-03, -1.1511e-04,  ...,  1.9928e-02,\n",
      "            4.1484e-03,  2.0924e-02],\n",
      "          [ 1.5737e-02,  1.6137e-02, -1.1462e-03,  ...,  1.6396e-02,\n",
      "           -1.3076e-02, -1.1087e-02],\n",
      "          [ 1.8334e-02,  3.7660e-03, -3.8673e-03,  ...,  9.3072e-03,\n",
      "           -9.6438e-03, -1.7849e-02]],\n",
      "\n",
      "         [[-1.2115e-02, -1.0483e-03,  6.4863e-03,  ..., -1.8323e-02,\n",
      "           -1.7019e-03,  1.2776e-03],\n",
      "          [ 1.6787e-02, -4.2354e-03, -1.8669e-02,  ...,  1.2304e-02,\n",
      "           -1.0774e-03,  1.1373e-02],\n",
      "          [-1.0243e-02, -2.1836e-02,  2.1070e-02,  ...,  1.5816e-02,\n",
      "            5.2527e-03, -4.1510e-03],\n",
      "          ...,\n",
      "          [ 5.2596e-05,  1.7075e-02,  2.0164e-02,  ..., -4.4226e-03,\n",
      "           -1.2047e-02, -2.1632e-02],\n",
      "          [-1.8281e-03,  7.1539e-03,  1.3779e-02,  ...,  9.5900e-03,\n",
      "            5.7458e-03,  2.1714e-02],\n",
      "          [-1.0705e-02,  1.5101e-02, -2.5427e-03,  ..., -1.0197e-02,\n",
      "           -1.4091e-02,  1.4229e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 6.9979e-03, -1.2292e-02, -4.3431e-03,  ...,  2.2310e-02,\n",
      "            1.8332e-02, -4.1405e-03],\n",
      "          [ 1.2094e-02,  1.5117e-02,  8.9351e-03,  ..., -1.1053e-02,\n",
      "           -2.2265e-02,  4.8214e-03],\n",
      "          [ 1.3300e-02, -6.4013e-03,  3.4072e-02,  ..., -1.9585e-02,\n",
      "           -1.9479e-02, -1.8179e-02],\n",
      "          ...,\n",
      "          [-1.2999e-02,  1.6452e-02, -1.7882e-02,  ...,  2.6793e-02,\n",
      "            1.6342e-02,  4.8434e-05],\n",
      "          [-1.9704e-03,  1.7379e-04,  8.0086e-03,  ..., -2.7438e-03,\n",
      "           -4.5063e-03,  2.0979e-02],\n",
      "          [-8.6926e-03, -9.9459e-03,  1.8813e-03,  ...,  1.9044e-03,\n",
      "            1.8594e-02,  4.0757e-03]],\n",
      "\n",
      "         [[-1.7791e-02,  1.5144e-02,  1.5235e-02,  ..., -1.2484e-02,\n",
      "           -1.7082e-02,  1.9066e-02],\n",
      "          [-2.1954e-02, -1.1016e-02,  9.2578e-03,  ..., -1.1711e-02,\n",
      "            2.0832e-02, -2.2453e-02],\n",
      "          [ 1.1555e-02, -1.8234e-02, -3.1331e-02,  ..., -2.6612e-02,\n",
      "           -9.7565e-03,  2.7166e-03],\n",
      "          ...,\n",
      "          [-1.7363e-02, -1.8232e-02,  2.7397e-02,  ..., -6.7311e-03,\n",
      "            2.0473e-03, -1.9258e-02],\n",
      "          [ 5.1088e-03,  1.9255e-02, -2.2715e-02,  ..., -1.6001e-02,\n",
      "            2.0740e-02,  1.5105e-03],\n",
      "          [-5.9326e-04,  3.0945e-03,  1.1818e-02,  ...,  1.4406e-02,\n",
      "           -7.8926e-03, -6.3831e-03]],\n",
      "\n",
      "         [[-1.6798e-02,  1.2117e-02, -7.4709e-03,  ..., -2.1267e-02,\n",
      "            2.2178e-02, -7.8679e-03],\n",
      "          [ 1.6672e-03, -1.7623e-02, -7.7323e-03,  ...,  1.7682e-02,\n",
      "           -1.3038e-02, -2.1425e-02],\n",
      "          [-1.7921e-02,  2.2568e-02, -2.5772e-02,  ...,  3.6560e-03,\n",
      "            2.0615e-02,  1.6814e-02],\n",
      "          ...,\n",
      "          [-1.4681e-02,  1.1884e-02,  8.3109e-03,  ..., -5.4670e-03,\n",
      "           -7.4426e-03, -9.3698e-03],\n",
      "          [-1.6926e-02, -1.9732e-02,  1.8262e-02,  ...,  6.1135e-03,\n",
      "            1.6639e-02, -1.4644e-02],\n",
      "          [-3.0033e-03,  7.2607e-03,  1.7058e-02,  ..., -3.8019e-03,\n",
      "            4.8815e-03,  1.6271e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-9.3885e-03, -4.8883e-03,  2.7457e-03,  ..., -9.5636e-03,\n",
      "           -9.2917e-03, -9.5674e-03],\n",
      "          [-1.5343e-02,  1.7980e-02,  8.2250e-03,  ...,  1.9754e-02,\n",
      "            1.4570e-02, -2.6882e-03],\n",
      "          [-2.0936e-02,  1.6084e-02,  3.6909e-02,  ..., -2.7591e-02,\n",
      "           -3.5637e-03,  1.2645e-02],\n",
      "          ...,\n",
      "          [ 1.7969e-02, -1.5820e-02, -7.8938e-03,  ..., -5.8706e-03,\n",
      "           -1.9012e-02, -1.1040e-02],\n",
      "          [ 8.0502e-03,  1.6607e-02,  8.6041e-03,  ..., -2.0461e-02,\n",
      "           -1.0823e-03, -1.8841e-02],\n",
      "          [-1.4281e-02,  1.2025e-03,  2.3169e-03,  ...,  1.9681e-02,\n",
      "            3.7054e-03, -2.2056e-02]],\n",
      "\n",
      "         [[-1.1362e-03,  2.1937e-02,  1.7873e-02,  ..., -1.4961e-02,\n",
      "            2.5140e-03,  1.5648e-02],\n",
      "          [-1.6006e-03, -6.2006e-03, -1.1167e-03,  ...,  1.8014e-02,\n",
      "           -1.1761e-02,  4.4439e-03],\n",
      "          [-2.1281e-02, -1.3029e-02, -1.7436e-02,  ..., -2.0429e-02,\n",
      "           -2.2493e-02,  2.2564e-02],\n",
      "          ...,\n",
      "          [ 1.1383e-02, -1.6011e-02, -1.8898e-02,  ..., -2.3873e-02,\n",
      "           -1.7137e-02, -8.5114e-03],\n",
      "          [ 2.0049e-02, -1.0905e-02, -1.2128e-02,  ...,  2.0908e-02,\n",
      "            3.8433e-03, -1.4652e-02],\n",
      "          [ 1.7953e-02, -4.8520e-03,  1.0436e-02,  ...,  1.2164e-03,\n",
      "           -7.9491e-03, -4.1335e-03]],\n",
      "\n",
      "         [[-1.8501e-02, -1.2117e-02, -2.9985e-03,  ...,  3.4956e-03,\n",
      "            1.8291e-02, -1.2542e-02],\n",
      "          [ 1.4985e-03,  9.4044e-03, -1.0367e-02,  ...,  1.7077e-02,\n",
      "            1.6847e-02, -1.4965e-02],\n",
      "          [-3.5668e-03,  1.7118e-02, -2.5983e-04,  ...,  1.2283e-02,\n",
      "           -4.4474e-03,  2.2288e-02],\n",
      "          ...,\n",
      "          [ 1.1122e-02,  2.1620e-02, -4.8208e-02,  ...,  1.2204e-02,\n",
      "            5.7216e-03, -1.9570e-02],\n",
      "          [ 6.0990e-04,  7.1543e-03,  2.2093e-02,  ...,  1.6245e-02,\n",
      "           -3.9893e-03, -3.5606e-04],\n",
      "          [-2.0282e-02, -1.8040e-02,  2.1476e-02,  ...,  1.5956e-03,\n",
      "            1.6167e-02, -1.4140e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-5.6494e-03,  7.1688e-03,  1.3667e-02,  ...,  1.1899e-02,\n",
      "           -7.4006e-04,  6.6634e-03],\n",
      "          [-1.9340e-03,  1.3104e-02,  1.1349e-02,  ...,  2.0246e-02,\n",
      "            1.4709e-02,  4.2860e-03],\n",
      "          [-3.6218e-03,  1.3910e-02,  5.1007e-02,  ...,  9.4950e-03,\n",
      "            3.1793e-03,  1.1723e-02],\n",
      "          ...,\n",
      "          [ 1.0887e-03, -1.7977e-02, -2.7458e-02,  ..., -4.1679e-02,\n",
      "            2.0133e-02,  2.0167e-02],\n",
      "          [ 2.6829e-03, -1.0590e-02, -1.8966e-02,  ...,  4.0776e-03,\n",
      "            8.3583e-03,  3.0868e-03],\n",
      "          [ 1.6167e-02, -2.5122e-03,  1.6505e-02,  ..., -7.0042e-03,\n",
      "            1.7153e-02,  1.6997e-02]],\n",
      "\n",
      "         [[-7.8715e-03, -9.2376e-03, -1.0939e-02,  ...,  4.6003e-03,\n",
      "           -1.5675e-02,  1.1551e-02],\n",
      "          [-1.7284e-02,  8.2942e-03,  1.3424e-02,  ...,  1.5319e-02,\n",
      "            1.1930e-02,  7.6626e-03],\n",
      "          [-2.2596e-02,  5.6918e-03, -1.4279e-03,  ..., -3.9822e-02,\n",
      "           -1.1754e-02,  1.8492e-02],\n",
      "          ...,\n",
      "          [ 2.1381e-02,  1.6851e-02,  1.3534e-02,  ...,  3.6444e-02,\n",
      "            1.7082e-02,  1.8022e-02],\n",
      "          [ 2.1025e-02, -1.4230e-02,  8.4459e-03,  ..., -8.1787e-03,\n",
      "           -1.4496e-02,  6.7964e-03],\n",
      "          [ 1.9432e-02,  1.1985e-02, -4.9012e-03,  ..., -2.1934e-02,\n",
      "           -2.5637e-03,  9.5481e-03]],\n",
      "\n",
      "         [[ 7.9007e-03,  6.2072e-03, -1.8031e-02,  ...,  1.3182e-02,\n",
      "           -2.2252e-02,  1.3015e-02],\n",
      "          [-1.0529e-02, -8.8966e-03,  1.3231e-02,  ...,  9.1973e-03,\n",
      "           -2.2153e-02,  9.7795e-03],\n",
      "          [ 1.3971e-02,  7.3739e-05, -3.2187e-02,  ...,  2.2502e-02,\n",
      "            3.0728e-03, -1.3245e-03],\n",
      "          ...,\n",
      "          [-1.8722e-02, -1.1112e-02,  3.6010e-02,  ...,  9.9542e-03,\n",
      "           -7.9151e-03, -1.4012e-02],\n",
      "          [ 1.9181e-02, -4.2348e-04,  1.9569e-02,  ..., -1.0951e-02,\n",
      "           -3.6051e-03,  1.4279e-02],\n",
      "          [-1.3249e-02, -8.1482e-03,  1.5504e-02,  ...,  1.6324e-02,\n",
      "            2.8964e-03,  1.1970e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.0027e-03, -4.3254e-03,  1.8059e-02,  ...,  9.0608e-03,\n",
      "            1.6433e-02,  4.7514e-03],\n",
      "          [-7.3297e-03,  6.7450e-03, -2.0749e-02,  ..., -2.1129e-02,\n",
      "           -1.7697e-02, -1.7484e-02],\n",
      "          [-6.7801e-04,  1.7511e-03, -1.5440e-02,  ..., -3.4896e-03,\n",
      "           -9.5609e-03,  1.1427e-02],\n",
      "          ...,\n",
      "          [ 1.5182e-02,  9.5189e-03, -2.4626e-02,  ..., -2.4769e-02,\n",
      "            1.1238e-02,  1.0238e-02],\n",
      "          [ 1.5876e-02, -2.2043e-02,  5.0673e-03,  ...,  8.5195e-03,\n",
      "            2.0599e-02, -1.8081e-02],\n",
      "          [-1.6609e-02,  1.8909e-02,  4.4467e-03,  ..., -5.0890e-03,\n",
      "            6.8084e-03,  1.7079e-02]],\n",
      "\n",
      "         [[ 1.3888e-02, -6.1435e-03,  1.8965e-02,  ..., -4.4798e-03,\n",
      "           -2.1480e-02,  1.2806e-02],\n",
      "          [-3.5666e-03, -1.3394e-02,  5.5216e-03,  ..., -1.0663e-02,\n",
      "            1.7556e-02,  1.4484e-02],\n",
      "          [-4.4144e-03, -2.7824e-03,  3.4822e-02,  ..., -3.6803e-03,\n",
      "            3.6164e-03,  1.6213e-02],\n",
      "          ...,\n",
      "          [ 6.5168e-03, -2.2677e-02, -1.2339e-02,  ...,  4.5480e-02,\n",
      "            1.0965e-02,  1.8077e-02],\n",
      "          [-2.1950e-02, -7.8661e-03,  1.4561e-02,  ..., -1.9097e-02,\n",
      "           -6.1096e-03, -1.3664e-02],\n",
      "          [ 2.1860e-02, -1.9461e-02,  5.3359e-03,  ..., -1.7214e-02,\n",
      "           -2.2563e-02, -7.9327e-04]],\n",
      "\n",
      "         [[ 2.1930e-02, -2.1811e-02, -8.2961e-03,  ...,  1.3112e-02,\n",
      "           -1.7626e-02,  3.7686e-03],\n",
      "          [-1.1171e-02,  1.0020e-02,  1.5199e-02,  ...,  9.6507e-03,\n",
      "            1.1021e-02, -1.6544e-03],\n",
      "          [-1.6592e-03,  1.5695e-02, -4.3503e-03,  ...,  8.8264e-03,\n",
      "           -2.2316e-02,  2.0531e-02],\n",
      "          ...,\n",
      "          [ 7.9590e-03,  2.1184e-02,  1.8955e-02,  ...,  2.4493e-03,\n",
      "            2.3009e-03,  1.4779e-02],\n",
      "          [-2.7655e-04, -5.9636e-04,  2.0292e-02,  ...,  2.2112e-02,\n",
      "           -1.7039e-02, -1.0133e-02],\n",
      "          [ 2.4687e-03, -1.2695e-02,  1.0885e-03,  ...,  5.0274e-03,\n",
      "           -9.5705e-03,  9.0314e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8398e-02, -3.6240e-03,  1.5568e-02,  ...,  9.1697e-03,\n",
      "           -1.1255e-03, -1.6194e-02],\n",
      "          [-1.3110e-02,  1.3176e-02,  1.4252e-02,  ...,  1.6034e-02,\n",
      "            3.9257e-03,  6.1099e-03],\n",
      "          [-1.7384e-02,  2.0009e-02,  1.4415e-02,  ..., -2.9674e-02,\n",
      "           -1.5915e-02, -1.9586e-02],\n",
      "          ...,\n",
      "          [-2.1021e-02, -1.2227e-02,  1.1349e-03,  ..., -2.6677e-02,\n",
      "            1.5832e-02,  8.3910e-03],\n",
      "          [-1.1489e-02, -5.7271e-03,  2.2577e-02,  ..., -1.4916e-02,\n",
      "            2.2373e-02,  2.1892e-03],\n",
      "          [-1.7905e-02, -4.0676e-03, -8.6435e-03,  ...,  7.3950e-03,\n",
      "           -1.7733e-02, -8.1507e-03]],\n",
      "\n",
      "         [[-7.4631e-03, -1.5370e-02, -1.7337e-02,  ..., -1.5996e-02,\n",
      "            2.1924e-02, -9.7179e-03],\n",
      "          [ 6.0023e-03, -4.8449e-03,  1.7297e-02,  ...,  3.6974e-03,\n",
      "           -1.1684e-02, -6.7710e-03],\n",
      "          [ 3.7584e-03, -1.4278e-02,  2.3470e-02,  ...,  6.5353e-03,\n",
      "            1.5493e-02, -1.8699e-04],\n",
      "          ...,\n",
      "          [-1.8156e-02, -1.8005e-02,  1.2558e-02,  ..., -1.2876e-02,\n",
      "           -2.0049e-02, -4.7466e-03],\n",
      "          [-9.3287e-03, -2.1796e-02,  1.6897e-02,  ..., -3.4846e-03,\n",
      "            1.1161e-02, -1.0669e-02],\n",
      "          [-2.1623e-02,  1.4643e-02,  1.0652e-02,  ...,  3.1670e-03,\n",
      "            1.3370e-02, -6.7959e-03]],\n",
      "\n",
      "         [[ 6.6600e-03,  4.5773e-04,  9.3950e-03,  ...,  1.0884e-02,\n",
      "            1.6098e-02,  2.0708e-02],\n",
      "          [-8.9112e-03,  5.2030e-03,  1.1853e-02,  ..., -9.8059e-03,\n",
      "           -2.1676e-02, -1.9289e-02],\n",
      "          [ 7.2154e-03, -2.2646e-02,  1.4142e-02,  ...,  4.3154e-02,\n",
      "           -5.5616e-03,  6.6211e-03],\n",
      "          ...,\n",
      "          [ 2.4110e-03, -1.7123e-02, -5.3577e-03,  ..., -4.6803e-02,\n",
      "           -2.1548e-02, -1.6763e-02],\n",
      "          [-2.2169e-02, -4.9228e-03,  1.1988e-02,  ...,  1.9970e-02,\n",
      "            1.7264e-02, -5.0536e-03],\n",
      "          [ 1.2766e-02,  5.5328e-03, -2.1339e-02,  ...,  1.1479e-03,\n",
      "           -1.7752e-02, -1.0402e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.2290e-03, -2.1911e-02, -2.1052e-02,  ...,  1.2068e-02,\n",
      "           -5.6563e-03, -6.9996e-03],\n",
      "          [ 4.6714e-03,  2.1719e-02,  2.1896e-02,  ..., -4.1320e-03,\n",
      "           -4.1528e-03,  1.4149e-02],\n",
      "          [ 9.7108e-03,  1.9704e-02,  2.7189e-02,  ..., -4.9076e-02,\n",
      "           -1.0178e-02,  5.0007e-03],\n",
      "          ...,\n",
      "          [ 4.2352e-03, -1.8198e-02,  1.3582e-02,  ..., -3.7279e-02,\n",
      "           -2.1732e-02, -1.2447e-02],\n",
      "          [ 1.4554e-02,  1.5620e-03,  1.8648e-02,  ..., -1.3303e-02,\n",
      "            2.4491e-03,  5.2818e-03],\n",
      "          [-2.3192e-03,  1.0513e-02, -2.0891e-02,  ...,  7.3781e-03,\n",
      "           -1.7766e-02,  1.1319e-02]],\n",
      "\n",
      "         [[-2.1097e-02, -1.9247e-02,  1.3851e-02,  ..., -1.9584e-02,\n",
      "            1.5769e-02, -2.1442e-02],\n",
      "          [ 1.4700e-02, -2.1180e-03, -1.1347e-02,  ...,  1.3545e-02,\n",
      "            1.3810e-04,  2.1099e-02],\n",
      "          [ 1.8098e-02,  8.6537e-03, -4.0359e-03,  ..., -1.7937e-02,\n",
      "            1.5761e-02,  1.0843e-02],\n",
      "          ...,\n",
      "          [ 3.2821e-03,  2.7016e-03,  8.2188e-03,  ..., -1.1966e-03,\n",
      "           -2.0703e-02,  5.9386e-03],\n",
      "          [-2.1488e-02, -1.0389e-03,  2.7198e-04,  ..., -1.3696e-02,\n",
      "            3.3707e-03, -2.0317e-03],\n",
      "          [ 1.6569e-02, -2.1339e-02, -6.1290e-03,  ..., -2.2611e-02,\n",
      "           -3.0471e-03, -2.1476e-02]],\n",
      "\n",
      "         [[ 2.0452e-02, -9.5516e-03, -2.5439e-03,  ...,  2.1784e-03,\n",
      "            2.0199e-02,  1.3465e-02],\n",
      "          [ 2.0459e-02,  1.8875e-02,  5.8779e-03,  ..., -2.6330e-03,\n",
      "           -2.0115e-02, -2.0923e-02],\n",
      "          [-1.7828e-02,  2.1628e-02,  3.9341e-03,  ..., -2.6758e-02,\n",
      "           -1.5163e-02,  1.0248e-02],\n",
      "          ...,\n",
      "          [ 6.6904e-03, -1.1935e-02,  1.8296e-02,  ...,  4.4788e-02,\n",
      "           -1.2942e-02, -1.4376e-02],\n",
      "          [-1.0011e-02, -9.4659e-03,  1.1633e-02,  ...,  8.0845e-03,\n",
      "            1.1847e-02, -8.0934e-03],\n",
      "          [-2.1806e-02, -1.3954e-02, -1.8932e-02,  ...,  7.7857e-03,\n",
      "           -1.5009e-02,  2.0792e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 9.2710e-03,  1.4037e-02,  9.2149e-03,  ...,  1.9663e-03,\n",
      "           -1.3898e-02, -1.9405e-02],\n",
      "          [-2.2212e-02,  2.0775e-02,  1.9051e-02,  ...,  6.4407e-03,\n",
      "           -1.5741e-02,  2.0571e-02],\n",
      "          [ 1.9837e-02,  1.8797e-03, -8.2331e-03,  ..., -1.0703e-02,\n",
      "           -4.4433e-04,  1.6301e-02],\n",
      "          ...,\n",
      "          [-7.3506e-03,  1.8318e-02,  3.0788e-02,  ..., -1.2660e-02,\n",
      "            2.1411e-02, -5.9824e-03],\n",
      "          [ 2.0508e-02,  6.3494e-03,  8.5532e-03,  ..., -1.7058e-02,\n",
      "           -2.1287e-02, -8.9159e-03],\n",
      "          [ 3.3889e-03, -4.9679e-03,  6.5395e-03,  ...,  1.8284e-02,\n",
      "            7.5328e-03, -1.8885e-02]],\n",
      "\n",
      "         [[-5.7566e-03,  1.4389e-02,  1.2969e-02,  ..., -1.6405e-02,\n",
      "            1.3502e-02, -2.8486e-03],\n",
      "          [-2.1878e-02, -1.7219e-02,  1.8653e-02,  ..., -1.7826e-03,\n",
      "            1.9433e-02, -2.0119e-02],\n",
      "          [ 2.2128e-02, -1.1544e-02, -1.2911e-02,  ..., -2.5714e-02,\n",
      "            2.9504e-03, -1.4996e-02],\n",
      "          ...,\n",
      "          [ 1.8362e-02,  7.1258e-03,  2.1787e-02,  ..., -1.4764e-03,\n",
      "            1.5446e-02,  8.2925e-04],\n",
      "          [-1.6264e-02, -2.2133e-02, -1.2474e-03,  ..., -1.4518e-02,\n",
      "            5.6986e-03,  1.0117e-02],\n",
      "          [-2.1302e-02, -6.7496e-03,  1.8661e-03,  ..., -9.4111e-03,\n",
      "            2.8610e-03,  1.9673e-02]],\n",
      "\n",
      "         [[-7.6129e-03, -1.5370e-02,  2.0162e-02,  ...,  1.3538e-02,\n",
      "            2.2039e-03, -1.1795e-02],\n",
      "          [ 7.7022e-03, -2.7759e-03,  3.3521e-04,  ..., -1.5520e-02,\n",
      "            1.5059e-02,  2.5783e-03],\n",
      "          [ 1.6712e-02, -2.5610e-03,  1.6243e-02,  ..., -4.8803e-02,\n",
      "           -1.3410e-02,  1.4762e-02],\n",
      "          ...,\n",
      "          [-1.9552e-02,  3.1163e-03,  6.7966e-03,  ..., -4.3113e-04,\n",
      "           -5.9499e-03,  1.2323e-02],\n",
      "          [-2.0778e-03, -3.2999e-03, -2.2640e-02,  ...,  7.1531e-03,\n",
      "            1.7074e-02, -5.4162e-03],\n",
      "          [-1.8430e-02,  1.6582e-03,  7.4096e-03,  ...,  1.0599e-02,\n",
      "            9.7643e-03,  1.0625e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-8.2011e-03,  1.7486e-02,  5.4532e-03,  ...,  6.2399e-03,\n",
      "            1.7619e-03,  7.3095e-03],\n",
      "          [-1.5975e-02,  9.1640e-03, -6.0373e-03,  ...,  1.9505e-02,\n",
      "           -4.0635e-03,  1.2546e-02],\n",
      "          [ 2.8657e-04,  2.1963e-03,  1.4789e-02,  ...,  3.7650e-03,\n",
      "            1.4437e-02, -1.0737e-02],\n",
      "          ...,\n",
      "          [ 5.9849e-03, -1.5255e-02,  5.7151e-03,  ...,  5.1456e-03,\n",
      "            5.6241e-03,  4.4205e-03],\n",
      "          [ 1.4574e-04, -1.5835e-02,  6.9212e-03,  ...,  8.1221e-03,\n",
      "            2.1298e-02, -4.4645e-03],\n",
      "          [ 2.2652e-02, -1.3618e-02, -6.6505e-04,  ...,  2.0340e-02,\n",
      "            1.5653e-02, -6.4455e-03]],\n",
      "\n",
      "         [[ 9.5643e-03, -2.2057e-02,  7.9561e-03,  ..., -1.8478e-02,\n",
      "           -1.9491e-02, -6.9141e-03],\n",
      "          [-6.7306e-03, -1.3389e-02,  2.1509e-02,  ..., -1.4963e-02,\n",
      "            1.0320e-02, -2.1256e-02],\n",
      "          [ 1.0673e-02, -2.0737e-02, -1.5535e-02,  ..., -4.2125e-02,\n",
      "            5.7379e-03,  1.7105e-02],\n",
      "          ...,\n",
      "          [ 2.1722e-02, -2.9431e-03, -2.7038e-02,  ...,  1.7973e-02,\n",
      "           -4.2937e-04, -8.8402e-03],\n",
      "          [ 1.5644e-02, -2.0697e-02, -1.1537e-02,  ..., -7.6360e-03,\n",
      "            1.9066e-03,  1.9989e-02],\n",
      "          [-2.0685e-02, -1.1248e-02,  2.1629e-02,  ...,  2.8325e-03,\n",
      "            4.7991e-03, -7.7661e-03]],\n",
      "\n",
      "         [[ 3.0409e-03, -2.2268e-02, -2.2520e-02,  ..., -1.9549e-02,\n",
      "            2.7387e-03, -2.2284e-02],\n",
      "          [ 2.0342e-02, -7.4967e-03,  1.5689e-02,  ...,  7.1514e-04,\n",
      "           -2.2605e-02,  9.3572e-03],\n",
      "          [ 1.4273e-02,  1.1831e-02,  8.5937e-03,  ...,  1.2664e-02,\n",
      "            5.5475e-03,  4.0873e-03],\n",
      "          ...,\n",
      "          [ 1.8071e-03,  1.6477e-02, -4.1010e-03,  ...,  4.7660e-02,\n",
      "           -7.4727e-04, -1.7519e-02],\n",
      "          [ 7.4621e-03, -1.8529e-02,  3.0136e-03,  ...,  1.6455e-02,\n",
      "            7.2142e-03,  1.1779e-02],\n",
      "          [-2.2268e-03,  6.0389e-03, -5.7412e-03,  ...,  9.1087e-03,\n",
      "            1.0623e-03, -1.5178e-03]]]], requires_grad=True) 11111\n",
      "RepConv2d(\n",
      "  (fused_conv): Conv2d(16, 16, kernel_size=(11, 11), stride=(1, 1), padding=(15, 15), dilation=(3, 3))\n",
      ")\n",
      "The outputs are the same before and after deployment.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create the RepConv2d model with kernel sizes that include 0\n",
    "model = RepConv2d(in_channels=16, out_channels=16, kernel_sizes=(0,1, 3,7,11), stride=1, dilation=3, groups=1, use_bn=False, deploy=False).eval()\n",
    "print(model)\n",
    "#initialize_conv_weights_to_one(model)\n",
    "# Create a dummy input tensor (e.g., batch size of 1, 3 channels, 224x224 image)\n",
    "x = torch.randn(1, 16, 224, 224)\n",
    "\n",
    "# Run the model before deployment (non-deployed state)\n",
    "\n",
    "output_before_deploy = model(x)\n",
    "\n",
    "# Switch the model to deploy mode\n",
    "model.switch_to_deploy()\n",
    "print(model)\n",
    "# Run the model after deployment (fused conv state)\n",
    "\n",
    "output_after_deploy = model(x)\n",
    "\n",
    "# Check if the outputs are \n",
    "# the same\n",
    "if torch.allclose(output_before_deploy, output_after_deploy, atol=1e-5):\n",
    "    print(\"The outputs are the same before and after deployment.\")\n",
    "else:\n",
    "    print(\"The outputs differ before and after deployment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  2.7438,   3.8604,   0.7208,  ...,   5.0495,  -3.1655,   0.6610],\n",
       "          [ -6.2636,  -6.7629,   1.0658,  ...,   8.5792,   4.3009,  -2.7788],\n",
       "          [ -3.7285,  -2.6189,   0.3735,  ...,  13.6187,   2.4866,   2.6248],\n",
       "          ...,\n",
       "          [ -7.9906, -14.5445,  -9.1656,  ..., -11.7440,  -1.0764,   5.0076],\n",
       "          [ -5.1502, -10.2577,  -5.7804,  ...,   5.8917,   2.9787,  -2.1047],\n",
       "          [ -1.5465,  -1.5369,  -1.9339,  ...,   5.5483,   3.7897,  -7.4650]]]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_before_deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  2.7438,   3.8604,   0.7208,  ...,   5.0495,  -3.1655,   0.6610],\n",
       "          [ -6.2636,  -6.7629,   1.0658,  ...,   8.5792,   4.3009,  -2.7788],\n",
       "          [ -3.7285,  -2.6189,   0.3735,  ...,  13.6187,   2.4866,   2.6248],\n",
       "          ...,\n",
       "          [ -7.9906, -14.5445,  -9.1656,  ..., -11.7440,  -1.0764,   5.0076],\n",
       "          [ -5.1502, -10.2577,  -5.7804,  ...,   5.8917,   2.9787,  -2.1047],\n",
       "          [ -1.5465,  -1.5369,  -1.9339,  ...,   5.5483,   3.7897,  -7.4650]]]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_after_deploy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class ConvBn(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', deploy=False):\n",
    "        super(ConvBn, self).__init__()\n",
    "        self.deploy = deploy\n",
    "        if deploy:\n",
    "            self.fused_conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(kernel_size, kernel_size), stride=stride,\n",
    "                                      padding=padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode)\n",
    "        else:\n",
    "            self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                         kernel_size=(kernel_size, kernel_size), stride=stride,\n",
    "                                         padding=padding, dilation=dilation, groups=groups, bias=False,\n",
    "                                         padding_mode=padding_mode)\n",
    "            self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
    "\n",
    "    def _fuse_bn_tensor(self, conv, bn):\n",
    "        std = (bn.running_var + bn.eps).sqrt()\n",
    "        t = (bn.weight / std).reshape(-1, 1, 1, 1)\n",
    "        return conv.weight * t, bn.bias - bn.running_mean * bn.weight / std\n",
    "\n",
    "    def switch_to_deploy(self):\n",
    "        if self.bn.training:\n",
    "            raise RuntimeError(\"BatchNorm should be in evaluation mode (eval) before deployment.\")\n",
    "        deploy_k, deploy_b = self._fuse_bn_tensor(self.conv, self.bn)\n",
    "        self.deploy = True\n",
    "        self.fused_conv = nn.Conv2d(in_channels=self.conv.in_channels, out_channels=self.conv.out_channels,\n",
    "                                    kernel_size=self.conv.kernel_size, stride=self.conv.stride,\n",
    "                                    padding=self.conv.padding, dilation=self.conv.dilation, groups=self.conv.groups, bias=True,\n",
    "                                    padding_mode=self.conv.padding_mode)\n",
    "        self.__delattr__('conv')\n",
    "        self.__delattr__('bn')\n",
    "        self.fused_conv.weight.data = deploy_k\n",
    "        self.fused_conv.bias.data = deploy_b\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.deploy:\n",
    "            return self.fused_conv(input)\n",
    "        else:\n",
    "            square_outputs = self.conv(input)\n",
    "            square_outputs = self.bn(square_outputs)\n",
    "            return square_outputs\n",
    "class RepConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_sizes=(0, 3, 7, 11), stride=1, dilation=1, groups=1, use_bn=True, deploy=False):\n",
    "        super(RepConv2d, self).__init__()\n",
    "        self.deploy = deploy\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_sizes = sorted(kernel_sizes)\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.max_kernel_size = max(self.kernel_sizes)\n",
    "        self.max_padding = (self.max_kernel_size - 1) // 2 * dilation\n",
    "\n",
    "        if deploy:\n",
    "            self.fused_conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=self.max_kernel_size, stride=stride,\n",
    "                                        padding=self.max_padding, dilation=dilation, groups=groups, bias=True)\n",
    "        else:\n",
    "            convs = []\n",
    "            if use_bn:\n",
    "                if 0 in self.kernel_sizes:\n",
    "                    assert in_channels == out_channels, \"in_channels and out_channels should be equal when kernel_size is 0\"\n",
    "                    assert stride == 1, \"stride should be 1 when kernel size is 0\"\n",
    "                    convs.append(nn.BatchNorm2d(out_channels))  # Add BatchNorm for kernel size 0\n",
    "                    self.kernel_sizes.remove(0)\n",
    "                # Add ConvBn layers for other kernel sizes\n",
    "                convs.extend([\n",
    "                    ConvBn(in_channels, out_channels, k, stride=stride, dilation=dilation,\n",
    "                           padding=(k - 1) // 2 * dilation, groups=groups)\n",
    "                    for k in self.kernel_sizes\n",
    "                ])\n",
    "            else:\n",
    "                if 0 in self.kernel_sizes:\n",
    "                    assert in_channels == out_channels, \"in_channels and out_channels should be equal when kernel_size is 0\"\n",
    "                    assert stride == 1, \"stride should be 1 when kernel size is 0\"\n",
    "                    convs.append(nn.Identity())  # Identity for kernel size 0\n",
    "                    self.kernel_sizes.remove(0)\n",
    "                # Add Conv2d layers for other kernel sizes\n",
    "                convs.extend([\n",
    "                    nn.Conv2d(in_channels, out_channels, k, stride=stride, dilation=dilation,\n",
    "                              padding=(k - 1) // 2 * dilation, bias=True, groups=groups)\n",
    "                    for k in self.kernel_sizes\n",
    "                ])\n",
    "            self.convs = nn.ModuleList(convs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.deploy:\n",
    "            return self.fused_conv(x)\n",
    "        else:\n",
    "            conv_outputs = []\n",
    "            for conv in self.convs:\n",
    "                conv_outputs.append(conv(x))\n",
    "            return sum(conv_outputs)\n",
    "\n",
    "    def _convert_weight_and_bias(self):\n",
    "        if hasattr(self.convs[-1], 'switch_to_deploy'):\n",
    "            self.convs[-1].switch_to_deploy()\n",
    "            weight = self.convs[-1].fused_conv.weight\n",
    "            bias = self.convs[-1].fused_conv.bias\n",
    "        else:\n",
    "            weight = self.convs[-1].weight\n",
    "            bias = self.convs[-1].bias\n",
    "\n",
    "        for conv in self.convs[:-1]:\n",
    "            if isinstance(conv, nn.BatchNorm2d):\n",
    "                std = (conv.running_var + conv.eps).sqrt()\n",
    "                t = (conv.weight / std).reshape(-1, 1, 1, 1)\n",
    "                pad = (self.max_kernel_size - 1) // 2\n",
    "                input_dim = self.in_channels // self.groups\n",
    "                identity_weight = F.pad(\n",
    "                    torch.zeros(self.convs[-1].fused_conv.weight.shape[0], self.convs[-1].fused_conv.weight.shape[1], 1, 1).to(weight.device),\n",
    "                    [pad, pad, pad, pad]\n",
    "                )\n",
    "                for i in range(self.in_channels):\n",
    "                    identity_weight[i, i % input_dim, self.max_kernel_size//2, self.max_kernel_size//2] = 1\n",
    "                weight = weight + identity_weight* t\n",
    "                bias = bias + conv.bias - conv.running_mean * conv.weight / std\n",
    "            elif isinstance(conv, nn.Identity):\n",
    "                pad = (self.max_kernel_size - 1) // 2\n",
    "                input_dim = self.in_channels // self.groups\n",
    "                identity_weight = F.pad(\n",
    "                    torch.zeros(self.convs[-1].weight.shape[0], self.convs[-1].weight.shape[1], 1, 1).to(weight.device),\n",
    "                    [pad, pad, pad, pad]\n",
    "                ) \n",
    "                for i in range(self.in_channels):\n",
    "                    identity_weight[i, i % input_dim, self.max_kernel_size//2, self.max_kernel_size//2] = 1\n",
    "                weight = weight + identity_weight\n",
    "            elif isinstance(conv, ConvBn):\n",
    "                conv.switch_to_deploy()\n",
    "                conv_weight = conv.fused_conv.weight\n",
    "                pad = (self.max_kernel_size - conv.fused_conv.weight.shape[-1]) // 2\n",
    "                conv_weight = F.pad(conv_weight, [pad, pad, pad, pad])\n",
    "                conv_bias = conv.fused_conv.bias\n",
    "                weight = weight + conv_weight\n",
    "                bias = bias + conv_bias\n",
    "            elif isinstance(conv, nn.Conv2d):\n",
    "                conv_weight = conv.weight\n",
    "                pad = (self.max_kernel_size - conv.weight.shape[-1]) // 2\n",
    "                conv_weight = F.pad(conv_weight, [pad, pad, pad, pad])\n",
    "                conv_bias = conv.bias\n",
    "                weight = weight + conv_weight\n",
    "                bias = bias + conv_bias\n",
    "            else:\n",
    "                raise TypeError(f\"Unsupported layer type: {type(conv)}\")\n",
    "\n",
    "        self.weight = weight.detach()\n",
    "        self.bias = bias.detach()\n",
    "    @torch.no_grad()\n",
    "    def switch_to_deploy(self):\n",
    "        if hasattr(self, 'rbr_reparam'):\n",
    "            return\n",
    "        self._convert_weight_and_bias()\n",
    "        self.fused_conv = nn.Conv2d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=(self.max_kernel_size, self.max_kernel_size),\n",
    "            stride=self.stride,\n",
    "            padding=self.max_padding,\n",
    "            dilation=self.dilation,\n",
    "            groups=self.groups,\n",
    "            bias=True\n",
    "        )\n",
    "        self.fused_conv.weight.data = self.weight\n",
    "        self.fused_conv.bias.data = self.bias\n",
    "        del self.convs\n",
    "        self.deploy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RepConv2d(\n",
      "  (convs): ModuleList(\n",
      "    (0): Identity()\n",
      "    (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), groups=8)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
      "    (3): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
      "  )\n",
      ")\n",
      "RepConv2d(\n",
      "  (fused_conv): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
      ")\n",
      "The outputs are the same before and after deployment.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create the RepConv2d model with kernel sizes that include 0\n",
    "model = RepConv2d(in_channels=64, out_channels=64, kernel_sizes=(0,1, 3,5), stride=1, dilation=1, groups=8, use_bn=False, deploy=False).eval()\n",
    "print(model)\n",
    "#initialize_conv_weights_to_one(model)\n",
    "# Create a dummy input tensor (e.g., batch size of 1, 3 channels, 224x224 image)\n",
    "x = torch.randn(1, 64, 224, 224)\n",
    "\n",
    "# Run the model before deployment (non-deployed state)\n",
    "\n",
    "output_before_deploy = model(x)\n",
    "\n",
    "# Switch the model to deploy mode\n",
    "model.switch_to_deploy()\n",
    "print(model)\n",
    "# Run the model after deployment (fused conv state)\n",
    "\n",
    "output_after_deploy = model(x)\n",
    "\n",
    "# Check if the outputs are \n",
    "# the same\n",
    "if torch.allclose(output_before_deploy, output_after_deploy, atol=1e-5):\n",
    "    print(\"The outputs are the same before and after deployment.\")\n",
    "else:\n",
    "    print(\"The outputs differ before and after deployment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, True),\n",
       " (1, True),\n",
       " (2, True),\n",
       " (3, True),\n",
       " (4, True),\n",
       " (5, True),\n",
       " (6, True),\n",
       " (7, True),\n",
       " (8, True),\n",
       " (9, True)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def test_deployment_consistency_sequential():\n",
    "    # Define a larger set of parameter combinations for testing\n",
    "    parameter_combinations = [\n",
    "        (3, 3, (0, 1, 3), 1, 1, 1, True), \n",
    "        (3, 3, (1, 3, 5), 1, 1, 1, False), \n",
    "        (16, 16, (1, 3), 1, 1, 4, True),\n",
    "        (16, 16, (3, 7, 11), 1, 2, 2, False), \n",
    "        (32, 32, (1, 3, 5, 7), 1, 1, 4, True),\n",
    "        (64, 64, (1, 3, 5), 1, 1, 1, False), \n",
    "        (128, 128, (0, 1, 3, 5), 1, 2, 16, True),\n",
    "        (256, 256, (0,3, 7), 1, 1, 32, False),\n",
    "        (64, 64, (1, 5), 1, 1, 4, True),\n",
    "        (128, 64, (3, 5, 7), 1, 2, 2, False)\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for i, params in enumerate(parameter_combinations):\n",
    "        in_channels, out_channels, kernel_sizes, stride, dilation, groups, use_bn = params\n",
    "        \n",
    "        # Create model instance for each parameter combination\n",
    "        model = RepConv2d(in_channels=in_channels, out_channels=out_channels, \n",
    "                          kernel_sizes=kernel_sizes, stride=stride, dilation=dilation, \n",
    "                          groups=groups, use_bn=use_bn, deploy=False).eval()\n",
    "\n",
    "        # Create a dummy input tensor with a smaller size to reduce memory usage\n",
    "        x = torch.randn(1, in_channels, 64, 64)  # Reduced input size\n",
    "\n",
    "        # Run the model before deployment (non-deployed state)\n",
    "        with torch.no_grad():\n",
    "            output_before_deploy = model(x)\n",
    "\n",
    "        # Switch the model to deploy mode\n",
    "        model.switch_to_deploy()\n",
    "\n",
    "        # Run the model after deployment (fused conv state)\n",
    "        with torch.no_grad():\n",
    "            output_after_deploy = model(x)\n",
    "\n",
    "        # Check if the outputs are the same (ignoring small numerical differences)\n",
    "        outputs_are_the_same = torch.allclose(output_before_deploy, output_after_deploy, atol=1e-5)\n",
    "        results.append((i, outputs_are_the_same))\n",
    "        \n",
    "        # Clear GPU memory (if you're using CUDA) after each test\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run the test for all combinations\n",
    "test_results = test_deployment_consistency_sequential()\n",
    "\n",
    "# Output the results\n",
    "test_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 3.2503, -2.3336, -0.3168,  ...,  1.6845,  1.8223,  0.5355],\n",
       "          [-0.7155, -1.3402, -1.5542,  ...,  1.8213,  0.3087,  0.4821],\n",
       "          [ 1.1372,  2.1317,  0.5604,  ..., -1.3089,  1.2011,  1.0502],\n",
       "          ...,\n",
       "          [-1.0010,  0.4254, -1.7083,  ...,  0.6790, -1.0151,  2.9918],\n",
       "          [-0.9721,  0.8507, -0.8526,  ...,  2.0636, -2.1848,  0.4944],\n",
       "          [-1.6197,  1.3920,  2.0821,  ...,  3.0186,  1.5093, -0.8927]],\n",
       "\n",
       "         [[ 4.2952, -2.6332, -2.8288,  ..., -1.3617,  0.1294, -0.8138],\n",
       "          [-1.1744, -1.9993, -3.4749,  ...,  0.2073,  1.2643, -0.7046],\n",
       "          [-3.1230,  1.4844, -0.8444,  ...,  0.4686, -1.0576, -2.8354],\n",
       "          ...,\n",
       "          [ 1.1390, -1.3227, -1.9438,  ..., -0.1970, -0.8131, -0.0600],\n",
       "          [-1.8434,  0.0814, -0.4531,  ..., -1.8145, -1.4689,  0.6596],\n",
       "          [-1.7036,  0.1385,  1.2308,  ..., -4.2808,  1.7674,  1.1973]],\n",
       "\n",
       "         [[-0.9048, -3.2768,  3.2289,  ..., -2.0433,  0.2257, -0.5137],\n",
       "          [-3.4765, -1.6263, -0.8672,  ..., -0.7963, -4.4799, -1.7442],\n",
       "          [ 0.1329, -0.3687,  2.2495,  ...,  0.2296,  0.5441, -1.2434],\n",
       "          ...,\n",
       "          [ 3.4208,  1.9702,  0.8285,  ...,  1.3894, -2.1858, -2.7786],\n",
       "          [-2.1475, -0.8727,  0.3430,  ...,  2.9407, -3.5709, -2.6814],\n",
       "          [-1.2054,  0.1061,  0.6453,  ...,  2.6181, -4.2726,  0.6714]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_after_deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, False),\n",
       " (1, True),\n",
       " (2, True),\n",
       " (3, False),\n",
       " (4, False),\n",
       " (5, False),\n",
       " (6, True),\n",
       " (7, True),\n",
       " (8, False),\n",
       " (9, False)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBn(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, padding_mode='zeros', deploy=False):\n",
    "        super(ConvBn, self).__init__()\n",
    "        self.deploy = deploy\n",
    "        self.padding = (kernel_size - 1) // 2 * dilation\n",
    "        if deploy:\n",
    "            self.fused_conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(kernel_size,kernel_size), stride=stride,\n",
    "                                        padding=self.padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode)\n",
    "        else:\n",
    "            self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                         kernel_size=(kernel_size, kernel_size), stride=stride,\n",
    "                                         padding=self.padding, dilation=dilation, groups=groups, bias=False,\n",
    "                                         padding_mode=padding_mode)\n",
    "            self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
    "\n",
    "    def _fuse_bn_tensor(self, conv, bn):\n",
    "        # Normalization and scaling\n",
    "        std = (bn.running_var + bn.eps).sqrt()\n",
    "        t = (bn.weight / std).reshape(-1, 1, 1, 1)\n",
    "        \n",
    "        # Apply scaling and shifting to bias\n",
    "        deploy_weight = conv.weight * t\n",
    "        deploy_bias = bn.bias - bn.running_mean * bn.weight / std\n",
    "        \n",
    "        return deploy_weight, deploy_bias\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def switch_to_deploy(self):\n",
    "        if self.bn.training:\n",
    "            raise RuntimeError(\"BatchNorm should be in evaluation mode (eval) before deployment.\")\n",
    "        \n",
    "        deploy_k, deploy_b = self._fuse_bn_tensor(self.conv, self.bn)\n",
    "        self.deploy = True\n",
    "        self.fused_conv = nn.Conv2d(in_channels=self.conv.in_channels, out_channels=self.conv.out_channels,\n",
    "                                    kernel_size=self.conv.kernel_size, stride=self.conv.stride,\n",
    "                                    padding=self.conv.padding, dilation=self.conv.dilation, groups=self.conv.groups, bias=True,\n",
    "                                    padding_mode=self.conv.padding_mode)\n",
    "        self.__delattr__('conv')\n",
    "        self.__delattr__('bn')\n",
    "        \n",
    "        # Apply the fused weights and biases\n",
    "        self.fused_conv.weight.data = deploy_k\n",
    "        self.fused_conv.bias.data = deploy_b\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.deploy:\n",
    "            return self.fused_conv(input)\n",
    "        else:\n",
    "            square_outputs = self.conv(input)\n",
    "            square_outputs = self.bn(square_outputs)\n",
    "            return square_outputs\n",
    "\n",
    "        \n",
    "def test_deployment_consistency_sequential():\n",
    "    # Define a larger set of parameter combinations for testing\n",
    "    parameter_combinations = [\n",
    "        (256, 256,  3, 1, 1, 1, True), \n",
    "        (3, 3,  5, 1, 1, 1, False), \n",
    "        (16, 16, 3, 1, 1, 4, True),\n",
    "        (16, 16,  11, 1, 2, 2, False), \n",
    "        (32, 32,  7, 1, 1, 4, True),\n",
    "        (64, 64,  5, 1, 1, 1, False), \n",
    "        (128, 128,  5, 1, 2, 16, True),\n",
    "        (256, 256,  7, 1, 1, 32, False),\n",
    "        (64, 64,  5, 1, 1, 4, True),\n",
    "        (128, 64,  7, 1, 2, 2, False)\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for i, params in enumerate(parameter_combinations):\n",
    "        in_channels, out_channels, kernel_sizes, stride, dilation, groups, use_bn = params\n",
    "        \n",
    "        # Create model instance for each parameter combination\n",
    "        model = ConvBn(in_channels=in_channels, out_channels=out_channels, \n",
    "                          kernel_size=kernel_sizes, stride=stride, dilation=dilation, \n",
    "                          groups=groups, deploy=False).eval()\n",
    "\n",
    "        # Create a dummy input tensor with a smaller size to reduce memory usage\n",
    "        x = torch.randn(1, in_channels, 64, 64)  # Reduced input size\n",
    "\n",
    "        # Run the model before deployment (non-deployed state)\n",
    "        with torch.no_grad():\n",
    "            output_before_deploy = model(x)\n",
    "\n",
    "        # Switch the model to deploy mode\n",
    "        model.switch_to_deploy()\n",
    "\n",
    "        # Run the model after deployment (fused conv state)\n",
    "        with torch.no_grad():\n",
    "            output_after_deploy = model(x)\n",
    "\n",
    "        # Check if the outputs are the same (ignoring small numerical differences)\n",
    "        outputs_are_the_same = torch.allclose(output_before_deploy, output_after_deploy, atol=1e-6)\n",
    "        results.append((i, outputs_are_the_same))\n",
    "        \n",
    "        # Clear GPU memory (if you're using CUDA) after each test\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run the test for all combinations\n",
    "test_results = test_deployment_consistency_sequential()\n",
    "\n",
    "# Output the results\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 590336\n",
      "Trainable Parameters: 590336\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "模型的 FLOPs: 9.680453632 GigaFLOPs\n",
      "模型的 params: 590.336K params\n",
      "Do the outputs match? True\n",
      "Total Parameters after switch: 590080\n",
      "Trainable Parameters after switch: 590080\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "模型的 FLOPs after switch: 9.663676416 GigaFLOPs\n",
      "模型的 params after switch: 590.080K params\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch \n",
    "\n",
    "# 初始化模型\n",
    "model = ConvBn(256,256,3).eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# 准备输入数据\n",
    "input_tensor_1 = torch.randn(1, 256, 128, 128).to(device)\n",
    "input_tensor_2 = torch.randn(1, 256, 128, 128).to(device)\n",
    "\n",
    "# 打印模型的输出（切换前）\n",
    "output_before = model(input_tensor_1)\n",
    "\n",
    "\n",
    "# 计算初始模型参数\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total Parameters:\", total_params)\n",
    "print(\"Trainable Parameters:\", trainable_params)\n",
    "\n",
    "# 使用 thop 计算 FLOPs 和参数\n",
    "from thop import profile\n",
    "from thop import clever_format\n",
    "flops, params = profile(model, inputs=[input_tensor_1])\n",
    "macs, params = clever_format([flops, total_params], \"%.3f\")\n",
    "gflops = flops / 1e9\n",
    "print(f\"模型的 FLOPs: {gflops} GigaFLOPs\")\n",
    "print(f\"模型的 params: {params} params\")\n",
    "\n",
    "# 切换到部署模式\n",
    "model = repvgg_model_convert(model)\n",
    "\n",
    "# 打印模型的输出（切换后）\n",
    "output_after = model(input_tensor_1)\n",
    "\n",
    "\n",
    "# 检查输出是否相同\n",
    "output_difference = torch.allclose(output_before[0], output_after[0],atol=1e-5)\n",
    "print(\"Do the outputs match?\", output_difference)\n",
    "\n",
    "# 重新计算模型参数\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total Parameters after switch:\", total_params)\n",
    "print(\"Trainable Parameters after switch:\", trainable_params)\n",
    "\n",
    "# 重新计算 FLOPs 和参数\n",
    "flops, params = profile(model, inputs=[input_tensor_1])\n",
    "macs, params = clever_format([flops, total_params], \"%.3f\")\n",
    "gflops = flops / 1e9\n",
    "print(f\"模型的 FLOPs after switch: {gflops} GigaFLOPs\")\n",
    "print(f\"模型的 params after switch: {params} params\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RepVGG Block, identity =  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, True)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RepVGGBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', deploy=False, use_se=False):\n",
    "        super(RepVGGBlock, self).__init__()\n",
    "        self.deploy = deploy\n",
    "        self.groups = groups\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        assert kernel_size == 3\n",
    "        assert padding == 1\n",
    "\n",
    "        padding_11 = padding - kernel_size // 2\n",
    "\n",
    "        self.nonlinearity = nn.ReLU()\n",
    "\n",
    "        if use_se:\n",
    "            #   Note that RepVGG-D2se uses SE before nonlinearity. But RepVGGplus models uses SE after nonlinearity.\n",
    "            self.se = nn.Identity()\n",
    "        else:\n",
    "            self.se = nn.Identity()\n",
    "\n",
    "        if deploy:\n",
    "            self.rbr_reparam = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                                      padding=padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode)\n",
    "\n",
    "        else:\n",
    "            self.rbr_identity = nn.BatchNorm2d(num_features=in_channels) if out_channels == in_channels and stride == 1 else None\n",
    "            self.rbr_dense = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n",
    "            self.rbr_1x1 = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, padding=padding_11, groups=groups)\n",
    "            print('RepVGG Block, identity = ', self.rbr_identity)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if hasattr(self, 'rbr_reparam'):\n",
    "            return self.nonlinearity(self.se(self.rbr_reparam(inputs)))\n",
    "\n",
    "        if self.rbr_identity is None:\n",
    "            id_out = 0\n",
    "        else:\n",
    "            id_out = self.rbr_identity(inputs)\n",
    "\n",
    "        return self.nonlinearity(self.se(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out))\n",
    "\n",
    "\n",
    "    #   Optional. This may improve the accuracy and facilitates quantization in some cases.\n",
    "    #   1.  Cancel the original weight decay on rbr_dense.conv.weight and rbr_1x1.conv.weight.\n",
    "    #   2.  Use like this.\n",
    "    #       loss = criterion(....)\n",
    "    #       for every RepVGGBlock blk:\n",
    "    #           loss += weight_decay_coefficient * 0.5 * blk.get_cust_L2()\n",
    "    #       optimizer.zero_grad()\n",
    "    #       loss.backward()\n",
    "    def get_custom_L2(self):\n",
    "        K3 = self.rbr_dense.conv.weight\n",
    "        K1 = self.rbr_1x1.conv.weight\n",
    "        t3 = (self.rbr_dense.bn.weight / ((self.rbr_dense.bn.running_var + self.rbr_dense.bn.eps).sqrt())).reshape(-1, 1, 1, 1).detach()\n",
    "        t1 = (self.rbr_1x1.bn.weight / ((self.rbr_1x1.bn.running_var + self.rbr_1x1.bn.eps).sqrt())).reshape(-1, 1, 1, 1).detach()\n",
    "\n",
    "        l2_loss_circle = (K3 ** 2).sum() - (K3[:, :, 1:2, 1:2] ** 2).sum()      # The L2 loss of the \"circle\" of weights in 3x3 kernel. Use regular L2 on them.\n",
    "        eq_kernel = K3[:, :, 1:2, 1:2] * t3 + K1 * t1                           # The equivalent resultant central point of 3x3 kernel.\n",
    "        l2_loss_eq_kernel = (eq_kernel ** 2 / (t3 ** 2 + t1 ** 2)).sum()        # Normalize for an L2 coefficient comparable to regular L2.\n",
    "        return l2_loss_eq_kernel + l2_loss_circle\n",
    "\n",
    "\n",
    "\n",
    "#   This func derives the equivalent kernel and bias in a DIFFERENTIABLE way.\n",
    "#   You can get the equivalent kernel and bias at any time and do whatever you want,\n",
    "    #   for example, apply some penalties or constraints during training, just like you do to the other models.\n",
    "#   May be useful for quantization or pruning.\n",
    "    def get_equivalent_kernel_bias(self):\n",
    "        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.rbr_dense)\n",
    "        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.rbr_1x1)\n",
    "        kernelid, biasid = self._fuse_bn_tensor(self.rbr_identity)\n",
    "        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid\n",
    "\n",
    "    def _pad_1x1_to_3x3_tensor(self, kernel1x1):\n",
    "        if kernel1x1 is None:\n",
    "            return 0\n",
    "        else:\n",
    "            return torch.nn.functional.pad(kernel1x1, [1,1,1,1])\n",
    "\n",
    "    def _fuse_bn_tensor(self, branch):\n",
    "        if branch is None:\n",
    "            return 0, 0\n",
    "        if isinstance(branch, nn.Sequential):\n",
    "            kernel = branch.conv.weight\n",
    "            running_mean = branch.bn.running_mean\n",
    "            running_var = branch.bn.running_var\n",
    "            gamma = branch.bn.weight\n",
    "            beta = branch.bn.bias\n",
    "            eps = branch.bn.eps\n",
    "        else:\n",
    "            assert isinstance(branch, nn.BatchNorm2d)\n",
    "            if not hasattr(self, 'id_tensor'):\n",
    "                input_dim = self.in_channels // self.groups\n",
    "                kernel_value = np.zeros((self.in_channels, input_dim, 3, 3), dtype=np.float32)\n",
    "                for i in range(self.in_channels):\n",
    "                    kernel_value[i, i % input_dim, 1, 1] = 1\n",
    "                self.id_tensor = torch.from_numpy(kernel_value).to(branch.weight.device)\n",
    "            kernel = self.id_tensor\n",
    "            running_mean = branch.running_mean\n",
    "            running_var = branch.running_var\n",
    "            gamma = branch.weight\n",
    "            beta = branch.bias\n",
    "            eps = branch.eps\n",
    "        std = (running_var + eps).sqrt()\n",
    "        t = (gamma / std).reshape(-1, 1, 1, 1)\n",
    "        return kernel * t, beta - running_mean * gamma / std\n",
    "\n",
    "    def switch_to_deploy(self):\n",
    "        if hasattr(self, 'rbr_reparam'):\n",
    "            return\n",
    "        kernel, bias = self.get_equivalent_kernel_bias()\n",
    "        self.rbr_reparam = nn.Conv2d(in_channels=self.rbr_dense.conv.in_channels, out_channels=self.rbr_dense.conv.out_channels,\n",
    "                                     kernel_size=self.rbr_dense.conv.kernel_size, stride=self.rbr_dense.conv.stride,\n",
    "                                     padding=self.rbr_dense.conv.padding, dilation=self.rbr_dense.conv.dilation, groups=self.rbr_dense.conv.groups, bias=True)\n",
    "        self.rbr_reparam.weight.data = kernel\n",
    "        self.rbr_reparam.bias.data = bias\n",
    "        self.__delattr__('rbr_dense')\n",
    "        self.__delattr__('rbr_1x1')\n",
    "        if hasattr(self, 'rbr_identity'):\n",
    "            self.__delattr__('rbr_identity')\n",
    "        if hasattr(self, 'id_tensor'):\n",
    "            self.__delattr__('id_tensor')\n",
    "        self.deploy = True\n",
    "\n",
    "def test_deployment_consistency_sequential():\n",
    "    # Define a larger set of parameter combinations for testing\n",
    "    parameter_combinations = [\n",
    "        (256, 256,  3, 1, 1, 1, True), \n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for i, params in enumerate(parameter_combinations):\n",
    "        in_channels, out_channels, kernel_sizes, stride, dilation, groups, use_bn = params\n",
    "        \n",
    "        # Create model instance for each parameter combination\n",
    "        model = RepVGGBlock(in_channels=in_channels, out_channels=out_channels, \n",
    "                          kernel_size=kernel_sizes, stride=stride, dilation=dilation, \n",
    "                          groups=groups, deploy=False,padding=1).eval()\n",
    "\n",
    "        # Create a dummy input tensor with a smaller size to reduce memory usage\n",
    "        x = torch.randn(1, in_channels, 64, 64)  # Reduced input size\n",
    "\n",
    "        # Run the model before deployment (non-deployed state)\n",
    "        with torch.no_grad():\n",
    "            output_before_deploy = model(x)\n",
    "\n",
    "        # Switch the model to deploy mode\n",
    "        model.switch_to_deploy()\n",
    "\n",
    "        # Run the model after deployment (fused conv state)\n",
    "        with torch.no_grad():\n",
    "            output_after_deploy = model(x)\n",
    "\n",
    "        # Check if the outputs are the same (ignoring small numerical differences)\n",
    "        outputs_are_the_same = torch.allclose(output_before_deploy, output_after_deploy, atol=1e-5)\n",
    "        results.append((i, outputs_are_the_same))\n",
    "        \n",
    "        # Clear GPU memory (if you're using CUDA) after each test\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run the test for all combinations\n",
    "test_results = test_deployment_consistency_sequential()\n",
    "\n",
    "# Output the results\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# RepVGG: Making VGG-style ConvNets Great Again (https://openaccess.thecvf.com/content/CVPR2021/papers/Ding_RepVGG_Making_VGG-Style_ConvNets_Great_Again_CVPR_2021_paper.pdf)\n",
    "# Github source: https://github.com/DingXiaoH/RepVGG\n",
    "# Licensed under The MIT License [see LICENSE for details]\n",
    "# --------------------------------------------------------\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "\n",
    "def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups=1):\n",
    "    result = nn.Sequential()\n",
    "    result.add_module('conv', nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                                  kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, bias=False))\n",
    "    result.add_module('bn', nn.BatchNorm2d(num_features=out_channels))\n",
    "    return result\n",
    "\n",
    "class RepVGGBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', deploy=False, use_se=False):\n",
    "        super(RepVGGBlock, self).__init__()\n",
    "        self.deploy = deploy\n",
    "        self.groups = groups\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        assert kernel_size == 3\n",
    "        assert padding == 1\n",
    "\n",
    "        padding_11 = padding - kernel_size // 2\n",
    "\n",
    "        self.nonlinearity = nn.ReLU()\n",
    "\n",
    "        if use_se:\n",
    "            #   Note that RepVGG-D2se uses SE before nonlinearity. But RepVGGplus models uses SE after nonlinearity.\n",
    "            self.se = nn.Identity()\n",
    "        else:\n",
    "            self.se = nn.Identity()\n",
    "\n",
    "        if deploy:\n",
    "            self.rbr_reparam = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                                      padding=padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode)\n",
    "\n",
    "        else:\n",
    "            self.rbr_identity = nn.BatchNorm2d(num_features=in_channels) if out_channels == in_channels and stride == 1 else None\n",
    "            self.rbr_dense = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n",
    "            self.rbr_1x1 = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, padding=padding_11, groups=groups)\n",
    "            print('RepVGG Block, identity = ', self.rbr_identity)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if hasattr(self, 'rbr_reparam'):\n",
    "            return self.nonlinearity(self.se(self.rbr_reparam(inputs)))\n",
    "\n",
    "        if self.rbr_identity is None:\n",
    "            id_out = 0\n",
    "        else:\n",
    "            id_out = self.rbr_identity(inputs)\n",
    "\n",
    "        return self.nonlinearity(self.se(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out))\n",
    "\n",
    "\n",
    "    #   Optional. This may improve the accuracy and facilitates quantization in some cases.\n",
    "    #   1.  Cancel the original weight decay on rbr_dense.conv.weight and rbr_1x1.conv.weight.\n",
    "    #   2.  Use like this.\n",
    "    #       loss = criterion(....)\n",
    "    #       for every RepVGGBlock blk:\n",
    "    #           loss += weight_decay_coefficient * 0.5 * blk.get_cust_L2()\n",
    "    #       optimizer.zero_grad()\n",
    "    #       loss.backward()\n",
    "    def get_custom_L2(self):\n",
    "        K3 = self.rbr_dense.conv.weight\n",
    "        K1 = self.rbr_1x1.conv.weight\n",
    "        t3 = (self.rbr_dense.bn.weight / ((self.rbr_dense.bn.running_var + self.rbr_dense.bn.eps).sqrt())).reshape(-1, 1, 1, 1).detach()\n",
    "        t1 = (self.rbr_1x1.bn.weight / ((self.rbr_1x1.bn.running_var + self.rbr_1x1.bn.eps).sqrt())).reshape(-1, 1, 1, 1).detach()\n",
    "\n",
    "        l2_loss_circle = (K3 ** 2).sum() - (K3[:, :, 1:2, 1:2] ** 2).sum()      # The L2 loss of the \"circle\" of weights in 3x3 kernel. Use regular L2 on them.\n",
    "        eq_kernel = K3[:, :, 1:2, 1:2] * t3 + K1 * t1                           # The equivalent resultant central point of 3x3 kernel.\n",
    "        l2_loss_eq_kernel = (eq_kernel ** 2 / (t3 ** 2 + t1 ** 2)).sum()        # Normalize for an L2 coefficient comparable to regular L2.\n",
    "        return l2_loss_eq_kernel + l2_loss_circle\n",
    "\n",
    "\n",
    "\n",
    "#   This func derives the equivalent kernel and bias in a DIFFERENTIABLE way.\n",
    "#   You can get the equivalent kernel and bias at any time and do whatever you want,\n",
    "    #   for example, apply some penalties or constraints during training, just like you do to the other models.\n",
    "#   May be useful for quantization or pruning.\n",
    "    def get_equivalent_kernel_bias(self):\n",
    "        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.rbr_dense)\n",
    "        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.rbr_1x1)\n",
    "        kernelid, biasid = self._fuse_bn_tensor(self.rbr_identity)\n",
    "        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid\n",
    "\n",
    "    def _pad_1x1_to_3x3_tensor(self, kernel1x1):\n",
    "        if kernel1x1 is None:\n",
    "            return 0\n",
    "        else:\n",
    "            return torch.nn.functional.pad(kernel1x1, [1,1,1,1])\n",
    "\n",
    "    def _fuse_bn_tensor(self, branch):\n",
    "        if branch is None:\n",
    "            return 0, 0\n",
    "        if isinstance(branch, nn.Sequential):\n",
    "            kernel = branch.conv.weight\n",
    "            running_mean = branch.bn.running_mean\n",
    "            running_var = branch.bn.running_var\n",
    "            gamma = branch.bn.weight\n",
    "            beta = branch.bn.bias\n",
    "            eps = branch.bn.eps\n",
    "        else:\n",
    "            assert isinstance(branch, nn.BatchNorm2d)\n",
    "            if not hasattr(self, 'id_tensor'):\n",
    "                input_dim = self.in_channels // self.groups\n",
    "                kernel_value = np.zeros((self.in_channels, input_dim, 3, 3), dtype=np.float32)\n",
    "                for i in range(self.in_channels):\n",
    "                    kernel_value[i, i % input_dim, 1, 1] = 1\n",
    "                self.id_tensor = torch.from_numpy(kernel_value).to(branch.weight.device)\n",
    "            kernel = self.id_tensor\n",
    "            running_mean = branch.running_mean\n",
    "            running_var = branch.running_var\n",
    "            gamma = branch.weight\n",
    "            beta = branch.bias\n",
    "            eps = branch.eps\n",
    "        std = (running_var + eps).sqrt()\n",
    "        t = (gamma / std).reshape(-1, 1, 1, 1)\n",
    "        return kernel * t, beta - running_mean * gamma / std\n",
    "\n",
    "    def switch_to_deploy(self):\n",
    "        if hasattr(self, 'rbr_reparam'):\n",
    "            return\n",
    "        kernel, bias = self.get_equivalent_kernel_bias()\n",
    "        self.rbr_reparam = nn.Conv2d(in_channels=self.rbr_dense.conv.in_channels, out_channels=self.rbr_dense.conv.out_channels,\n",
    "                                     kernel_size=self.rbr_dense.conv.kernel_size, stride=self.rbr_dense.conv.stride,\n",
    "                                     padding=self.rbr_dense.conv.padding, dilation=self.rbr_dense.conv.dilation, groups=self.rbr_dense.conv.groups, bias=True)\n",
    "        self.rbr_reparam.weight.data = kernel\n",
    "        self.rbr_reparam.bias.data = bias\n",
    "        self.__delattr__('rbr_dense')\n",
    "        self.__delattr__('rbr_1x1')\n",
    "        if hasattr(self, 'rbr_identity'):\n",
    "            self.__delattr__('rbr_identity')\n",
    "        if hasattr(self, 'id_tensor'):\n",
    "            self.__delattr__('id_tensor')\n",
    "        self.deploy = True\n",
    "\n",
    "\n",
    "\n",
    "class RepVGG(nn.Module):\n",
    "\n",
    "    def __init__(self, num_blocks, num_classes=1000, width_multiplier=None, override_groups_map=None, deploy=False, use_se=False, use_checkpoint=False):\n",
    "        super(RepVGG, self).__init__()\n",
    "        assert len(width_multiplier) == 4\n",
    "        self.deploy = deploy\n",
    "        self.override_groups_map = override_groups_map or dict()\n",
    "        assert 0 not in self.override_groups_map\n",
    "        self.use_se = use_se\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        self.in_planes = min(64, int(64 * width_multiplier[0]))\n",
    "        self.stage0 = RepVGGBlock(in_channels=3, out_channels=self.in_planes, kernel_size=3, stride=2, padding=1, deploy=self.deploy, use_se=self.use_se)\n",
    "        self.cur_layer_idx = 1\n",
    "        self.stage1 = self._make_stage(int(64 * width_multiplier[0]), num_blocks[0], stride=2)\n",
    "        self.stage2 = self._make_stage(int(128 * width_multiplier[1]), num_blocks[1], stride=2)\n",
    "        self.stage3 = self._make_stage(int(256 * width_multiplier[2]), num_blocks[2], stride=2)\n",
    "        self.stage4 = self._make_stage(int(512 * width_multiplier[3]), num_blocks[3], stride=2)\n",
    "        self.gap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.linear = nn.Linear(int(512 * width_multiplier[3]), num_classes)\n",
    "\n",
    "    def _make_stage(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        blocks = []\n",
    "        for stride in strides:\n",
    "            cur_groups = self.override_groups_map.get(self.cur_layer_idx, 1)\n",
    "            blocks.append(RepVGGBlock(in_channels=self.in_planes, out_channels=planes, kernel_size=3,\n",
    "                                      stride=stride, padding=1, groups=cur_groups, deploy=self.deploy, use_se=self.use_se))\n",
    "            self.in_planes = planes\n",
    "            self.cur_layer_idx += 1\n",
    "        return nn.ModuleList(blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.stage0(x)\n",
    "        for stage in (self.stage1, self.stage2, self.stage3, self.stage4):\n",
    "            for block in stage:\n",
    "                if self.use_checkpoint:\n",
    "                    out = checkpoint.checkpoint(block, out)\n",
    "                else:\n",
    "                    out = block(out)\n",
    "        out = self.gap(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "optional_groupwise_layers = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26]\n",
    "g2_map = {l: 2 for l in optional_groupwise_layers}\n",
    "g4_map = {l: 4 for l in optional_groupwise_layers}\n",
    "\n",
    "def create_RepVGG_A0(deploy=False, use_checkpoint=False):\n",
    "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
    "                  width_multiplier=[0.75, 0.75, 0.75, 2.5], override_groups_map=None, deploy=deploy, use_checkpoint=use_checkpoint)\n",
    "\n",
    "def create_RepVGG_A1(deploy=False, use_checkpoint=False):\n",
    "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
    "                  width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy, use_checkpoint=use_checkpoint)\n",
    "\n",
    "def create_RepVGG_A2(deploy=False, use_checkpoint=False):\n",
    "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
    "                  width_multiplier=[1.5, 1.5, 1.5, 2.75], override_groups_map=None, deploy=deploy, use_checkpoint=use_checkpoint)\n",
    "\n",
    "def create_RepVGG_B0(deploy=False, use_checkpoint=False):\n",
    "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
    "                  width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy, use_checkpoint=use_checkpoint)\n",
    "\n",
    "def create_RepVGG_B1(deploy=False, use_checkpoint=False):\n",
    "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
    "                  width_multiplier=[2, 2, 2, 4], override_groups_map=None, deploy=deploy, use_checkpoint=use_checkpoint)\n",
    "\n",
    "def create_RepVGG_B1g2(deploy=False, use_checkpoint=False):\n",
    "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
    "                  width_multiplier=[2, 2, 2, 4], override_groups_map=g2_map, deploy=deploy, use_checkpoint=use_checkpoint)\n",
    "\n",
    "def create_RepVGG_B1g4(deploy=False, use_checkpoint=False):\n",
    "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
    "                  width_multiplier=[2, 2, 2, 4], override_groups_map=g4_map, deploy=deploy, use_checkpoint=use_checkpoint)\n",
    "\n",
    "\n",
    "def create_RepVGG_B2(deploy=False, use_checkpoint=False):\n",
    "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
    "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=None, deploy=deploy, use_checkpoint=use_checkpoint)\n",
    "\n",
    "def create_RepVGG_B2g2(deploy=False, use_checkpoint=False):\n",
    "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
    "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=g2_map, deploy=deploy, use_checkpoint=use_checkpoint)\n",
    "\n",
    "def create_RepVGG_B2g4(deploy=False, use_checkpoint=False):\n",
    "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
    "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=g4_map, deploy=deploy, use_checkpoint=use_checkpoint)\n",
    "\n",
    "\n",
    "def create_RepVGG_B3(deploy=False, use_checkpoint=False):\n",
    "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
    "                  width_multiplier=[3, 3, 3, 5], override_groups_map=None, deploy=deploy, use_checkpoint=use_checkpoint)\n",
    "\n",
    "def create_RepVGG_B3g2(deploy=False, use_checkpoint=False):\n",
    "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
    "                  width_multiplier=[3, 3, 3, 5], override_groups_map=g2_map, deploy=deploy, use_checkpoint=use_checkpoint)\n",
    "\n",
    "def create_RepVGG_B3g4(deploy=False, use_checkpoint=False):\n",
    "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
    "                  width_multiplier=[3, 3, 3, 5], override_groups_map=g4_map, deploy=deploy, use_checkpoint=use_checkpoint)\n",
    "\n",
    "def create_RepVGG_D2se(deploy=False, use_checkpoint=False):\n",
    "    return RepVGG(num_blocks=[8, 14, 24, 1], num_classes=1000,\n",
    "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=None, deploy=deploy, use_se=True, use_checkpoint=use_checkpoint)\n",
    "\n",
    "\n",
    "func_dict = {\n",
    "'RepVGG-A0': create_RepVGG_A0,\n",
    "'RepVGG-A1': create_RepVGG_A1,\n",
    "'RepVGG-A2': create_RepVGG_A2,\n",
    "'RepVGG-B0': create_RepVGG_B0,\n",
    "'RepVGG-B1': create_RepVGG_B1,\n",
    "'RepVGG-B1g2': create_RepVGG_B1g2,\n",
    "'RepVGG-B1g4': create_RepVGG_B1g4,\n",
    "'RepVGG-B2': create_RepVGG_B2,\n",
    "'RepVGG-B2g2': create_RepVGG_B2g2,\n",
    "'RepVGG-B2g4': create_RepVGG_B2g4,\n",
    "'RepVGG-B3': create_RepVGG_B3,\n",
    "'RepVGG-B3g2': create_RepVGG_B3g2,\n",
    "'RepVGG-B3g4': create_RepVGG_B3g4,\n",
    "'RepVGG-D2se': create_RepVGG_D2se,      #   Updated at April 25, 2021. This is not reported in the CVPR paper.\n",
    "}\n",
    "def get_RepVGG_func_by_name(name):\n",
    "    return func_dict[name]\n",
    "\n",
    "\n",
    "\n",
    "#   Use this for converting a RepVGG model or a bigger model with RepVGG as its component\n",
    "#   Use like this\n",
    "#   model = create_RepVGG_A0(deploy=False)\n",
    "#   train model or load weights\n",
    "#   repvgg_model_convert(model, save_path='repvgg_deploy.pth')\n",
    "#   If you want to preserve the original model, call with do_copy=True\n",
    "\n",
    "#   ====================== for using RepVGG as the backbone of a bigger model, e.g., PSPNet, the pseudo code will be like\n",
    "#   train_backbone = create_RepVGG_B2(deploy=False)\n",
    "#   train_backbone.load_state_dict(torch.load('RepVGG-B2-train.pth'))\n",
    "#   train_pspnet = build_pspnet(backbone=train_backbone)\n",
    "#   segmentation_train(train_pspnet)\n",
    "#   deploy_pspnet = repvgg_model_convert(train_pspnet)\n",
    "#   segmentation_test(deploy_pspnet)\n",
    "#   =====================   example_pspnet.py shows an example\n",
    "\n",
    "def repvgg_model_convert(model:torch.nn.Module, save_path=None, do_copy=True):\n",
    "    if do_copy:\n",
    "        model = copy.deepcopy(model)\n",
    "    for module in model.modules():\n",
    "        if hasattr(module, 'switch_to_deploy'):\n",
    "            module.switch_to_deploy()\n",
    "    if save_path is not None:\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RepVGG Block, identity =  None\n",
      "RepVGG Block, identity =  None\n",
      "RepVGG Block, identity =  BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "RepVGG Block, identity =  BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "RepVGG Block, identity =  BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "RepVGG Block, identity =  None\n",
      "RepVGG Block, identity =  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "RepVGG Block, identity =  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "RepVGG Block, identity =  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "RepVGG Block, identity =  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "RepVGG Block, identity =  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "RepVGG Block, identity =  None\n",
      "RepVGG Block, identity =  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "RepVGG Block, identity =  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "RepVGG Block, identity =  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "RepVGG Block, identity =  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "RepVGG Block, identity =  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "RepVGG Block, identity =  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "RepVGG Block, identity =  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "RepVGG Block, identity =  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "RepVGG Block, identity =  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "RepVGG Block, identity =  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "RepVGG Block, identity =  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "RepVGG Block, identity =  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "RepVGG Block, identity =  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "RepVGG Block, identity =  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "RepVGG Block, identity =  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "RepVGG Block, identity =  None\n",
      "Total Parameters: 45782376\n",
      "Trainable Parameters: 45782376\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "模型的 FLOPs: 9.845878784 GigaFLOPs\n",
      "模型的 params: 45.782M params\n",
      "Total Parameters: 41360104\n",
      "Trainable Parameters: 41360104\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "模型的 FLOPs: 8.809844736 GigaFLOPs\n",
      "模型的 params: 41.360M params\n",
      "The outputs are the same before and after deployment.\n"
     ]
    }
   ],
   "source": [
    "model = create_RepVGG_B1g2().eval()\n",
    "\n",
    "#initialize_conv_weights_to_one(model)\n",
    "# Create a dummy input tensor (e.g., batch size of 1, 3 channels, 224x224 image)\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# 计算初始模型参数\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total Parameters:\", total_params)\n",
    "print(\"Trainable Parameters:\", trainable_params)\n",
    "\n",
    "# 使用 thop 计算 FLOPs 和参数\n",
    "from thop import profile\n",
    "from thop import clever_format\n",
    "flops, params = profile(model, inputs=[x])\n",
    "macs, params = clever_format([flops, total_params], \"%.3f\")\n",
    "gflops = flops / 1e9\n",
    "print(f\"模型的 FLOPs: {gflops} GigaFLOPs\")\n",
    "print(f\"模型的 params: {params} params\")\n",
    "\n",
    "\n",
    "output_before_deploy = model(x)\n",
    "\n",
    "# Switch the model to deploy mode\n",
    "model= repvgg_model_convert(model, save_path=None, do_copy=True)\n",
    "\n",
    "# Run the model after deployment (fused conv state)\n",
    "\n",
    "output_after_deploy = model(x)\n",
    "# 计算初始模型参数\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total Parameters:\", total_params)\n",
    "print(\"Trainable Parameters:\", trainable_params)\n",
    "\n",
    "# 使用 thop 计算 FLOPs 和参数\n",
    "from thop import profile\n",
    "from thop import clever_format\n",
    "flops, params = profile(model, inputs=[x])\n",
    "macs, params = clever_format([flops, total_params], \"%.3f\")\n",
    "gflops = flops / 1e9\n",
    "print(f\"模型的 FLOPs: {gflops} GigaFLOPs\")\n",
    "print(f\"模型的 params: {params} params\")\n",
    "# Check if the outputs are \n",
    "# the same\n",
    "if torch.allclose(output_before_deploy, output_after_deploy, atol=1e-5):\n",
    "    print(\"The outputs are the same before and after deployment.\")\n",
    "else:\n",
    "    print(\"The outputs differ before and after deployment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting thop\n",
      "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from thop) (2.6.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from torch->thop) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from torch->thop) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from torch->thop) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from torch->thop) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from torch->thop) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from torch->thop) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from sympy==1.13.1->torch->thop) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/vistar/lib/python3.10/site-packages (from jinja2->torch->thop) (3.0.2)\n",
      "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: thop\n",
      "Successfully installed thop-0.1.1.post2209072238\n"
     ]
    }
   ],
   "source": [
    "!pip install thop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vistar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
